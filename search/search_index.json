{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome","text":"<p>The Terraform AWS Provider is the work of thousands of contributors, and is maintained by a small team within HashiCorp. This site contains extensive instructions about how to contribute and how the AWS provider works.</p> <p>Please Note: This documentation is intended for Terraform AWS Provider code developers. Typical operators writing and applying Terraform configurations do not need to read or understand this material.</p>"},{"location":"#contribute","title":"Contribute","text":"<p>Please follow the following steps to ensure your contribution goes smoothly.</p>"},{"location":"#1-configure-development-environment","title":"1. Configure Development Environment","text":"<p>Install Terraform and Go. Clone the repository, compile the provider, and set up testing. Refer to Configure Development Environment.</p>"},{"location":"#2-write-code","title":"2. Write Code","text":"<p>Follow the guide for your contribution type and refer to the Development Reference materials as needed for additional details about provider design, expected naming conventions, guidance for error handling, etc.</p> Contribution Guide Description Resources Allow the management of a logical resource within AWS by adding a new resource to the Terraform AWS Provider. Data Source Let your Terraform configurations use data from resources not under local management by creating ready only data sources. Services Allow Terraform (via the AWS Provider) to manage an entirely new AWS service by introducing the resources and data sources required to manage configuration of the service. AWS Region New regions are immediately usable with the provider with the caveat that a configuration workaround is required to skip validation of the region during cli operations. A small set of changes are required to makes this workaround necessary. Bug Fix or Enhancement These constitute the majority of pull requests submitted, many of which we address and merge regardless of priority in our regular internal gardening days. Resource Name Generation Allow a resource to either fully, or partially, generate its own resource names. This can be useful in cases where the resource name uniquely identifes the resource and it needs to be recreated. It can also be used when a name is required, but the specific name is not important. Tagging Support Many AWS resources allow assigning metadata via tags. However, frequently AWS services are launched without tagging support so this will often need to be added later. Import Support Adding import support allows <code>terraform import</code> to be run targeting an existing unmanaged resource and pulling its configuration into Terraform state. Typically import support is added during initial resource implementation but in some cases this will need to be added later. Documentation Changes The provider documentation is displayed on the Terraform Registry and is sourced and refreshed from the provider repository during the release process."},{"location":"#3-write-tests","title":"3. Write Tests","text":"<p>We require changes to be covered by acceptance tests for all contributions. If you are unable to pay for acceptance tests for your contributions, mention this in your pull request. We will happily accept \"best effort\" acceptance tests implementations and run them for you on our side. Your PR may take longer to merge, but this is not a blocker for contributions.</p>"},{"location":"#4-update-the-changelog","title":"4. Update the Changelog","text":"<p>HashiCorp's open-source projects have always maintained a user-friendly, readable CHANGELOG.md that allows users to tell at a glance whether a release should have any effect on them, and to gauge the risk of an upgrade. Not all changes require an entry in the changelog, refer to our Changelog Process for details about when and how to create a changelog.</p>"},{"location":"#5-create-a-pull-request","title":"5. Create a Pull Request","text":"<p>When your contribution is ready, Create a Pull Request in the AWS provider repository.</p> <p>Pull requests are usually triaged within a few days of creation and are prioritized based on community reactions. Our Prioritization Guides provides more details about the process.</p>"},{"location":"#submit-an-issue","title":"Submit an Issue","text":"<p>In addition to contributions, we welcome bug reports and feature requests.</p>"},{"location":"#join-the-contributors-slack","title":"Join the Contributors Slack","text":"<p>For frequent contributors, it's useful to join the contributors Slack channel hosted within the HashiCorp Slack workspace. This Slack channel is used to discuss topics such as general contribution questions, suggestions for improving the contribution process, coordinating on pair programming sessions, etc. The channel is not intended as a place to request status updates on open issues or pull requests. For prioritization questions, instead refer to the prioritization guide.</p> <p>To request to join, fill out the request form and allow time for the request to be reviewed and processed.</p>"},{"location":"acc-test-environment-variables/","title":"Acceptance Testing Environment Variable Dictionary","text":"<p>Environment variables (beyond standard AWS Go SDK ones) used by acceptance testing. See also the <code>internal/acctest</code> package.</p> Variable Description <code>ACM_CERTIFICATE_ROOT_DOMAIN</code> Root domain name to use with ACM Certificate testing. <code>ACM_CERTIFICATE_MULTIPLE_ISSUED_DOMAIN</code> Domain name of ACM Certificate with a multiple issued certificates. DEPRECATED: Should be replaced with <code>aws_acm_certficate</code> resource usage in tests. <code>ACM_CERTIFICATE_MULTIPLE_ISSUED_MOST_RECENT_ARN</code> Amazon Resource Name of most recent ACM Certificate with a multiple issued certificates. DEPRECATED: Should be replaced with <code>aws_acm_certficate</code> resource usage in tests. <code>ACM_CERTIFICATE_SINGLE_ISSUED_DOMAIN</code> Domain name of ACM Certificate with a single issued certificate. DEPRECATED: Should be replaced with <code>aws_acm_certficate</code> resource usage in tests. <code>ACM_CERTIFICATE_SINGLE_ISSUED_MOST_RECENT_ARN</code> Amazon Resource Name of most recent ACM Certificate with a single issued certificate. DEPRECATED: Should be replaced with <code>aws_acm_certficate</code> resource usage in tests. <code>ADM_CLIENT_ID</code> Identifier for Amazon Device Manager Client in Pinpoint testing. <code>AMPLIFY_DOMAIN_NAME</code> Domain name to use for Amplify domain association testing. <code>AMPLIFY_GITHUB_ACCESS_TOKEN</code> GitHub access token used for AWS Amplify testing. <code>AMPLIFY_GITHUB_REPOSITORY</code> GitHub repository used for AWS Amplify testing. <code>ADM_CLIENT_SECRET</code> Secret for Amazon Device Manager Client in Pinpoint testing. <code>APNS_BUNDLE_ID</code> Identifier for Apple Push Notification Service Bundle in Pinpoint testing. <code>APNS_CERTIFICATE</code> Certificate (PEM format) for Apple Push Notification Service in Pinpoint testing. <code>APNS_CERTIFICATE_PRIVATE_KEY</code> Private key for Apple Push Notification Service in Pinpoint testing. <code>APNS_SANDBOX_BUNDLE_ID</code> Identifier for Sandbox Apple Push Notification Service Bundle in Pinpoint testing. <code>APNS_SANDBOX_CERTIFICATE</code> Certificate (PEM format) for Sandbox Apple Push Notification Service in Pinpoint testing. <code>APNS_SANDBOX_CERTIFICATE_PRIVATE_KEY</code> Private key for Sandbox Apple Push Notification Service in Pinpoint testing. <code>APNS_SANDBOX_CREDENTIAL</code> Credential contents for Sandbox Apple Push Notification Service in SNS Application Platform testing. Conflicts with <code>APNS_SANDBOX_CREDENTIAL_PATH</code>. <code>APNS_SANDBOX_CREDENTIAL_PATH</code> Path to credential for Sandbox Apple Push Notification Service in SNS Application Platform testing. Conflicts with <code>APNS_SANDBOX_CREDENTIAL</code>. <code>APNS_SANDBOX_PRINCIPAL</code> Principal contents for Sandbox Apple Push Notification Service in SNS Application Platform testing. Conflicts with <code>APNS_SANDBOX_PRINCIPAL_PATH</code>. <code>APNS_SANDBOX_PRINCIPAL_PATH</code> Path to principal for Sandbox Apple Push Notification Service in SNS Application Platform testing. Conflicts with <code>APNS_SANDBOX_PRINCIPAL</code>. <code>APNS_SANDBOX_TEAM_ID</code> Identifier for Sandbox Apple Push Notification Service Team in Pinpoint testing. <code>APNS_SANDBOX_TOKEN_KEY</code> Token key file content (.p8 format) for Sandbox Apple Push Notification Service in Pinpoint testing. <code>APNS_SANDBOX_TOKEN_KEY_ID</code> Identifier for Sandbox Apple Push Notification Service Token Key in Pinpoint testing. <code>APNS_TEAM_ID</code> Identifier for Apple Push Notification Service Team in Pinpoint testing. <code>APNS_TOKEN_KEY</code> Token key file content (.p8 format) for Apple Push Notification Service in Pinpoint testing. <code>APNS_TOKEN_KEY_ID</code> Identifier for Apple Push Notification Service Token Key in Pinpoint testing. <code>APNS_VOIP_BUNDLE_ID</code> Identifier for VOIP Apple Push Notification Service Bundle in Pinpoint testing. <code>APNS_VOIP_CERTIFICATE</code> Certificate (PEM format) for VOIP Apple Push Notification Service in Pinpoint testing. <code>APNS_VOIP_CERTIFICATE_PRIVATE_KEY</code> Private key for VOIP Apple Push Notification Service in Pinpoint testing. <code>APNS_VOIP_TEAM_ID</code> Identifier for VOIP Apple Push Notification Service Team in Pinpoint testing. <code>APNS_VOIP_TOKEN_KEY</code> Token key file content (.p8 format) for VOIP Apple Push Notification Service in Pinpoint testing. <code>APNS_VOIP_TOKEN_KEY_ID</code> Identifier for VOIP Apple Push Notification Service Token Key in Pinpoint testing. <code>APPRUNNER_CUSTOM_DOMAIN</code> A custom domain endpoint (root domain, subdomain, or wildcard) for AppRunner Custom Domain Association testing. <code>AUDITMANAGER_DEREGISTER_ACCOUNT_ON_DESTROY</code> Flag to execute tests that will disable AuditManager in the account upon destruction. <code>AUDITMANAGER_ORGANIZATION_ADMIN_ACCOUNT_ID</code> Organization admin account identifier for use in AuditManager testing. <code>AWS_ALTERNATE_ACCESS_KEY_ID</code> AWS access key ID with access to a secondary AWS account for tests requiring multiple accounts. Requires <code>AWS_ALTERNATE_SECRET_ACCESS_KEY</code>. Conflicts with <code>AWS_ALTERNATE_PROFILE</code>. <code>AWS_ALTERNATE_SECRET_ACCESS_KEY</code> AWS secret access key with access to a secondary AWS account for tests requiring multiple accounts. Requires <code>AWS_ALTERNATE_ACCESS_KEY_ID</code>. Conflicts with <code>AWS_ALTERNATE_PROFILE</code>. <code>AWS_ALTERNATE_PROFILE</code> AWS profile with access to a secondary AWS account for tests requiring multiple accounts. Conflicts with <code>AWS_ALTERNATE_ACCESS_KEY_ID</code> and <code>AWS_ALTERNATE_SECRET_ACCESS_KEY</code>. <code>AWS_ALTERNATE_REGION</code> Secondary AWS region for tests requiring multiple regions. Defaults to <code>us-east-1</code>. <code>AWS_API_GATEWAY_DOMAIN_NAME_CERTIFICATE_BODY</code> Certificate body of publicly trusted certificate for API Gateway Domain Name testing. <code>AWS_API_GATEWAY_DOMAIN_NAME_CERTIFICATE_CHAIN</code> Certificate chain of publicly trusted certificate for API Gateway Domain Name testing. <code>AWS_API_GATEWAY_DOMAIN_NAME_CERTIFICATE_PRIVATE_KEY</code> Private key of publicly trusted certificate for API Gateway Domain Name testing. <code>AWS_API_GATEWAY_DOMAIN_NAME_REGIONAL_CERTIFICATE_NAME_ENABLED</code> Flag to enable API Gateway Domain Name regional certificate upload testing. <code>AWS_CODEBUILD_BITBUCKET_SOURCE_LOCATION</code> BitBucket source URL for CodeBuild testing. CodeBuild must have access to this repository via OAuth or Source Credentials. Defaults to <code>https://terraform@bitbucket.org/terraform/aws-test.git</code>. <code>AWS_CODEBUILD_GITHUB_SOURCE_LOCATION</code> GitHub source URL for CodeBuild testing. CodeBuild must have access to this repository via OAuth or Source Credentials. Defaults to <code>https://github.com/hashibot-test/aws-test.git</code>. <code>AWS_DEFAULT_REGION</code> Primary AWS region for tests. Defaults to <code>us-west-2</code>. <code>AWS_DETECTIVE_MEMBER_EMAIL</code> Email address for Detective Member testing. A valid email address associated with an AWS root account is required for tests to pass. <code>AWS_EC2_CLASSIC_REGION</code> AWS region for EC2-Classic testing. Defaults to <code>us-east-1</code> in AWS Commercial and <code>AWS_DEFAULT_REGION</code> otherwise. <code>AWS_EC2_CLIENT_VPN_LIMIT</code> Concurrency limit for Client VPN acceptance tests. Default is 5 if not specified. <code>AWS_EC2_EIP_PUBLIC_IPV4_POOL</code> Identifier for EC2 Public IPv4 Pool for EC2 EIP testing. <code>AWS_GUARDDUTY_MEMBER_ACCOUNT_ID</code> Identifier of AWS Account for GuardDuty Member testing. DEPRECATED: Should be replaced with standard alternate account handling for tests. <code>AWS_GUARDDUTY_MEMBER_EMAIL</code> Email address for GuardDuty Member testing. DEPRECATED: It may be possible to use a placeholder email address instead. <code>AWS_LAMBDA_IMAGE_LATEST_ID</code> ECR repository image URI (tagged as <code>latest</code>) for Lambda container image acceptance tests. <code>AWS_LAMBDA_IMAGE_V1_ID</code> ECR repository image URI (tagged as <code>v1</code>) for Lambda container image acceptance tests. <code>AWS_LAMBDA_IMAGE_V2_ID</code> ECR repository image URI (tagged as <code>v2</code>) for Lambda container image acceptance tests. <code>DX_CONNECTION_ID</code> Identifier for Direct Connect Connection testing. <code>DX_VIRTUAL_INTERFACE_ID</code> Identifier for Direct Connect Virtual Interface testing. <code>EC2_SECURITY_GROUP_RULES_PER_GROUP_LIMIT</code> EC2 Quota for Rules per Security Group. Defaults to 50. DEPRECATED: Can be augmented or replaced with Service Quotas lookup. <code>EVENT_BRIDGE_PARTNER_EVENT_BUS_NAME</code> Amazon EventBridge partner event bus name. <code>EVENT_BRIDGE_PARTNER_EVENT_SOURCE_NAME</code> Amazon EventBridge partner event source name. <code>GCM_API_KEY</code> API Key for Google Cloud Messaging in Pinpoint and SNS Platform Application testing. <code>GITHUB_TOKEN</code> GitHub token for CodePipeline testing. <code>GLOBALACCERATOR_BYOIP_IPV4_ADDRESS</code> IPv4 address from a BYOIP CIDR of AWS Account used for testing Global Accelerator's BYOIP accelerator. <code>GRAFANA_SSO_GROUP_ID</code> AWS SSO group ID for Grafana testing. <code>GRAFANA_SSO_USER_ID</code> AWS SSO user ID for Grafana testing. <code>MACIE_MEMBER_ACCOUNT_ID</code> Identifier of AWS Account for Macie Member testing. DEPRECATED: Should be replaced with standard alternate account handling for tests. <code>QUICKSIGHT_NAMESPACE</code> QuickSight namespace name for testing. <code>ROUTE53DOMAINS_DOMAIN_NAME</code> Registered domain for Route 53 Domains testing. <code>SAGEMAKER_IMAGE_VERSION_BASE_IMAGE</code> SageMaker base image to use for tests. <code>SERVICEQUOTAS_INCREASE_ON_CREATE_QUOTA_CODE</code> Quota Code for Service Quotas testing (submits support case). <code>SERVICEQUOTAS_INCREASE_ON_CREATE_SERVICE_CODE</code> Service Code for Service Quotas testing (submits support case). <code>SERVICEQUOTAS_INCREASE_ON_CREATE_VALUE</code> Value of quota increase for Service Quotas testing (submits support case). <code>SES_DOMAIN_IDENTITY_ROOT_DOMAIN</code> Root domain name of publicly accessible and Route 53 configurable domain for SES Domain Identity testing. <code>SES_DEDICATED_IP</code> Dedicated IP address for testing IP assignment with a \"Standard\" (non-managed) SES dedicated IP pool. <code>SWF_DOMAIN_TESTING_ENABLED</code> Enables SWF Domain testing (API does not support deletions). <code>TEST_AWS_ORGANIZATION_ACCOUNT_EMAIL_DOMAIN</code> Email address for Organizations Account testing. <code>TEST_AWS_SES_VERIFIED_EMAIL_ARN</code> Verified SES Email Identity for use in Cognito User Pool testing. <code>TF_ACC</code> Enables Go tests containing <code>resource.Test()</code> and <code>resource.ParallelTest()</code>. <code>TF_ACC_ASSUME_ROLE_ARN</code> Amazon Resource Name of existing IAM Role to use for limited permissions acceptance testing. <code>TF_TEST_CLOUDFRONT_RETAIN</code> Flag to disable but dangle CloudFront Distributions during testing to reduce feedback time (must be manually destroyed afterwards)"},{"location":"add-a-new-datasource/","title":"Adding a New Data Source","text":"<p>New data sources are required when AWS adds a new service, or adds new features within an existing service which would require a new data source to allow practitioners to query existing resources of that type for use in their configurations. Anything with a Describe or Get endpoint could make a data source, but some are more useful than others.</p> <p>Each data source should be submitted for review in isolation, pull requests containing multiple data sources and/or resources are harder to review and the maintainers will normally ask for them to be broken apart.</p>"},{"location":"add-a-new-datasource/#prerequisites","title":"Prerequisites","text":"<p>If this is the first addition of a data source for a new service, please ensure the Service Client for the new service has been added and merged. See Adding a new Service for details.</p> <p>Determine which version of the AWS SDK for Go the resource will be built upon. For more information and instructions on how to determine this choice, please read AWS SDK for Go Versions</p>"},{"location":"add-a-new-datasource/#steps-to-add-a-data-source","title":"Steps to Add a Data Source","text":""},{"location":"add-a-new-datasource/#fork-the-provider-and-create-a-feature-branch","title":"Fork the Provider and Create a Feature Branch","text":"<p>For a new data source use a branch named <code>f-{datasource name}</code> for example: <code>f-ec2-vpc</code>. See Raising a Pull Request for more details.</p>"},{"location":"add-a-new-datasource/#create-and-name-the-data-source","title":"Create and Name the Data Source","text":"<p>See the Naming Guide for details on how to name the new resource and the resource file. Not following the naming standards will cause extra delay as maintainers request that you make changes.</p> <p>Use the skaff provider scaffolding tool to generate new resource and test templates using your chosen name ensuring you provide the <code>v1</code> flag if you are targeting version 1 of the <code>aws-go-sdk</code>. Doing so will ensure that any boilerplate code, structural best practices and repetitive naming is done for you and always represents our most current standards.</p>"},{"location":"add-a-new-datasource/#fill-out-the-data-source-schema","title":"Fill out the Data Source Schema","text":"<p>In the <code>internal/service/&lt;service&gt;/&lt;service&gt;_data_source.go</code> file you will see a <code>Schema</code> property which exists as a map of <code>Schema</code> objects. This relates the AWS API data model with the Terraform resource itself. For each property you want to make available in Terraform, you will need to add it as an attribute, and choose the correct data type.</p> <p>Attribute names are to specified in <code>snake_case</code> as opposed to the AWS API which is <code>CamelCase</code></p>"},{"location":"add-a-new-datasource/#implement-read-handler","title":"Implement Read Handler","text":"<p>These will map the AWS API response to the data source schema. You will also need to handle different response types (including errors correctly). For complex attributes you will need to implement Flattener or Expander functions. The Data Handling and Conversion Guide covers everything you need to know for mapping AWS API responses to Terraform State and vice-versa. The Error Handling Guide covers everything you need to know about handling AWS API responses consistently.</p>"},{"location":"add-a-new-datasource/#register-data-source-to-the-provider","title":"Register Data Source to the provider","text":"<p>Data Sources use a self registration process that adds them to the provider using the <code>@SDKDataSource()</code> annotation in the datasource's comments. Run <code>make servicepackages</code> to register the datasource. This will add an entry to the <code>service_package_gen.go</code> file located in the service package folder.</p> <pre><code>package something\n\nimport \"github.com/hashicorp/terraform-plugin-sdk/v2/helper/schema\"\n\n// @SDKDataSource(\"aws_something_example\")\nfunc DataSourceExample() *schema.Resource {\n    return &amp;schema.Resource{\n        // some configuration\n    }\n}\n</code></pre>"},{"location":"add-a-new-datasource/#write-passing-acceptance-tests","title":"Write Passing Acceptance Tests","text":"<p>In order to adequately test the data source we will need to write a complete set of Acceptance Tests. You will need an AWS account for this which allows the provider to read to state of the associated resource. See Writing Acceptance Tests for a detailed guide on how to approach these.</p> <p>You will need at minimum:</p> <ul> <li>Basic Test - Tests full lifecycle (CRUD + Import) of a minimal configuration (all required fields, no optional).</li> <li>Disappears Test - Tests what Terraform does if a resource it is tracking can no longer be found.</li> <li>Per Attribute Tests - For each attribute a test should exists which tests that particular attribute in isolation alongside any required fields.</li> </ul>"},{"location":"add-a-new-datasource/#create-documentation-for-the-data-source","title":"Create Documentation for the Data Source","text":"<p>Add a file covering the use of the new data source in <code>website/docs/d/&lt;service&gt;_&lt;name&gt;.md</code>. You may want to also add examples of the data source in use particularly if its use is complex, or relies on resources in another service. This documentation will appear on the Terraform Registry when the data source is made available in a provider release. It is fine to link out to AWS Documentation where appropriate, particularly for values which are likely to change.</p>"},{"location":"add-a-new-datasource/#ensure-format-and-lint-checks-are-passing-locally","title":"Ensure Format and Lint Checks are Passing Locally","text":"<p>Run <code>go fmt</code> to format your code, and install and run all linters to detect and resolve any structural issues with the implementation or documentation.</p> <pre><code>make fmt\nmake tools        # install linters and dependencies\nmake lint         # run provider linters\nmake docs-lint    # run documentation linters\nmake website-lint # run website documentation linters\n</code></pre>"},{"location":"add-a-new-datasource/#raise-a-pull-request","title":"Raise a Pull Request","text":"<p>See Raising a Pull Request.</p>"},{"location":"add-a-new-datasource/#wait-for-prioritization","title":"Wait for Prioritization","text":"<p>In general, pull requests are triaged within a few days of creation and are prioritized based on community reactions. Please view our prioritization guide for full details of the process.</p>"},{"location":"add-a-new-region/","title":"Adding a Newly Released AWS Region","text":"<p>New regions can typically be used immediately with the provider, with two important caveats:</p> <ul> <li>Regions often need to be explicitly enabled via the AWS console. See ap-east-1 launch blog for an example of how to enable a new region for use.</li> <li>Until the provider is aware of the new region, automatic region validation will fail. In order to use the region before validation support is added to the provider you will need to disable region validation by doing the following:</li> </ul> <pre><code>provider \"aws\" {\n  # ... potentially other configuration ...\n\nregion                 = \"me-south-1\"\nskip_region_validation = true\n}\n</code></pre>"},{"location":"add-a-new-region/#enabling-region-validation","title":"Enabling Region Validation","text":"<p>Support for region validation requires that the provider has an updated AWS Go SDK dependency that includes the new region. These are added to the AWS Go SDK <code>aws/endpoints/defaults.go</code> file and generally noted in the AWS Go SDK <code>CHANGELOG</code> as <code>aws/endpoints: Updated Regions</code>. This also needs to be done in the core Terraform binary itself to enable it for the S3 backend. The provider currently takes a dependency on both v1 AND v2 of the AWS Go SDK, as we start to base new (and migrate) resources on v2. Many of the authentication and provider level configuration interactions are also located in the aws-go-sdk-base library. As all of these things take direct dependencies and as a result there ends up being quite a few places these dependency updates need to be made.</p>"},{"location":"add-a-new-region/#update-aws-go-sdk-base","title":"Update aws-go-sdk-base","text":"<p>aws-go-sdk-base</p> <ul> <li>Update aws-go-sdk</li> <li>Update aws-go-sdk-v2</li> </ul>"},{"location":"add-a-new-region/#update-terraform-aws-provider","title":"Update Terraform AWS Provider","text":"<p>provider</p> <ul> <li>Update aws-go-sdk</li> <li>Update aws-go-sdk-v2</li> <li>Update aws-go-sdk-base</li> </ul>"},{"location":"add-a-new-region/#update-terraform-core-s3-backend","title":"Update Terraform Core (S3 Backend)","text":"<p>core</p> <ul> <li>Update aws-go-sdk</li> <li>Update aws-go-sdk-v2</li> <li>Update aws-go-sdk-base</li> </ul> <pre><code>go get github.com/aws/aws-sdk-go@v#.#.#\ngo mod tidy\n</code></pre> <p>See the Changelog Process document for example changelog format.</p>"},{"location":"add-a-new-region/#update-region-specific-values-in-static-data-sources","title":"Update Region Specific values in static Data Sources","text":"<p>Some data sources include static values specific to regions that are not available via a standard AWS API call. These will need to be manually updated. AWS employees can code search previous region values to find new region values in internal packages like RIPStaticConfig if they are not documented yet.</p> <ul> <li>Check Elastic Load Balancing endpoints and quotas and add Route53 Hosted Zone ID if available to <code>internal/service/elb/hosted_zone_id_data_source.go</code> and <code>internal/service/elbv2/hosted_zone_id_data_source.go</code></li> <li>Check Amazon Simple Storage Service endpoints and quotas and add Route53 Hosted Zone ID if available to <code>internal/service/s3/hosted_zones.go</code></li> <li>Check CloudTrail Supported Regions docs and add AWS Account ID if available to <code>internal/service/cloudtrail/service_account_data_source.go</code></li> <li>~~Check Elastic Load Balancing Access Logs docs and add Elastic Load Balancing Account ID if available to <code>internal/service/elb/service_account_data_source.go</code>~~</li> <li>~~Check Redshift Database Audit Logging docs and add AWS Account ID if available to <code>internal/service/redshift/service_account_data_source.go</code>~~</li> <li>Check AWS Elastic Beanstalk endpoints and quotas and add Route53 Hosted Zone ID if available to <code>internal/service/elasticbeanstalk/hosted_zone_data_source.go</code></li> <li>Check SageMaker docs and add AWS Account IDs if available to <code>internal/service/sagemaker/prebuilt_ecr_image_data_source.go</code></li> </ul>"},{"location":"add-a-new-resource/","title":"Adding a New Resource","text":"<p>New resources are required when AWS adds a new service, or adds new features within an existing service which would require a new resource to manage in Terraform. Typically anything with a new set of CRUD API endpoints is a great candidate for a new resource.</p> <p>Each resource should be submitted for review in isolation. Pull requests containing multiple resources are harder to review and the maintainers will normally ask for them to be broken apart.</p>"},{"location":"add-a-new-resource/#prerequisites","title":"Prerequisites","text":"<p>If this is the first resource for a new service, please ensure the Service Client for the new service has been added and merged. See Adding a new Service for details.</p> <p>Determine which version of the AWS SDK for Go the resource will be built upon. For more information and instructions on how to determine this choice, please read AWS SDK for Go Versions</p>"},{"location":"add-a-new-resource/#steps-to-add-a-resource","title":"Steps to Add a Resource","text":""},{"location":"add-a-new-resource/#fork-the-provider-and-create-a-feature-branch","title":"Fork the provider and create a feature branch","text":"<p>For a new resources use a branch named <code>f-{resource name}</code> for example: <code>f-ec2-vpc</code>. See Raising a Pull Request for more details.</p>"},{"location":"add-a-new-resource/#create-and-name-the-resource","title":"Create and Name the Resource","text":"<p>See the Naming Guide for details on how to name the new resource and the resource file. Not following the naming standards will cause extra delay as maintainers request that you make changes.</p> <p>Use the skaff provider scaffolding tool to generate new resource and test templates using your chosen name ensuring you provide the <code>v1</code> flag if you are targeting version 1 of the <code>aws-go-sdk</code>. Doing so will ensure that any boilerplate code, structural best practices and repetitive naming is done for you and always represents our most current standards.</p>"},{"location":"add-a-new-resource/#fill-out-the-resource-schema","title":"Fill out the Resource Schema","text":"<p>In the <code>internal/service/&lt;service&gt;/&lt;service&gt;.go</code> file you will see a <code>Schema</code> property which exists as a map of <code>Schema</code> objects. This relates the AWS API data model with the Terraform resource itself. For each property you want to make available in Terraform, you will need to add it as an attribute, choose the correct data type and supply the correct Schema Behaviors in order to ensure Terraform knows how to correctly handle the value.</p> <p>Typically you will add arguments to represent the values that are under control by Terraform, and attributes in order to supply read-only values as references for Terraform. These are distinguished by Schema Behavior.</p> <p>Attribute names are to specified in <code>camel_case</code> as opposed to the AWS API which is <code>CamelCase</code></p>"},{"location":"add-a-new-resource/#implement-crud-handlers","title":"Implement CRUD handlers","text":"<p>These will map planned Terraform state to the AWS API call, or an AWS API response to an applied Terraform state. You will also need to handle different response types (including errors correctly). For complex attributes you will need to implement Flattener or Expander functions. The Data Handling and Conversion Guide covers everything you need to know for mapping AWS API responses to Terraform State and vice-versa. The Error Handling Guide covers everything you need to know about handling AWS API responses consistently.</p>"},{"location":"add-a-new-resource/#register-resource-to-the-provider","title":"Register Resource to the provider","text":"<p>Resources use a self registration process that adds them to the provider using the <code>@SDKResource()</code> annotation in the resource's comments. Run <code>make servicepackages</code> to register the resource. This will add an entry to the <code>service_package_gen.go</code> file located in the service package folder.</p> <pre><code>package something\n\nimport \"github.com/hashicorp/terraform-plugin-sdk/v2/helper/schema\"\n\n// @SDKResource(\"aws_something_example\")\nfunc ResourceExample() *schema.Resource {\n    return &amp;schema.Resource{\n        // some configuration\n    }\n}\n</code></pre>"},{"location":"add-a-new-resource/#write-passing-acceptance-tests","title":"Write passing Acceptance Tests","text":"<p>In order to adequately test the resource we will need to write a complete set of Acceptance Tests. You will need an AWS account for this which allows the creation of that resource. See Writing Acceptance Tests for a detailed guide on how to approach these.</p> <p>You will need at minimum:</p> <ul> <li>Basic Test - Tests full lifecycle (CRUD + Import) of a minimal configuration (all required fields, no optional).</li> <li>Disappears Test - Tests what Terraform does if a resource it is tracking can no longer be found.</li> <li>Argument Tests - All arguments should be tested in a pragmatic way. Ensure that each argument can be initially set, updated, and cleared, as applicable. Depending on the logic and interaction of arguments, this may take one to several separate tests.</li> </ul>"},{"location":"add-a-new-resource/#create-documentation-for-the-resource","title":"Create documentation for the resource","text":"<p>Add a file covering the use of the new resource in <code>website/docs/r/&lt;service&gt;_&lt;name&gt;.md</code>. Add more examples if it is complex or relies on resources in another service. This documentation will appear on the Terraform Registry when the resource is made available in a provider release. Link to AWS Documentation where appropriate, particularly for values which are likely to change.</p>"},{"location":"add-a-new-resource/#ensure-format-and-lint-checks-are-passing-locally","title":"Ensure format and lint checks are passing locally","text":"<p>Format your code and check linters to detect various issues.</p> <pre><code>make fmt\nmake tools     # install linters and dependencies\nmake lint      # run provider linters\nmake docs-lint # run documentation linters\n</code></pre>"},{"location":"add-a-new-resource/#raise-a-pull-request","title":"Raise a Pull Request","text":"<p>See Raising a Pull Request.</p>"},{"location":"add-a-new-resource/#wait-for-prioritization","title":"Wait for Prioritization","text":"<p>In general, pull requests are triaged within a few days of creation and are prioritized based on community reactions. Please view our Prioritization Guide for full details of the process.</p>"},{"location":"add-a-new-service/","title":"Adding a New AWS Service","text":"<p>AWS frequently launches new services, and Terraform support is frequently desired by the community shortly after launch. Depending on the API surface area of the new service, this could be a major undertaking. The following steps should be followed to prepare for adding the resources that allow for Terraform management of that service.</p>"},{"location":"add-a-new-service/#perform-service-design","title":"Perform Service Design","text":"<p>Before adding a new service to the provider its a good idea to familiarize yourself with the primary workflows practitioners are likely to want to accomplish with the provider to ensure the provider design can solve for this. Its not always necessary to cover 100% of the AWS service offering to unblock most workflows.</p> <p>You should have an idea of what resources and data sources should be added, their dependencies and relative importance in relation to the workflow. This should give you an idea of the order in which resources to be added. It's important to note that generally, we like to review and merge resources in isolation, and avoid combining multiple new resources in one Pull Request.</p> <p>Using the AWS API documentation as a reference, identify the various API's which correspond to the CRUD operations which consist of the management surface for that resource. These will be the set of API's called from the new resource. The API's model attributes will correspond to your resource schema.</p> <p>From there begin to map out the list of resources you would like to implement, and note your plan on the GitHub issue relating to the service (or create one if one does not exist) for the community and maintainers to feedback.</p>"},{"location":"add-a-new-service/#add-a-service-client","title":"Add a Service Client","text":"<p>Before new resources are submitted, please raise a separate pull request containing just the new AWS SDK for Go service client.</p> <p>To add an AWS SDK for Go service client:</p> <ol> <li> <p>Check the file <code>names/names_data.csv</code> for the service.   If it is already there, you are ready to implement the first resource or data source.</p> </li> <li> <p>Otherwise, determine the service identifier using the rule described in the Naming Guide.</p> </li> <li> <p>In <code>names/names_data.csv</code>, add a new line with all the requested information for the service following the guidance in the <code>names</code> README.   Be very careful when adding or changing data in <code>names_data.csv</code>!   The Provider and generators depend on the file being correct.   We strongly recommend using an editor with CSV support.</p> </li> <li> <p>Run the following then submit the pull request:</p> </li> </ol> <pre><code>make gen\nmake test\ngo mod tidy\n</code></pre> <p>Once the service client has been added, implement the first resource or data source in a separate PR.</p>"},{"location":"add-import-support/","title":"Adding Resource Import Support","text":"<p>Adding import support for Terraform resources will allow existing infrastructure to be managed within Terraform. This type of enhancement generally requires a small to moderate amount of code changes.</p> <p>Comprehensive code examples and information about resource import support can be found in the Terraform Plugin SDK v2 documentation.</p> <ul> <li>Resource Code Implementation: In the resource code (e.g., <code>internal/service/{service}/{thing}.go</code>), implementation of <code>Importer</code> <code>State</code> function. When possible, prefer using <code>schema.ImportStatePassthrough</code> as the <code>Importer</code> <code>State</code> function</li> <li>Resource Acceptance Testing Implementation: In the resource acceptance testing (e.g., <code>internal/service/{service}/{thing}_test.go</code>), implementation of <code>TestStep</code>s with <code>ImportState: true</code></li> <li>Resource Documentation Implementation: In the resource documentation (e.g., <code>website/docs/r/service_thing.html.markdown</code>), addition of <code>Import</code> documentation section at the bottom of the page</li> </ul>"},{"location":"adding-a-tag-resource/","title":"Adding a New Tag Resource","text":"<p>Adding a tag resource, similar to the <code>aws_ecs_tag</code> resource, has its own implementation procedure since the resource code and initial acceptance testing functions are automatically generated. The rest of the resource acceptance testing and resource documentation must still be manually created.</p> <ul> <li>In <code>internal/generate</code>: Ensure the service is supported by all generators. Run <code>make gen</code> after any modifications.</li> <li>In <code>internal/service/{service}/generate.go</code>: Add the new <code>//go:generate</code> call with the correct generator directives. Run <code>make gen</code> after any modifications.</li> <li>In <code>internal/provider/provider.go</code>: Add the new resource.</li> <li>Run <code>make test</code> and ensure there are no failures.</li> <li>Create <code>internal/service/{service}/tag_gen_test.go</code> with initial acceptance testing similar to the following (where the parent resource is simple to provision):</li> </ul> <pre><code>import (\n\"fmt\"\n\"testing\"\n\n\"github.com/aws/aws-sdk-go/service/{Service}\"\n\"github.com/hashicorp/terraform-plugin-sdk/v2/helper/acctest\"\n\"github.com/hashicorp/terraform-plugin-sdk/v2/helper/resource\"\n)\n\nfunc TestAcc{Service}Tag_basic(t *testing.T) {\nctx := acctest.Context(t)\nrName := sdkacctest.RandomWithPrefix(acctest.ResourcePrefix)\nresourceName := \"aws_{service}_tag.test\"\n\nresource.ParallelTest(t, resource.TestCase{\nPreCheck:                 func() { acctest.PreCheck(t) },\nErrorCheck:               acctest.ErrorCheck(t, {Service}.EndpointsID),\nProtoV5ProviderFactories: acctest.ProtoV5ProviderFactories,\nCheckDestroy:             testAccCheck{Service}TagDestroy(ctx),\nSteps: []resource.TestStep{\n{\nConfig: testAcc{Service}TagConfig(rName, \"key1\", \"value1\"),\nCheck: resource.ComposeTestCheckFunc(\ntestAccCheck{Service}TagExists(ctx, resourceName),\nresource.TestCheckResourceAttr(resourceName, \"key\", \"key1\"),\nresource.TestCheckResourceAttr(resourceName, \"value\", \"value1\"),\n),\n},\n{\nResourceName:      resourceName,\nImportState:       true,\nImportStateVerify: true,\n},\n},\n})\n}\n\nfunc TestAcc{Service}Tag_disappears(t *testing.T) {\nctx := acctest.Context(t)\nrName := sdkacctest.RandomWithPrefix(acctest.ResourcePrefix)\nresourceName := \"aws_{service}_tag.test\"\n\nresource.ParallelTest(t, resource.TestCase{\nPreCheck:                 func() { acctest.PreCheck(t) },\nErrorCheck:               acctest.ErrorCheck(t, {Service}.EndpointsID),\nProtoV5ProviderFactories: acctest.ProtoV5ProviderFactories,\nCheckDestroy:             testAccCheck{Service}TagDestroy(ctx),\nSteps: []resource.TestStep{\n{\nConfig: testAcc{Service}TagConfig(rName, \"key1\", \"value1\"),\nCheck: resource.ComposeTestCheckFunc(\ntestAccCheck{Service}TagExists(ctx, resourceName),\nacctest.CheckResourceDisappears(ctx, acctest.Provider, resourceAws{Service}Tag(), resourceName),\n),\nExpectNonEmptyPlan: true,\n},\n},\n})\n}\n\nfunc TestAcc{Service}Tag_Value(t *testing.T) {\nctx := acctest.Context(t)\nrName := sdkacctest.RandomWithPrefix(acctest.ResourcePrefix)\nresourceName := \"aws_{service}_tag.test\"\n\nresource.ParallelTest(t, resource.TestCase{\nPreCheck:                 func() { acctest.PreCheck(t) },\nErrorCheck:               acctest.ErrorCheck(t, {Service}.EndpointsID),\nProtoV5ProviderFactories: acctest.ProtoV5ProviderFactories,\nCheckDestroy:             testAccCheck{Service}TagDestroy(ctx),\nSteps: []resource.TestStep{\n{\nConfig: testAcc{Service}TagConfig(rName, \"key1\", \"value1\"),\nCheck: resource.ComposeTestCheckFunc(\ntestAccCheck{Service}TagExists(ctx, resourceName),\nresource.TestCheckResourceAttr(resourceName, \"key\", \"key1\"),\nresource.TestCheckResourceAttr(resourceName, \"value\", \"value1\"),\n),\n},\n{\nResourceName:      resourceName,\nImportState:       true,\nImportStateVerify: true,\n},\n{\nConfig: testAcc{Service}TagConfig(rName, \"key1\", \"value1updated\"),\nCheck: resource.ComposeTestCheckFunc(\ntestAccCheck{Service}TagExists(ctx, resourceName),\nresource.TestCheckResourceAttr(resourceName, \"key\", \"key1\"),\nresource.TestCheckResourceAttr(resourceName, \"value\", \"value1updated\"),\n),\n},\n},\n})\n}\n\nfunc testAcc{Service}TagConfig(rName string, key string, value string) string {\nreturn fmt.Sprintf(`\nresource \"aws_{service}_{thing}\" \"test\" {\n  name = %[1]q\n\n  lifecycle {\n    ignore_changes = [tags]\n  }\n}\n\nresource \"aws_{service}_tag\" \"test\" {\n  resource_arn = aws_{service}_{thing}.test.arn\n  key          = %[2]q\n  value        = %[3]q\n}\n`, rName, key, value)\n}\n</code></pre> <ul> <li>Run <code>make testacc TESTS=TestAcc{Service}Tags_ PKG={Service}</code> and ensure there are no failures.</li> <li>Create <code>website/docs/r/{service}_tag.html.markdown</code> with initial documentation similar to the following:</li> </ul> <pre><code>---\nsubcategory: \"{SERVICE}\"\nlayout: \"aws\"\npage_title: \"AWS: aws_{service}_tag\"\ndescription: |-\n  Manages an individual {SERVICE} resource tag\n---\n\n# Resource: aws_{service}_tag\n\nManages an individual {SERVICE} resource tag. This resource should only be used in cases where {SERVICE} resources are created outside Terraform (e.g., {SERVICE} {THING}s implicitly created by {OTHER SERVICE THING}).\n\n~&gt; **NOTE:** This tagging resource should not be combined with the Terraform resource for managing the parent resource. For example, using `aws_{service}_{thing}` and `aws_{service}_tag` to manage tags of the same {SERVICE} {THING} will cause a perpetual difference where the `aws_{service}_{thing}` resource will try to remove the tag being added by the `aws_{service}_tag` resource.\n\n~&gt; **NOTE:** This tagging resource does not use the [provider `ignore_tags` configuration](/docs/providers/aws/index.html#ignore_tags).\n\n## Example Usage\n\n```terraform\nresource \"aws_{service}_tag\" \"example\" {\nresource_arn = \"...\"\nkey          = \"Name\"\nvalue        = \"Hello World\"\n}\n```\n\n## Argument Reference\n\nThe following arguments are supported:\n\n* `resource_arn` - (Required) ARN of the {SERVICE} resource to tag.\n* `key` - (Required) Tag name.\n* `value` - (Required) Tag value.\n\n## Attributes Reference\n\nIn addition to all arguments above, the following attributes are exported:\n\n* `id` - {SERVICE} resource identifier and key, separated by a comma (`,`)\n\n## Import\n\n`aws_{service}_tag` can be imported by using the {SERVICE} resource identifier and key, separated by a comma (`,`), e.g.\n\n```console\n$ terraform import aws_{service}_tag.example arn:aws:{service}:us-east-1:123456789012:{thing}/example,Name\n```\n</code></pre>"},{"location":"aws-go-sdk-base/","title":"aws-go-sdk-base","text":"<p>https://github.com/hashicorp/aws-sdk-go-base</p> <p>This is a base library used by the AWS Provider, AWSCC Provider and the Terraform S3 Backend to allow them to handle authentication and other non-service level AWS interactions consistently.</p> <p>Typically this changes infrequently and changes are normally performed by HashiCorp maintainers. It should not be necessary to change this library for the majority of provider contributions.</p>"},{"location":"aws-go-sdk-versions/","title":"AWS Go SDK Versions","text":"<p>The Terraform AWS Provider relies on the AWS SDK for Go which is maintained and published by AWS to allow us to safely and securely interact with AWS API's in a consistent fashion. There are two versions of this API, both of which are considered Generally Available and fully supported by AWS at present.</p> <ul> <li>AWS SDKs and Tools maintenance policy</li> <li>AWS SDKs and Tools version support matrix</li> </ul> <p>While the vast majority of the provider is based on the AWS SDK for Go v1, the provider also allows the use of the AWS SDK for Go v2.</p>"},{"location":"aws-go-sdk-versions/#which-sdk-version-should-i-use","title":"Which SDK Version should I use?","text":"<p>Each Terraform provider implementation for an AWS service relies on a service client which in turn is constructed based on a specific SDK version. At present, we are slowly increasing our footprint on SDK v2, but are not actively migrating existing code to use v2. The choice of SDK will be as follows:</p> <p>For new services, you should use AWS SDK for Go v2. AWS has a migration guide that details the differences between the versions of the SDK.</p> <p>For existing services, use the version of the SDK that service currently uses. You can determine this by looking at the <code>import</code> section in the service's Go files.</p>"},{"location":"aws-go-sdk-versions/#what-does-the-sdk-handle","title":"What does the SDK handle?","text":"<p>The AWS SDKs handle calling the various web service interfaces for AWS services. In addition to encoding and decoding the Go structures in the correct JSON or XML payloads, the SDKs handle authentication, request logging, and retrying requests.</p> <p>The various language SDKs and the AWS CLI share a consistent configuration interface, using environment variables and shared configuration and credentials files.</p> <p>The AWS SDKs also automatically retry several common failure cases, such as network errors.</p>"},{"location":"aws-go-sdk-versions/#how-do-the-sdk-versions-differ","title":"How do the SDK versions differ?","text":"<p>The AWS SDK for Go v1.0.0 was released in late 2015, when the current version of Go was v1.5. The Go language has evolved significantly since then. Many currently-recommended practices were not possible at that time, including the use of the <code>context</code> package, introduced in Go v1.7, and error wrapping, introduced in Go v1.13.</p> <p>The AWS SDK for Go v2 uses a modern Go style and has also been modularized, so that individual services are packaged and imported separately.</p> <p>For details on the specific changes to the AWS SDK for Go v2, see Migrating to the AWS SDK for Go v2, especially the Service Clients section.</p>"},{"location":"bugs-and-enhancements/","title":"Making Small Changes to Existing Resources","text":"<p>Most contributions to the provider will take the form of small additions or bug-fixes on existing resources/data sources. In this case the existing resource will give you the best guidance on how the change should be structured, but we require the following to allow the change to be merged:</p> <ul> <li>Acceptance test coverage of new behavior: Existing resources each    have a set of acceptance tests covering their functionality.    These tests should exercise all the behavior of the resource. Whether you are    adding something or fixing a bug, the idea is to have an acceptance test that    fails if your code were to be removed. Sometimes it is sufficient to    \"enhance\" an existing test by adding an assertion or tweaking the config    that is used, but it's often better to add a new test. You can copy/paste an    existing test and follow the conventions you see there, modifying the test    to exercise the behavior of your code.</li> <li>Documentation updates: If your code makes any changes that need to    be documented, you should include those documentation changes    in the same PR. This includes things like new resource attributes or changes in default values.</li> <li>Well-formed Code: Do your best to follow existing conventions you    see in the codebase, and ensure your code is formatted with <code>go fmt</code>.    The PR reviewers can help out on this front, and may provide comments with    suggestions on how to improve the code.</li> <li>Dependency updates: Create a separate PR if you are updating dependencies.    This is to avoid conflicts as version updates tend to be fast-    moving targets. We will plan to merge the PR with this change first.</li> <li>Changelog entry: Assuming the code change affects Terraform operators,    the relevant PR ought to include a user-facing changelog entry    describing the new behavior.</li> </ul>"},{"location":"changelog-process/","title":"Changelog Process","text":"<p>HashiCorp\u2019s open-source projects have always maintained user-friendly, readable CHANGELOG.md that allow users to tell at a glance whether a release should have any effect on them, and to gauge the risk of an upgrade.</p> <p>We use the go-changelog to generate and update the changelog from files created in the <code>.changelog/</code> directory. It is important that when you raise your Pull Request, there is a changelog entry which describes the changes your contribution makes. Not all changes require an entry in the changelog, guidance follows on what changes do.</p>"},{"location":"changelog-process/#changelog-format","title":"Changelog format","text":"<p>The changelog format requires an entry in the following format, where HEADER corresponds to the changelog category, and the entry is the changelog entry itself. The entry should be included in a file in the <code>.changelog</code> directory with the naming convention <code>{PR-NUMBER}.txt</code>. For example, to create a changelog entry for pull request 1234, there should be a file named <code>.changelog/1234.txt</code>.</p> <pre><code>```release-note:{HEADER}\n{ENTRY}\n```\n</code></pre> <p>If a pull request should contain multiple changelog entries, then multiple blocks can be added to the same changelog file. For example:</p> <pre><code>```release-note:note\nresource/aws_example_thing: The `broken` attribute has been deprecated. All configurations using `broken` should be updated to use the new `not_broken` attribute instead.\n```\n\n```release-note:enhancement\nresource/aws_example_thing: Add `not_broken` attribute\n```\n</code></pre>"},{"location":"changelog-process/#pull-request-types-to-changelog","title":"Pull request types to CHANGELOG","text":"<p>The CHANGELOG is intended to show operator-impacting changes to the codebase for a particular version. If every change or commit to the code resulted in an entry, the CHANGELOG would become less useful for operators. The lists below are general guidelines and examples for when a decision needs to be made to decide whether a change should have an entry.</p>"},{"location":"changelog-process/#changes-that-should-have-a-changelog-entry","title":"Changes that should have a CHANGELOG entry","text":""},{"location":"changelog-process/#new-resource","title":"New resource","text":"<p>A new resource entry should only contain the name of the resource, and use the <code>release-note:new-resource</code> header.</p> <pre><code>```release-note:new-resource\naws_secretsmanager_secret_policy\n```\n</code></pre>"},{"location":"changelog-process/#new-data-source","title":"New data source","text":"<p>A new data source entry should only contain the name of the data source, and use the <code>release-note:new-data-source</code> header.</p> <pre><code>```release-note:new-data-source\naws_workspaces_workspace\n```\n</code></pre>"},{"location":"changelog-process/#new-full-length-documentation-guides-eg-eks-getting-started-guide-iam-policy-documents-with-terraform","title":"New full-length documentation guides (e.g., EKS Getting Started Guide, IAM Policy Documents with Terraform)","text":"<p>A new full length documentation entry gives the title of the documentation added, using the <code>release-note:new-guide</code> header.</p> <pre><code>```release-note:new-guide\nCustom Service Endpoint Configuration\n```\n</code></pre>"},{"location":"changelog-process/#resource-and-provider-bug-fixes","title":"Resource and provider bug fixes","text":"<p>A new bug entry should use the <code>release-note:bug</code> header and have a prefix indicating the resource or data source it corresponds to, a colon, then followed by a brief summary. Use a <code>provider</code> prefix for provider level fixes.</p> <pre><code>```release-note:bug\nresource/aws_glue_classifier: Fix quote_symbol being optional\n```\n</code></pre>"},{"location":"changelog-process/#resource-and-provider-enhancements","title":"Resource and provider enhancements","text":"<p>A new enhancement entry should use the <code>release-note:enhancement</code> header and have a prefix indicating the resource or data source it corresponds to, a colon, then followed by a brief summary. Use a <code>provider</code> prefix for provider level enhancements.</p> <pre><code>```release-note:enhancement\nresource/aws_eip: Add network_border_group argument\n```\n</code></pre>"},{"location":"changelog-process/#deprecations","title":"Deprecations","text":"<p>A deprecation entry should use the <code>release-note:note</code> header and have a prefix indicating the resource or data source it corresponds to, a colon, then followed by a brief summary. Use a <code>provider</code> prefix for provider level changes.</p> <pre><code>```release-note:note\nresource/aws_dx_gateway_association: The vpn_gateway_id attribute is being deprecated in favor of the new associated_gateway_id attribute to support transit gateway associations\n```\n</code></pre>"},{"location":"changelog-process/#breaking-changes-and-removals","title":"Breaking changes and removals","text":"<p>A breaking-change entry should use the <code>release-note:breaking-change</code> header and have a prefix indicating the resource or data source it corresponds to, a colon, then followed by a brief summary. Use a <code>provider</code> prefix for provider level changes.</p> <pre><code>```release-note:breaking-change\nresource/aws_lambda_alias: Resource import no longer converts Lambda Function name to ARN\n```\n</code></pre>"},{"location":"changelog-process/#region-validation-support","title":"Region validation support","text":"<pre><code>```release-note:note\nprovider: Region validation now automatically supports the new `XX-XXXXX-#` (Location) region. For AWS operations to work in the new region, the region must be explicitly enabled as outlined in the [AWS Documentation](https://docs.aws.amazon.com/general/latest/gr/rande-manage.html#rande-manage-enable). When the region is not enabled, the Terraform AWS Provider will return errors during credential validation (e.g., `error validating provider credentials: error calling sts:GetCallerIdentity: InvalidClientTokenId: The security token included in the request is invalid`) or AWS operations will throw their own errors (e.g., `data.aws_availability_zones.available: Error fetching Availability Zones: AuthFailure: AWS was not able to validate the provided access credentials`). [GH-####]\n```\n\n```release-note:enhancement\n* provider: Support automatic region validation for `XX-XXXXX-#` [GH-####]\n```\n</code></pre>"},{"location":"changelog-process/#changes-that-may-have-a-changelog-entry","title":"Changes that may have a CHANGELOG entry","text":"<p>Dependency updates: If the update contains relevant bug fixes or enhancements that affect operators, those should be called out. Any changes which do not fit into the above categories but warrant highlighting. Use resource/data source/provider prefixes where appropriate.</p> <pre><code>```release-note:note\nresource/aws_lambda_alias: Resource import no longer converts Lambda Function name to ARN\n```\n</code></pre>"},{"location":"changelog-process/#changes-that-should-not-have-a-changelog-entry","title":"Changes that should not have a CHANGELOG entry","text":"<ul> <li>Resource and provider documentation updates</li> <li>Testing updates</li> <li>Code refactoring</li> </ul>"},{"location":"core-services/","title":"Terraform AWS Provider Core Services","text":"<p>Core Services are AWS services we have identified as critical for a large majority of our users. Our goal is to continually increase the depth of coverage for these services. We will work to prioritize features and enhancements to these services in each weekly release, even if they are not necessarily highlighted in our quarterly roadmap.</p> <p>The core services we have identified are:</p> <ul> <li> <p>EC2</p> </li> <li> <p>Lambda</p> </li> <li> <p>EKS</p> </li> <li> <p>ECS</p> </li> <li> <p>VPC</p> </li> <li> <p>S3</p> </li> <li> <p>RDS</p> </li> <li> <p>DynamoDB</p> </li> <li> <p>IAM</p> </li> <li> <p>Autoscaling (ASG)</p> </li> <li> <p>ElastiCache</p> </li> </ul> <p>We'll continue to evaluate the selected services as our user base grows and changes.</p>"},{"location":"data-handling-and-conversion/","title":"Data Handling and Conversion","text":"<p>The Terraform AWS Provider codebase bridges the implementation of a Terraform Plugin and an AWS API client to support AWS operations and data types as Terraform Resources. Data handling and conversion is a large portion of resource implementation given the domain specific implementations of each side of the provider. The first where Terraform is a generic infrastructure as code tool with a generic data model and the other where the details are driven by AWS API data modeling concepts. This guide is intended to explain and show preferred Terraform AWS Provider code implementations required to successfully translate data between these two systems.</p> <p>At the bottom of this documentation is a Glossary section, which may be a helpful reference while reading the other sections.</p>"},{"location":"data-handling-and-conversion/#data-conversions-in-terraform-providers","title":"Data Conversions in Terraform Providers","text":"<p>Before getting into highly specific documentation about the Terraform AWS Provider handling of data, it may be helpful to briefly highlight how Terraform Plugins (Terraform Providers in this case) interact with Terraform CLI and the Terraform State in general and where this documentation fits into the whole process.</p> <p>There are two primary data flows that are typically handled by resources within a Terraform Provider. Data is either being converted from a planned new Terraform State into making a remote system request or a remote system response is being converted into a applied new Terraform State. The semantics of how the data of the planned new Terraform State is surfaced to the resource implementation is determined by where a resource is in its lifecycle and mainly handled by Terraform CLI. This concept can be explored further in the Terraform Resource Instance Change Lifecycle documentation, with the caveat that some additional behaviors occur within the Terraform Plugin SDK as well (if the Terraform Plugin uses that implementation detail).</p> <p>As a generic walkthrough, the following data handling occurs when creating a Terraform Resource:</p> <ul> <li>An operator creates a Terraform configuration with a new resource defined and runs <code>terraform apply</code></li> <li>Terraform CLI merges an empty prior state for the resource, along with the given configuration state, to create a planned new state for the resource</li> <li>Terraform CLI sends a Terraform Plugin Protocol request to create the new resource with its planned new state data</li> <li>If the Terraform Plugin is using a higher level library, such as the Terraform Plugin SDK, that library receives the request and translates the Terraform Plugin Protocol data types into the expected library types</li> <li>Terraform Plugin invokes the resource creation function with the planned new state data<ul> <li>The planned new state data is converted into an remote system request (e.g., API creation request) that is invoked</li> <li>The remote system response is received and the data is converted into an applied new state</li> </ul> </li> <li>If the Terraform Plugin is using a higher level library, such as the Terraform Plugin SDK, that library translates the library types back into Terraform Plugin Protocol data types</li> <li>Terraform Plugin responds to Terraform Plugin Protocol request with the new state data</li> <li>Terraform CLI verifies and stores the new state</li> </ul> <p>The highlighted lines are the focus of this documentation today. In the future however, the Terraform AWS Provider may replace certain functionality in the items mentioning the Terraform Plugin SDK above to workaround certain limitations of that particular library.</p>"},{"location":"data-handling-and-conversion/#implicit-state-passthrough","title":"Implicit State Passthrough","text":"<p>An important behavior to note with Terraform State handling is if the value of a particular root attribute or block is not refreshed during plan or apply operations, then the prior Terraform State is implicitly deep copied to the new Terraform State for that attribute or block.</p> <p>Given a resource with a writeable root attribute named <code>not_set_attr</code> that never calls <code>d.Set(\"not_set_attr\", /* ... nil or value */)</code>, the following happens:</p> <ul> <li>If the Terraform configuration contains <code>not_set_attr = \"anything\"</code> on resource creation, the Terraform State contains <code>not_set_attr</code> equal to <code>\"anything\"</code> after apply.</li> <li>If the Terraform configuration is updated to <code>not_set_attr = \"updated\"</code>, the Terraform State contains <code>not_set_attr</code> equal to <code>\"updated\"</code> after apply.</li> <li>If the attribute was meant to be associated with a remote system value, it will never update the Terraform State on plan or apply with the remote value. Effectively, it cannot perform drift detection with the remote value.</li> </ul> <p>This however does not apply for nested attributes and blocks if the parent block is refreshed. Given a resource with a root block named <code>parent</code>, nested child attributes named <code>set_attr</code> and <code>not_set_attr</code>, and that calls <code>d.Set(\"parent\", /* ... only refreshes nested set_attr ... */)</code>, the Terraform State for the nested <code>not_set_attr</code> will not be copied.</p> <p>There are valid use cases for passthrough attribute values such as these (see the Virtual Attributes section), however the behavior can be confusing or incorrect for operators if the drift detection is expected. Typically these types of drift detection issues can be discovered by implementing resource import testing with state verification.</p>"},{"location":"data-handling-and-conversion/#data-conversions-in-the-terraform-aws-provider","title":"Data Conversions in the Terraform AWS Provider","text":"<p>To expand on the data handling that occurs specifically within the Terraform AWS Provider resource implementations, the above resource creation items become the below in practice given our current usage of the Terraform Plugin SDK:</p> <ul> <li>The <code>Create</code>/<code>CreateWithoutTimeout</code> function of a <code>schema.Resource</code> is invoked with <code>*schema.ResourceData</code> containing the planned new state data (conventionally named <code>d</code>) and an AWS API client (conventionally named <code>meta</code>).<ul> <li>Note: Before reaching this point, the <code>ResourceData</code> was already translated from the Terraform Plugin Protocol data types by the Terraform Plugin SDK so values can be read by invoking <code>d.Get()</code> and <code>d.GetOk()</code> receiver methods with Attribute and Block names from the <code>Schema</code> of the <code>schema.Resource</code>.</li> </ul> </li> <li>An AWS Go SDK operation input type (e.g., <code>*ec2.CreateVpcInput</code>) is initialized</li> <li>For each necessary field to configure in the operation input type, the data is read from the <code>ResourceData</code> (e.g., <code>d.Get()</code>, <code>d.GetOk()</code>) and converted into the AWS Go SDK type for the field (e.g., <code>*string</code>)</li> <li>The AWS Go SDK operation is invoked and the output type (e.g., <code>*ec2.CreateVpcOutput</code>) is initialized</li> <li>For each necessary Attribute, Block, or resource identifier to be saved in the state, the data is read from the AWS Go SDK type for the field (<code>*string</code>), if necessary converted into a <code>ResourceData</code> compatible type, and saved into a mutated <code>ResourceData</code> (e.g., <code>d.Set()</code>, <code>d.SetId()</code>)</li> <li>Function is returned</li> </ul>"},{"location":"data-handling-and-conversion/#type-mapping","title":"Type Mapping","text":"<p>To further understand the necessary data conversions used throughout the Terraform AWS Provider codebase between AWS Go SDK types and the Terraform Plugin SDK, the following table can be referenced for most scenarios:</p> AWS API Model AWS Go SDK Terraform Plugin SDK Terraform Language/State <code>boolean</code> <code>*bool</code> <code>TypeBool</code> (<code>bool</code>) <code>bool</code> <code>float</code> <code>*float64</code> <code>TypeFloat</code> (<code>float64</code>) <code>number</code> <code>integer</code> <code>*int64</code> <code>TypeInt</code> (<code>int</code>) <code>number</code> <code>list</code> <code>[]*T</code> <code>TypeList</code> (<code>[]interface{}</code> of <code>T</code>)<code>TypeSet</code> (<code>*schema.Set</code> of <code>T</code>) <code>list(any)</code><code>set(any)</code> <code>map</code> <code>map[T1]*T2</code> <code>TypeMap</code> (<code>map[string]interface{}</code>) <code>map(any)</code> <code>string</code> <code>*string</code> <code>TypeString</code> (<code>string</code>) <code>string</code> <code>structure</code> <code>struct</code> <code>TypeList</code> (<code>[]interface{}</code> of <code>map[string]interface{}</code>) with <code>MaxItems: 1</code> <code>list(object(any))</code> <code>timestamp</code> <code>*time.Time</code> <code>TypeString</code> (typically RFC3339 formatted) <code>string</code> <p>You may notice there are type encoding differences the AWS Go SDK and Terraform Plugin SDK:</p> <ul> <li>AWS Go SDK types are all Go pointer types, while Terraform Plugin SDK types are not.</li> <li>AWS Go SDK structures are the Go <code>struct</code> type, while there is no semantically equivalent Terraform Plugin SDK type. Instead they are represented as a slice of interfaces with an underlying map of interfaces.</li> <li>AWS Go SDK types are all Go concrete types, while the Terraform Plugin SDK types for collections and maps are interfaces.</li> <li>AWS Go SDK whole numeric type is always 64-bit, while the Terraform Plugin SDK type is implementation-specific.</li> </ul> <p>Conceptually, the first and second items above the most problematic in the Terraform AWS Provider codebase. The first item because non-pointer types in Go cannot implement the concept of no value (<code>nil</code>). The Zero Value Mapping section will go into more details about the implications of this limitation. The second item because it can be confusing to always handle a structure (\"object\") type as a list.</p> <p>There are efforts to replace the Terraform Plugin type system with one similar the underlying Terraform CLI type system. As these efforts materialize, this documentation will be updated.</p>"},{"location":"data-handling-and-conversion/#zero-value-mapping","title":"Zero Value Mapping","text":"<p>As mentioned in the Type Mapping section, there is a discrepancy with how the Terraform Plugin SDK represents values and the reality that a Terraform State may not configure an Attribute. These values will default to the matching underlying Go type \"zero value\" if not set:</p> Terraform Plugin SDK Go Type Zero Value <code>TypeBool</code> <code>bool</code> <code>false</code> <code>TypeFloat</code> <code>float64</code> <code>0.0</code> <code>TypeInt</code> <code>int</code> <code>0</code> <code>TypeString</code> <code>string</code> <code>\"\"</code> <p>For Terraform resource logic this means that these special values must always be accounted for in implementation. The semantics of the API and its meaning of the zero value will determine whether:</p> <ul> <li>If it is not used/needed, then generally the zero value can safely be used to store an \"unset\" value and should be ignored when sending to the API.</li> <li>If it is used/needed, whether:<ul> <li>A value can always be set and it is safe to always send to the API. Generally, boolean values fall into this category.</li> <li>A different default/sentinel value must be used as the \"unset\" value so it can either match the default of the API or be ignored when sending to the API.</li> <li>A special type implementation is required within the schema to workaround the limitation.</li> </ul> </li> </ul> <p>The maintainers can provide guidance on appropriate solutions for cases not mentioned in the Recommended Implementation section.</p>"},{"location":"data-handling-and-conversion/#root-attributes-versus-block-attributes","title":"Root Attributes Versus Block Attributes","text":"<p>All Attributes and Blocks at the top level of <code>schema.Resource</code> <code>Schema</code> are considered \"root\" attributes. These will always be handled with receiver methods on <code>ResourceData</code>, such as reading with <code>d.Get()</code>, <code>d.GetOk()</code>, etc. and writing with <code>d.Set()</code>. Any nested Attributes and Blocks inside those root Blocks will then be handled with standard Go types according to the table in the Type Mapping section.</p> <p>By convention in the codebase, each level of Block handling beyond root attributes should be separated into \"expand\" functions that convert Terraform Plugin SDK data into the equivalent AWS Go SDK type (typically named <code>expand{Service}{Type}</code>) and \"flatten\" functions that convert an AWS Go SDK type into the equivalent Terraform Plugin SDK data (typically named <code>flatten{Service}{Type}</code>). The Recommended Implementations section will go into those details.</p> <p>NOTE: While it is possible in certain type scenarios to deeply read and write ResourceData information for a Block Attribute, this practice is discouraged in preference of only handling root Attributes and Blocks.</p>"},{"location":"data-handling-and-conversion/#recommended-implementations","title":"Recommended Implementations","text":"<p>Given the various complexities around the Terraform Plugin SDK type system, this section contains recommended implementations for Terraform AWS Provider resource code based on the Type Mapping section and the features of the Terraform Plugin SDK and AWS Go SDK. The eventual goal and styling for many of these recommendations is to ease static analysis of the codebase and future potential code generation efforts.</p> <p>Some of these coding patterns may not be well represented in the codebase, as refactoring the many older styles over years of community development is a large task. However this is meant to represent the preferred implementations today. These will continue to evolve as this codebase and the Terraform Plugin ecosystem changes.</p>"},{"location":"data-handling-and-conversion/#where-to-define-flex-functions","title":"Where to Define Flex Functions","text":"<p>Define FLatten and EXpand (i.e., flex) functions at the most local level possible. This table provides guidance on the preferred place to define flex functions based on usage.</p> Where Used Where to Define Include Service in Name One resource (e.g., <code>aws_instance</code>) Resource file (e.g., <code>internal/service/ec2/instance.go</code>) No Few resources in one service (e.g., <code>EC2</code>) Resource file or service flex file (e.g., <code>internal/service/ec2/flex.go</code>) No Widely used in one service (e.g., <code>EC2</code>) Service flex file (e.g., <code>internal/service/ec2/flex.go</code>) No Two services (e.g., <code>EC2</code> and <code>EKS</code>) Define a copy in each service If helpful 3+ services <code>internal/flex/flex.go</code> Yes"},{"location":"data-handling-and-conversion/#expand-functions-for-blocks","title":"Expand Functions for Blocks","text":"<pre><code>func expandStructure(tfMap map[string]interface{}) *service.Structure {\nif tfMap == nil {\nreturn nil\n}\n\napiObject := &amp;service.Structure{}\n\n// ... nested attribute handling ...\n\nreturn apiObject\n}\n\nfunc expandStructures(tfList []interface{}) []*service.Structure {\nif len(tfList) == 0 {\nreturn nil\n}\n\nvar apiObjects []*service.Structure\n\nfor _, tfMapRaw := range tfList {\ntfMap, ok := tfMapRaw.(map[string]interface{})\n\nif !ok {\ncontinue\n}\n\napiObject := expandStructure(tfMap)\n\nif apiObject == nil {\ncontinue\n}\n\napiObjects = append(apiObjects, apiObject)\n}\n\nreturn apiObjects\n}\n</code></pre>"},{"location":"data-handling-and-conversion/#flatten-functions-for-blocks","title":"Flatten Functions for Blocks","text":"<pre><code>func flattenStructure(apiObject *service.Structure) map[string]interface{} {\nif apiObject == nil {\nreturn nil\n}\n\ntfMap := map[string]interface{}{}\n\n// ... nested attribute handling ...\n\nreturn tfMap\n}\n\nfunc flattenStructures(apiObjects []*service.Structure) []interface{} {\nif len(apiObjects) == 0 {\nreturn nil\n}\n\nvar tfList []interface{}\n\nfor _, apiObject := range apiObjects {\nif apiObject == nil {\ncontinue\n}\n\ntfList = append(tfList, flattenStructure(apiObject))\n}\n\nreturn tfList\n}\n</code></pre>"},{"location":"data-handling-and-conversion/#root-typebool-and-aws-boolean","title":"Root TypeBool and AWS Boolean","text":"<p>To read, if always sending the attribute value is correct:</p> <pre><code>input := service.ExampleOperationInput{\nAttributeName: aws.String(d.Get(\"attribute_name\").(bool))\n}\n</code></pre> <p>Otherwise to read, if only sending the attribute value when <code>true</code> is preferred (<code>!ok</code> for opposite):</p> <pre><code>input := service.ExampleOperationInput{}\n\nif v, ok := d.GetOk(\"attribute_name\"); ok {\ninput.AttributeName = aws.Bool(v.(bool))\n}\n</code></pre> <p>To write:</p> <pre><code>d.Set(\"attribute_name\", output.Thing.AttributeName)\n</code></pre>"},{"location":"data-handling-and-conversion/#root-typefloat-and-aws-float","title":"Root TypeFloat and AWS Float","text":"<p>To read:</p> <pre><code>input := service.ExampleOperationInput{}\n\nif v, ok := d.GetOk(\"attribute_name\"); ok {\ninput.AttributeName = aws.Float64(v.(float64))\n}\n</code></pre> <p>To write:</p> <pre><code>d.Set(\"attribute_name\", output.Thing.AttributeName)\n</code></pre>"},{"location":"data-handling-and-conversion/#root-typeint-and-aws-integer","title":"Root TypeInt and AWS Integer","text":"<p>To read:</p> <pre><code>input := service.ExampleOperationInput{}\n\nif v, ok := d.GetOk(\"attribute_name\"); ok {\ninput.AttributeName = aws.Int64(int64(v.(int)))\n}\n</code></pre> <p>To write:</p> <pre><code>d.Set(\"attribute_name\", output.Thing.AttributeName)\n</code></pre>"},{"location":"data-handling-and-conversion/#root-typelist-of-resource-and-aws-list-of-structure","title":"Root TypeList of Resource and AWS List of Structure","text":"<p>To read:</p> <pre><code>input := service.ExampleOperationInput{}\n\nif v, ok := d.GetOk(\"attribute_name\"); ok &amp;&amp; len(v.([]interface{})) &gt; 0 {\ninput.AttributeName = expandStructures(v.([]interface{}))\n}\n</code></pre> <p>To write:</p> <pre><code>if err := d.Set(\"attribute_name\", flattenStructures(output.Thing.AttributeName)); err != nil {\nreturn fmt.Errorf(\"setting attribute_name: %w\", err)\n}\n</code></pre>"},{"location":"data-handling-and-conversion/#root-typelist-of-resource-and-aws-structure","title":"Root TypeList of Resource and AWS Structure","text":"<p>To read:</p> <pre><code>input := service.ExampleOperationInput{}\n\nif v, ok := d.GetOk(\"attribute_name\"); ok &amp;&amp; len(v.([]interface{})) &gt; 0 &amp;&amp; v.([]interface{})[0] != nil {\ninput.AttributeName = expandStructure(v.([]interface{})[0].(map[string]interface{}))\n}\n</code></pre> <p>To write (likely to have helper function introduced soon):</p> <pre><code>if output.Thing.AttributeName != nil {\nif err := d.Set(\"attribute_name\", []interface{}{flattenStructure(output.Thing.AttributeName)}); err != nil {\nreturn fmt.Errorf(\"setting attribute_name: %w\", err)\n}\n} else {\nd.Set(\"attribute_name\", nil)\n}\n</code></pre>"},{"location":"data-handling-and-conversion/#root-typelist-of-typestring-and-aws-list-of-string","title":"Root TypeList of TypeString and AWS List of String","text":"<p>To read:</p> <pre><code>input := service.ExampleOperationInput{}\n\nif v, ok := d.GetOk(\"attribute_name\"); ok &amp;&amp; len(v.([]interface{})) &gt; 0 {\ninput.AttributeName = flex.ExpandStringList(v.([]interface{}))\n}\n</code></pre> <p>To write:</p> <pre><code>d.Set(\"attribute_name\", aws.StringValueSlice(output.Thing.AttributeName))\n</code></pre>"},{"location":"data-handling-and-conversion/#root-typemap-of-typestring-and-aws-map-of-string","title":"Root TypeMap of TypeString and AWS Map of String","text":"<p>To read:</p> <pre><code>input := service.ExampleOperationInput{}\n\nif v, ok := d.GetOk(\"attribute_name\"); ok &amp;&amp; len(v.(map[string]interface{})) &gt; 0 {\ninput.AttributeName = flex.ExpandStringMap(v.(map[string]interface{}))\n}\n</code></pre> <p>To write:</p> <pre><code>d.Set(\"attribute_name\", aws.StringValueMap(output.Thing.AttributeName))\n</code></pre>"},{"location":"data-handling-and-conversion/#root-typeset-of-resource-and-aws-list-of-structure","title":"Root TypeSet of Resource and AWS List of Structure","text":"<p>To read:</p> <pre><code>input := service.ExampleOperationInput{}\n\nif v, ok := d.GetOk(\"attribute_name\"); ok &amp;&amp; v.(*schema.Set).Len() &gt; 0 {\ninput.AttributeName = expandStructures(v.(*schema.Set).List())\n}\n</code></pre> <p>To write:</p> <pre><code>if err := d.Set(\"attribute_name\", flattenStructures(output.Thing.AttributeNames)); err != nil {\nreturn fmt.Errorf(\"setting attribute_name: %w\", err)\n}\n</code></pre>"},{"location":"data-handling-and-conversion/#root-typeset-of-typestring-and-aws-list-of-string","title":"Root TypeSet of TypeString and AWS List of String","text":"<p>To read:</p> <pre><code>input := service.ExampleOperationInput{}\n\nif v, ok := d.GetOk(\"attribute_name\"); ok &amp;&amp; v.(*schema.Set).Len() &gt; 0 {\ninput.AttributeName = flex.ExpandStringSet(v.(*schema.Set))\n}\n</code></pre> <p>To write:</p> <pre><code>d.Set(\"attribute_name\", aws.StringValueSlice(output.Thing.AttributeName))\n</code></pre>"},{"location":"data-handling-and-conversion/#root-typestring-and-aws-string","title":"Root TypeString and AWS String","text":"<p>To read:</p> <pre><code>input := service.ExampleOperationInput{}\n\nif v, ok := d.GetOk(\"attribute_name\"); ok {\ninput.AttributeName = aws.String(v.(string))\n}\n</code></pre> <p>To write:</p> <pre><code>d.Set(\"attribute_name\", output.Thing.AttributeName)\n</code></pre>"},{"location":"data-handling-and-conversion/#root-typestring-and-aws-timestamp","title":"Root TypeString and AWS Timestamp","text":"<p>To ensure that parsing the read string value does not fail, define <code>attribute_name</code>'s <code>schema.Schema</code> with an appropriate <code>ValidateFunc</code>:</p> <pre><code>\"attribute_name\": {\nType:         schema.TypeString,\n// ...\nValidateFunc: validation.IsRFC3339Time,\n},\n</code></pre> <p>To read:</p> <pre><code>input := service.ExampleOperationInput{}\n\nif v, ok := d.GetOk(\"attribute_name\"); ok {\nv, _ := time.Parse(time.RFC3339, v.(string))\n\ninput.AttributeName = aws.Time(v)\n}\n</code></pre> <p>To write:</p> <pre><code>if output.Thing.AttributeName != nil {\nd.Set(\"attribute_name\", aws.TimeValue(output.Thing.AttributeName).Format(time.RFC3339))\n} else {\nd.Set(\"attribute_name\", nil)\n}\n</code></pre>"},{"location":"data-handling-and-conversion/#nested-typebool-and-aws-boolean","title":"Nested TypeBool and AWS Boolean","text":"<p>To read, if always sending the attribute value is correct:</p> <pre><code>func expandStructure(tfMap map[string]interface{}) *service.Structure {\n// ...\n\nif v, ok := tfMap[\"nested_attribute_name\"].(bool); ok {\napiObject.NestedAttributeName = aws.Bool(v)\n}\n\n// ...\n}\n</code></pre> <p>To read, if only sending the attribute value when <code>true</code> is preferred (<code>!v</code> for opposite):</p> <pre><code>func expandStructure(tfMap map[string]interface{}) *service.Structure {\n// ...\n\nif v, ok := tfMap[\"nested_attribute_name\"].(bool); ok &amp;&amp; v {\napiObject.NestedAttributeName = aws.Bool(v)\n}\n\n// ...\n}\n</code></pre> <p>To write:</p> <pre><code>func flattenStructure(apiObject *service.Structure) map[string]interface{} {\n// ...\n\nif v := apiObject.NestedAttributeName; v != nil {\ntfMap[\"nested_attribute_name\"] = aws.BoolValue(v)\n}\n\n// ...\n}\n</code></pre>"},{"location":"data-handling-and-conversion/#nested-typefloat-and-aws-float","title":"Nested TypeFloat and AWS Float","text":"<p>To read:</p> <pre><code>func expandStructure(tfMap map[string]interface{}) *service.Structure {\n// ...\n\nif v, ok := tfMap[\"nested_attribute_name\"].(float64); ok &amp;&amp; v != 0.0 {\napiObject.NestedAttributeName = aws.Float64(v)\n}\n\n// ...\n}\n</code></pre> <p>To write:</p> <pre><code>func flattenStructure(apiObject *service.Structure) map[string]interface{} {\n// ...\n\nif v := apiObject.NestedAttributeName; v != nil {\ntfMap[\"nested_attribute_name\"] = aws.Float64Value(v)\n}\n\n// ...\n}\n</code></pre>"},{"location":"data-handling-and-conversion/#nested-typeint-and-aws-integer","title":"Nested TypeInt and AWS Integer","text":"<p>To read:</p> <pre><code>func expandStructure(tfMap map[string]interface{}) *service.Structure {\n// ...\n\nif v, ok := tfMap[\"nested_attribute_name\"].(int); ok &amp;&amp; v != 0 {\napiObject.NestedAttributeName = aws.Int64(int64(v))\n}\n\n// ...\n}\n</code></pre> <p>To write:</p> <pre><code>func flattenStructure(apiObject *service.Structure) map[string]interface{} {\n// ...\n\nif v := apiObject.NestedAttributeName; v != nil {\ntfMap[\"nested_attribute_name\"] = aws.Int64Value(v)\n}\n\n// ...\n}\n</code></pre>"},{"location":"data-handling-and-conversion/#nested-typelist-of-resource-and-aws-list-of-structure","title":"Nested TypeList of Resource and AWS List of Structure","text":"<p>To read:</p> <pre><code>func expandStructure(tfMap map[string]interface{}) *service.Structure {\n// ...\n\nif v, ok := tfMap[\"nested_attribute_name\"].([]interface{}); ok &amp;&amp; len(v) &gt; 0 {\napiObject.NestedAttributeName = expandStructures(v)\n}\n\n// ...\n}\n</code></pre> <p>To write:</p> <pre><code>func flattenStructure(apiObject *service.Structure) map[string]interface{} {\n// ...\n\nif v := apiObject.NestedAttributeName; v != nil {\ntfMap[\"nested_attribute_name\"] = flattenNestedStructures(v)\n}\n\n// ...\n}\n</code></pre>"},{"location":"data-handling-and-conversion/#nested-typelist-of-resource-and-aws-structure","title":"Nested TypeList of Resource and AWS Structure","text":"<p>To read:</p> <pre><code>func expandStructure(tfMap map[string]interface{}) *service.Structure {\n// ...\n\nif v, ok := tfMap[\"nested_attribute_name\"].([]interface{}); ok &amp;&amp; len(v) &gt; 0 &amp;&amp; v[0] != nil {\napiObject.NestedAttributeName = expandStructure(v[0].(map[string]interface{}))\n}\n\n// ...\n}\n</code></pre> <p>To write:</p> <pre><code>func flattenStructure(apiObject *service.Structure) map[string]interface{} {\n// ...\n\nif v := apiObject.NestedAttributeName; v != nil {\ntfMap[\"nested_attribute_name\"] = []interface{}{flattenNestedStructure(v)}\n}\n\n// ...\n}\n</code></pre>"},{"location":"data-handling-and-conversion/#nested-typelist-of-typestring-and-aws-list-of-string","title":"Nested TypeList of TypeString and AWS List of String","text":"<p>To read:</p> <pre><code>func expandStructure(tfMap map[string]interface{}) *service.Structure {\n// ...\n\nif v, ok := tfMap[\"nested_attribute_name\"].([]interface{}); ok &amp;&amp; len(v) &gt; 0 {\napiObject.NestedAttributeName = flex.ExpandStringList(v)\n}\n\n// ...\n}\n</code></pre> <p>To write:</p> <pre><code>func flattenStructure(apiObject *service.Structure) map[string]interface{} {\n// ...\n\nif v := apiObject.NestedAttributeName; v != nil {\ntfMap[\"nested_attribute_name\"] = aws.StringValueSlice(v)\n}\n\n// ...\n}\n</code></pre>"},{"location":"data-handling-and-conversion/#nested-typemap-of-typestring-and-aws-map-of-string","title":"Nested TypeMap of TypeString and AWS Map of String","text":"<p>To read:</p> <pre><code>input := service.ExampleOperationInput{}\n\nif v, ok := tfMap[\"nested_attribute_name\"].(map[string]interface{}); ok &amp;&amp; len(v) &gt; 0 {\napiObject.NestedAttributeName = flex.ExpandStringMap(v)\n}\n</code></pre> <p>To write:</p> <pre><code>func flattenStructure(apiObject *service.Structure) map[string]interface{} {\n// ...\n\nif v := apiObject.NestedAttributeName; v != nil {\ntfMap[\"nested_attribute_name\"] = aws.StringValueMap(v)\n}\n\n// ...\n}\n</code></pre>"},{"location":"data-handling-and-conversion/#nested-typeset-of-resource-and-aws-list-of-structure","title":"Nested TypeSet of Resource and AWS List of Structure","text":"<p>To read:</p> <pre><code>func expandStructure(tfMap map[string]interface{}) *service.Structure {\n// ...\n\nif v, ok := tfMap[\"nested_attribute_name\"].(*schema.Set); ok &amp;&amp; v.Len() &gt; 0 {\napiObject.NestedAttributeName = expandStructures(v.List())\n}\n\n// ...\n}\n</code></pre> <p>To write:</p> <pre><code>func flattenStructure(apiObject *service.Structure) map[string]interface{} {\n// ...\n\nif v := apiObject.NestedAttributeName; v != nil {\ntfMap[\"nested_attribute_name\"] = flattenNestedStructures(v)\n}\n\n// ...\n}\n</code></pre>"},{"location":"data-handling-and-conversion/#nested-typeset-of-typestring-and-aws-list-of-string","title":"Nested TypeSet of TypeString and AWS List of String","text":"<p>To read:</p> <pre><code>func expandStructure(tfMap map[string]interface{}) *service.Structure {\n// ...\n\nif v, ok := tfMap[\"nested_attribute_name\"].(*schema.Set); ok &amp;&amp; v.Len() &gt; 0 {\napiObject.NestedAttributeName = flex.ExpandStringSet(v)\n}\n\n// ...\n}\n</code></pre> <p>To write:</p> <pre><code>func flattenStructure(apiObject *service.Structure) map[string]interface{} {\n// ...\n\nif v := apiObject.NestedAttributeName; v != nil {\ntfMap[\"nested_attribute_name\"] = aws.StringValueSlice(v)\n}\n\n// ...\n}\n</code></pre>"},{"location":"data-handling-and-conversion/#nested-typestring-and-aws-string","title":"Nested TypeString and AWS String","text":"<p>To read:</p> <pre><code>func expandStructure(tfMap map[string]interface{}) *service.Structure {\n// ...\n\nif v, ok := tfMap[\"nested_attribute_name\"].(string); ok &amp;&amp; v != \"\" {\napiObject.NestedAttributeName = aws.String(v)\n}\n\n// ...\n}\n</code></pre> <p>To write:</p> <pre><code>func flattenStructure(apiObject *service.Structure) map[string]interface{} {\n// ...\n\nif v := apiObject.NestedAttributeName; v != nil {\ntfMap[\"nested_attribute_name\"] = aws.StringValue(v)\n}\n\n// ...\n}\n</code></pre>"},{"location":"data-handling-and-conversion/#nested-typestring-and-aws-timestamp","title":"Nested TypeString and AWS Timestamp","text":"<p>To ensure that parsing the read string value does not fail, define <code>nested_attribute_name</code>'s <code>schema.Schema</code> with an appropriate <code>ValidateFunc</code>:</p> <pre><code>\"nested_attribute_name\": {\nType:         schema.TypeString,\n// ...\nValidateFunc: validation.IsRFC3339Time,\n},\n</code></pre> <p>To read:</p> <pre><code>func expandStructure(tfMap map[string]interface{}) *service.Structure {\n// ...\n\nif v, ok := tfMap[\"nested_attribute_name\"].(string); ok &amp;&amp; v != \"\" {\nv, _ := time.Parse(time.RFC3339, v)\n\napiObject.NestedAttributeName = aws.Time(v)\n}\n\n// ...\n}\n</code></pre> <p>To write:</p> <pre><code>func flattenStructure(apiObject *service.Structure) map[string]interface{} {\n// ...\n\nif v := apiObject.NestedAttributeName; v != nil {\ntfMap[\"nested_attribute_name\"] = aws.TimeValue(v).Format(time.RFC3339)\n}\n\n// ...\n}\n</code></pre>"},{"location":"data-handling-and-conversion/#further-guidelines","title":"Further Guidelines","text":"<p>This section includes additional topics related to data design and decision making from the Terraform AWS Provider maintainers.</p>"},{"location":"data-handling-and-conversion/#binary-values","title":"Binary Values","text":"<p>Certain resources may need to interact with binary (non UTF-8) data while the Terraform State only supports UTF-8 data. Configurations attempting to pass binary data to an attribute will receive an error from Terraform CLI. These attributes should expect and store the value as a Base64 string while performing any necessary encoding or decoding in the resource logic.</p>"},{"location":"data-handling-and-conversion/#destroy-state-values","title":"Destroy State Values","text":"<p>During resource destroy operations, only previously applied Terraform State values are available to resource logic. Even if the configuration is updated in a manner where both the resource destroy is triggered (e.g., setting the resource meta-argument <code>count = 0</code>) and an attribute value is updated, the resource logic will only have the previously applied data values.</p> <p>Any usage of attribute values during destroy should explicitly note in the resource documentation that the desired value must be applied into the Terraform State before any apply to destroy the resource.</p>"},{"location":"data-handling-and-conversion/#hashed-values","title":"Hashed Values","text":"<p>Attribute values may be very lengthy or potentially contain Sensitive Values. A potential solution might be to use a hashing algorithm, such as MD5 or SHA256, to convert the value before saving in the Terraform State to reduce its relative size or attempt to obfuscate the value. However, there are a few reasons not to do so:</p> <ul> <li>Terraform expects any planned values to match applied values. Ensuring proper handling during the various Terraform operations such as difference planning and Terraform State storage can be a burden.</li> <li>Hashed values are generally unusable in downstream attribute references. If a value is hashed, it cannot be successfully used in another resource or provider configuration that expects the real value.</li> <li>Terraform plan differences are meant to be human readable. If a value is hashed, operators will only see the relatively unhelpful hash differences <code>abc123 -&gt; def456</code> in plans.</li> </ul> <p>Any value hashing implementation will not be accepted. An exception to this guidance is if the remote system explicitly provides a separate hash value in responses, in which a resource can provide a separate attribute with that hashed value.</p>"},{"location":"data-handling-and-conversion/#sensitive-values","title":"Sensitive Values","text":"<p>Marking an Attribute in the Terraform Plugin SDK Schema with <code>Sensitive</code> has the following real world implications:</p> <ul> <li>All occurrences of the Attribute will have the value hidden in plan difference output. In the context of an Attribute within a Block, all Blocks will hide all values of the Attribute.</li> <li>In Terraform CLI 0.14 (with the <code>provider_sensitive_attrs</code> experiment enabled) and later, any downstream references to the value in other configuration will hide the value in plan difference output.</li> </ul> <p>The value is either always hidden or not as the Terraform Plugin SDK does not currently implement conditional support for this functionality. Since Terraform Configurations have no control over the behavior, hiding values from the plan difference can incur a potentially undesirable user experience cost for operators.</p> <p>Given that and especially with the improvements in Terraform CLI 0.14, the Terraform AWS Provider maintainers guiding principles for determining whether an Attribute should be marked as <code>Sensitive</code> is if an Attribute value:</p> <ul> <li>Objectively will always contain a credential, password, or other secret material. Operators can have differing opinions on what constitutes secret material and the maintainers will make best effort determinations, if necessary consulting with the HashiCorp Security team.</li> <li>If the Attribute is within a Block, that all occurrences of the Attribute value will objectively contain secret material. Some APIs (and therefore the Terraform AWS Provider resources) implement generic \"setting\" and \"value\" structures which likely will contain a mixture of secret and non-secret material. These will generally not be accepted for marking as <code>Sensitive</code>.</li> </ul> <p>If you are unsatisfied with sensitive value handling, the maintainers can recommend ensuring there is a covering issue in the Terraform CLI and/or Terraform Plugin SDK projects explaining the use case. Ultimately, Terraform Plugins including the Terraform AWS Provider cannot implement their own sensitive value abilities if the upstream projects do not implement the appropriate functionality.</p>"},{"location":"data-handling-and-conversion/#virtual-attributes","title":"Virtual Attributes","text":"<p>Attributes which only exist within Terraform and not the remote system are typically referred as virtual attributes. Especially in the case of Destroy State Values, these attributes rely on the Implicit State Passthrough behavior of values in Terraform to be available in resource logic. A fictitous example of one of these may be a resource attribute such as a <code>skip_waiting</code> flag, which is used only in the resource logic to skip the typical behavior of waiting for operations to complete.</p> <p>If a virtual attribute has a default value that does not match the Zero Value Mapping for the type, it is recommended to explicitly call <code>d.Set()</code> with the default value in the <code>schema.Resource</code> <code>Importer</code> <code>State</code> function, for example:</p> <pre><code>&amp;schema.Resource{\n// ... other fields ...\nImporter: &amp;schema.ResourceImporter{\nState: func(d *schema.ResourceData, meta interface{}) ([]*schema.ResourceData, error) {\nd.Set(\"skip_waiting\", true)\n\nreturn []*schema.ResourceData{d}, nil\n},\n},\n}\n</code></pre> <p>This helps prevent an immediate plan difference after resource import unless the configuration has a non-default value.</p>"},{"location":"data-handling-and-conversion/#glossary","title":"Glossary","text":"<p>Below is a listing of relevant terms and descriptions for data handling and conversion in the Terraform AWS Provider to establish common conventions throughout this documentation. This list is not exhaustive of all concepts of Terraform Plugins, the Terraform AWS Provider, or the data handling that occurs during Terraform runs, but these should generally provide enough context about the topics discussed here.</p> <ul> <li>AWS Go SDK: Library that converts Go code into AWS Service API compatible operations and data types. Currently refers to version 1 (v1) available since 2015, however version 2 (v2) will reach general availability status soon. Project.</li> <li>AWS Go SDK Model: AWS Go SDK compatible format of AWS Service API Model.</li> <li>AWS Go SDK Service: AWS Service API Go code generated from the AWS Go SDK Model. Generated by the AWS Go SDK code.</li> <li>AWS Service API: Logical boundary of an AWS service by API endpoint. Some large AWS services may be marketed with many different product names under the same service API (e.g., VPC functionality is part of the EC2 API) and vice-versa where some services may be marketed with one product name but are split into multiple service APIs (e.g., Single Sign-On functionality is split into the Identity Store and SSO Admin APIs).</li> <li>AWS Service API Model: Declarative description of the AWS Service API operations and data types. Generated by the AWS service teams. Used to operate the API and generate API clients such as the various AWS Software Development Kits (SDKs).</li> <li>Terraform Language (\"Configuration\"): Configuration syntax interpreted by the Terraform CLI. An implementation of HCL. Full Documentation.</li> <li>Terraform Plugin Protocol: Description of Terraform Plugin operations and data types. Currently based on the Remote Procedure Call (RPC) library <code>gRPC</code>.</li> <li>Terraform Plugin Go: Low-level library that converts Go code into Terraform Plugin Protocol compatible operations and data types. Not currently implemented in the Terraform AWS Provider. Project.</li> <li>Terraform Plugin SDK: High-level library that converts Go code into Terraform Plugin Protocol compatible operations and data types. Project.</li> <li>Terraform Plugin SDK Schema: Declarative description of types and domain specific behaviors for a Terraform provider, including resources and attributes. Full Documentation.</li> <li>Terraform State: Bindings between objects in a remote system (e.g., an EC2 VPC) and a Terraform configuration (e.g., an <code>aws_vpc</code> resource configuration). Full Documentation.</li> </ul> <p>AWS Service API Models use specific terminology to describe data and types:</p> <ul> <li>Enumeration: Collection of valid values for a Shape.</li> <li>Operation: An API call. Includes information about input, output, and error Shapes.</li> <li>Shape: Type description.<ul> <li>boolean: Boolean value.</li> <li>float: Fractional numeric value. May contain value validation such as maximum or minimum.</li> <li>integer: Whole numeric value. May contain value validation such as maximum or minimum.</li> <li>list: Collection that contains member Shapes. May contain value validation such as maximum or minimum keys.</li> <li>map: Grouping of key Shape to value Shape. May contain value validation such as maximum or minimum keys.</li> <li>string: Sequence of characters. May contain value validation such as an enumeration, regular expression pattern, maximum length, or minimum length.</li> <li>structure: Object that contains member Shapes. May represent an error.</li> <li>timestamp: Date and time value.</li> </ul> </li> </ul> <p>The Terraform Language uses the following terminology to describe data and types:</p> <ul> <li>Attribute (\"Argument\"): Assigns a name to a data value.</li> <li>Block (\"Configuration Block\"): Container type for Attributes or Blocks.</li> <li>null: Virtual value equivalent to the Attribute not being set.</li> <li>Types: Full Documentation.<ul> <li>any: Virtual type representing any concrete type in type declarations.</li> <li>bool: Boolean value.</li> <li>list (\"tuple\"): Ordered collection of values.</li> <li>map (\"object\"): Grouping of string keys to values.</li> <li>number: Numeric value. Can be either whole or fractional numbers.</li> <li>set: Unordered collection of values.</li> <li>string: Sequence of characters.</li> </ul> </li> </ul> <p>Terraform Plugin SDK Schemas use the following terminology to describe data and types:</p> <ul> <li>Behaviors: Full Documentation.<ul> <li>Sensitive: Whether the value should be hidden from user interface output.</li> <li>StateFunc: Conversion function between the value set by the Terraform Plugin and the value seen by Terraform Plugin SDK (and ultimately the Terraform State).</li> </ul> </li> <li>Element: Underylying value type for a collection or grouping Schema.</li> <li>Resource Data: Data representation of a Resource Schema. Translation layer between the Schema and Go code of a Terraform Plugin. In the Terraform Plugin SDK, the <code>ResourceData</code> Go type.</li> <li>Resource Schema: Grouping of Schema that represents a Terraform Resource.</li> <li>Schema: Represents an Attribute or Block. Has a Type and Behavior(s).</li> <li>Types: Full Documentation.<ul> <li>TypeBool: Boolean value.</li> <li>TypeFloat: Fractional numeric value.</li> <li>TypeInt: Whole numeric value.</li> <li>TypeList: Ordered collection of values or Blocks.</li> <li>TypeMap: Grouping of key Type to value Type.</li> <li>TypeSet: Unordered collection of values or Blocks.</li> <li>TypeString: Sequence of characters value.</li> </ul> </li> </ul> <p>Some other terms that may be used:</p> <ul> <li>Block Attribute (\"Child Attribute\", \"Nested Attribute\"): Block level Attribute.</li> <li>Expand Function: Function that converts Terraform Plugin SDK data into the equivalent AWS Go SDK type.</li> <li>Flatten Function: Function that converts an AWS Go SDK type into the equivalent Terraform Plugin SDK data.</li> <li>NullableTypeBool: Workaround \"schema type\" created to accept a boolean value that is not configured in addition to true and false. Not implemented in the Terraform Plugin SDK, but uses <code>TypeString</code> (where <code>\"\"</code> represents not configured) and additional validation.</li> <li>NullableTypeFloat: Workaround \"schema type\" created to accept a fractional numeric value that is not configured in addition to <code>0.0</code>. Not implemented in the Terraform Plugin SDK, but uses <code>TypeString</code> (where <code>\"\"</code> represents not configured) and additional validation.</li> <li>NullableTypeInt: Workaround \"schema type\" created to accept a whole numeric value that is not configured in addition to <code>0</code>. Not implemented in the Terraform Plugin SDK, but uses <code>TypeString</code> (where <code>\"\"</code> represents not configured) and additional validation.</li> <li>Root Attribute: Resource top level Attribute or Block.</li> </ul> <p>For additional reference, the Terraform documentation also includes a full glossary of terminology.</p>"},{"location":"dependency-updates/","title":"Dependency Updates","text":"<p>Generally dependency updates are handled by maintainers.</p>"},{"location":"dependency-updates/#go-default-version-update","title":"Go Default Version Update","text":"<p>This project typically upgrades its Go version for development and testing shortly after release to get the latest and greatest Go functionality. Before beginning the update process, ensure that you review the new version release notes to look for any areas of possible friction when updating.</p> <p>Create an issue to cover the update noting down any areas of particular interest or friction.</p> <p>Ensure that the following steps are tracked within the issue and completed within the resulting pull request.</p> <ul> <li>Update go version in <code>go.mod</code></li> <li>Verify <code>make test lint</code> works as expected</li> <li>Verify <code>goreleaser build --snapshot</code> succeeds for all currently supported architectures</li> <li>Verify <code>goenv</code> support for the new version</li> <li>Update <code>docs/development-environment.md</code></li> <li>Update <code>.go-version</code></li> <li>Update <code>CHANGELOG.md</code> detailing the update and mention any notes practitioners need to be aware of.</li> </ul> <p>See #9992 / #10206  for a recent example.</p>"},{"location":"dependency-updates/#aws-go-sdk-updates","title":"AWS Go SDK Updates","text":"<p>Almost exclusively, <code>github.com/aws/aws-sdk-go</code> updates are additive in nature. It is generally safe to only scan through them before approving and merging. If you have any concerns about any of the service client updates such as suspicious code removals in the update, or deprecations introduced, run the acceptance testing for potentially affected resources before merging.</p>"},{"location":"dependency-updates/#authentication-changes","title":"Authentication changes","text":"<p>Occasionally, there will be changes listed in the authentication pieces of the AWS Go SDK codebase, e.g., changes to <code>aws/session</code>. The AWS Go SDK <code>CHANGELOG</code> should include a relevant description of these changes under a heading such as <code>SDK Enhancements</code> or <code>SDK Bug Fixes</code>. If they seem worthy of a callout in the Terraform AWS Provider <code>CHANGELOG</code>, then upon merging we should include a similar message prefixed with the <code>provider</code> subsystem, e.g., <code>* provider: ...</code>.</p> <p>Additionally, if a <code>CHANGELOG</code> addition seemed appropriate, this dependency and version should also be updated in the Terraform S3 Backend, which currently lives in Terraform Core. An example of this can be found with https://github.com/hashicorp/terraform-provider-aws/pull/9305 and https://github.com/hashicorp/terraform/pull/22055.</p>"},{"location":"dependency-updates/#cloudfront-changes","title":"CloudFront changes","text":"<p>CloudFront service client updates have previously caused an issue when a new field introduced in the SDK was not included with Terraform and caused all requests to error (https://github.com/hashicorp/terraform-provider-aws/issues/4091). As a precaution, if you see CloudFront updates, run all the CloudFront resource acceptance testing before merging (<code>TestAccCloudFront</code>).</p>"},{"location":"dependency-updates/#golangci-lint-updates","title":"golangci-lint Updates","text":"<p>Merge if CI passes.</p>"},{"location":"dependency-updates/#terraform-plugin-sdk-updates","title":"Terraform Plugin SDK Updates","text":"<p>Except for trivial changes, run the full acceptance testing suite against the pull request and verify there are no new or unexpected failures.</p>"},{"location":"dependency-updates/#tfproviderdocs-updates","title":"tfproviderdocs Updates","text":"<p>Merge if CI passes.</p>"},{"location":"dependency-updates/#tfproviderlint-updates","title":"tfproviderlint Updates","text":"<p>Merge if CI passes.</p>"},{"location":"dependency-updates/#yamlv2-updates","title":"yaml.v2 Updates","text":"<p>Run the acceptance testing pattern, <code>TestAccCloudFormationStack(_dataSource)?_yaml</code>, and merge if passing.</p>"},{"location":"development-environment/","title":"Development Environment Setup","text":""},{"location":"development-environment/#requirements","title":"Requirements","text":"<ul> <li>Terraform 0.12.26+ (to run acceptance tests)</li> <li>Go 1.19.3+ (to build the provider plugin)</li> </ul>"},{"location":"development-environment/#quick-start","title":"Quick Start","text":"<p>If you wish to work on the provider, you'll first need Go installed on your machine (please check the requirements before proceeding).</p> <p>Note: This project uses Go Modules making it safe to work with it outside of your existing GOPATH. The instructions that follow assume a directory in your home directory outside of the standard GOPATH (i.e <code>$HOME/development/hashicorp/</code>).</p> <p>Clone repository to: <code>$HOME/development/hashicorp/</code></p> <pre><code>$ mkdir -p $HOME/development/hashicorp/; cd $HOME/development/hashicorp/\n$ git clone git@github.com:hashicorp/terraform-provider-aws\n...\n</code></pre> <p>Enter the provider directory and run <code>make tools</code>. This will install the needed tools for the provider.</p> <pre><code>$ make tools\n</code></pre> <p>To compile the provider, run <code>make build</code>. This will build the provider and put the provider binary in the <code>$GOPATH/bin</code> directory.</p> <pre><code>$ make build\n...\n$ $GOPATH/bin/terraform-provider-aws\n...\n</code></pre>"},{"location":"development-environment/#testing-the-provider","title":"Testing the Provider","text":"<p>In order to test the provider, you can run <code>make test</code>.</p> <p>Note: Make sure no <code>AWS_ACCESS_KEY_ID</code> or <code>AWS_SECRET_ACCESS_KEY</code> variables are set, and there's no <code>[default]</code> section in the AWS credentials file <code>~/.aws/credentials</code>.</p> <pre><code>$ make test\n</code></pre> <p>In order to run the full suite of Acceptance tests, run <code>make testacc</code>.</p> <p>Note: Acceptance tests create real resources, and often cost money to run. Please read Running and Writing Acceptance Tests in the contribution guidelines for more information on usage.</p> <pre><code>$ make testacc\n</code></pre>"},{"location":"development-environment/#using-the-provider","title":"Using the Provider","text":"<p>With Terraform v0.14 and later, development overrides for provider developers can be leveraged in order to use the provider built from source.</p> <p>To do this, populate a Terraform CLI configuration file (<code>~/.terraformrc</code> for all platforms other than Windows; <code>terraform.rc</code> in the <code>%APPDATA%</code> directory when using Windows) with at least the following options:</p> <pre><code>provider_installation {\ndev_overrides {\n\"hashicorp/aws\" = \"[REPLACE WITH GOPATH]/bin\"\n}\ndirect {}\n}\n</code></pre>"},{"location":"documentation-changes/","title":"End User Documentation Changes","text":"<p>All practitioner focused documentation is found in the <code>/website</code> folder of the repository.</p> <pre><code>\u251c\u2500\u2500 website/\n    \u251c\u2500\u2500 r/                     # Documentation for resources\n    \u251c\u2500\u2500 d/                     # Documentation for data sources\n    \u251c\u2500\u2500 guides/                # Long format guides for provider level configuration or provider upgrades.\n    \u2514\u2500\u2500 index.html.markdown    # Home page and all provider level documentation.\n\u2514\u2500\u2500 examples/                  # Large example configurations\n</code></pre> <p>For any documentation change please raise a pull request including and adhering to the following:</p> <ul> <li>Reasoning for Change: Documentation updates should include an explanation for why the update is needed. If the change is a correction which is an alignment to AWS behavior, please include a link to the AWS Documentation in the PR.</li> <li>Prefer AWS Documentation: Documentation about AWS service features and valid argument values that are likely to update over time should link to AWS service user guides and API references where possible.</li> <li>Large Example Configurations: Example Terraform configuration that includes multiple resource definitions should be added to the repository <code>examples</code> directory instead of an individual resource documentation page. Each directory under <code>examples</code> should be self-contained to call <code>terraform apply</code> without special configuration.</li> <li>Avoid Terraform Configuration Language Features: Individual resource documentation pages and examples should refrain from highlighting particular Terraform configuration language syntax workarounds or features such as <code>variable</code>, <code>local</code>, <code>count</code>, and built-in functions.</li> </ul>"},{"location":"error-handling/","title":"Error Handling","text":"<p>The Terraform AWS Provider codebase bridges the implementation of a Terraform Plugin and an AWS API client to support AWS operations and data types as Terraform Resources. An important aspect of performing resource and remote actions is properly handling those operations, but those operations are not guaranteed to succeed every time. Some common examples include where network connections are unreliable, necessary permissions are not properly setup, incorrect Terraform configurations, or the remote system responds unexpectedly. All these situations lead to an unexpected workflow action that must be surfaced to the Terraform user interface for operators to troubleshoot. This guide is intended to explain and show various Terraform AWS Provider code implementations that are considered best practice for surfacing these issues properly to operators and code maintainers.</p> <p>For further details about how the AWS SDK for Go v1 and the Terraform AWS Provider resource logic handle retryable errors, see the Retries and Waiters documentation.</p>"},{"location":"error-handling/#general-guidelines-and-helpers","title":"General Guidelines and Helpers","text":""},{"location":"error-handling/#naming-and-check-style","title":"Naming and Check Style","text":"<p>Following typical Go conventions, error variables in the Terraform AWS Provider codebase should be named <code>err</code>, e.g.</p> <pre><code>result, err := strconv.Itoa(\"oh no!\")\n</code></pre> <p>The code that then checks these errors should prefer <code>if</code> conditionals that usually <code>return</code> (or in the case of looping constructs, <code>break</code>/<code>continue</code>) early, especially in the case of multiple error checks, e.g.</p> <pre><code>if /* ... something checking err first ... */ {\n// ... return, break, continue, etc. ...\n}\n\nif err != nil {\n// ... return, break, continue, etc. ...\n}\n\n// all good!\n</code></pre> <p>This is in preference of some other styles of error checking, such as <code>switch</code> conditionals without a condition.</p>"},{"location":"error-handling/#wrap-errors","title":"Wrap Errors","text":"<p>Go implements error wrapping, which means that a deeply nested function call can return a particular error type, while each function up the stack can provide additional error message context without losing the ability to determine the original error. Additional information about this concept can be found on the Go blog entry titled Working with Errors in Go 1.13.</p> <p>For most use cases in this codebase, this means if code is receiving an error and needs to return it, it should implement <code>fmt.Errorf()</code> and the <code>%w</code> verb, e.g.</p> <pre><code>return fmt.Errorf(\"adding some additional message: %w\", err)\n</code></pre> <p>This type of error wrapping should be applied to all Terraform resource logic. It should also be applied to any nested functions that contains two or more error conditions (e.g., a function that calls an update API and waits for the update to finish) so practitioners and code maintainers have a clear idea which generated the error. When returning errors in those situations, it is important to only include necessary additional context. Resource logic will typically include the information such as the type of operation and resource identifier (e.g., <code>updating Service Thing (%s): %w</code>), so these messages can be more terse such as <code>waiting for completion: %w</code>.</p>"},{"location":"error-handling/#aws-sdk-for-go-v1-errors","title":"AWS SDK for Go v1 Errors","text":"<p>The AWS SDK for Go v1 documentation includes a section on handling errors, which is recommended reading.</p> <p>For the purposes of this documentation, the most important concepts with handling these errors are:</p> <ul> <li>Each response error (which eventually implements <code>awserr.Error</code>) has a <code>string</code> error code (<code>Code</code>) and <code>string</code> error message (<code>Message</code>). When printed as a string, they format as: <code>Code: Message</code>, e.g., <code>InvalidParameterValueException: IAM Role arn:aws:iam::123456789012:role/XXX cannot be assumed by AWS Backup</code>.</li> <li>Error handling is almost exclusively done via those <code>string</code> fields and not other response information, such as HTTP Status Codes.</li> <li>When the error code is non-specific, the error message should also be checked. Unfortunately, AWS APIs generally do not provide documentation or API modeling with the contents of these messages and often the Terraform AWS Provider code must rely on substring matching.</li> <li>Not all errors are returned in the response error from an AWS API operation. This is service- and sometimes API-call-specific. For example, the EC2 <code>DeleteVpcEndpoints</code> API call can return a \"successful\" response (in terms of no response error) but include information in an <code>Unsuccessful</code> field in the response body.</li> </ul> <p>When working with AWS SDK for Go v1 errors, it is preferred to use the helpers outlined below and use the <code>%w</code> format verb. Code should generally avoid type assertions with the underlying <code>awserr.Error</code> type or calling its <code>Code()</code>, <code>Error()</code>, <code>Message()</code>, or <code>String()</code> receiver methods. Using the <code>%v</code>, <code>%#v</code>, or <code>%+v</code> format verbs generally provides extraneous information that is not helpful to operators or code maintainers.</p>"},{"location":"error-handling/#aws-sdk-for-go-error-helpers","title":"AWS SDK for Go Error Helpers","text":"<p>To simplify operations with AWS SDK for Go error types, the following helpers are available via the <code>github.com/hashicorp/aws-sdk-go-base/v2/awsv1shim/v2/tfawserr</code> Go package:</p> <ul> <li><code>tfawserr.ErrCodeEquals(err, \"Code\")</code>: Preferred when the error code is specific enough for the check condition. For example, a <code>ResourceNotFoundError</code> code provides enough information that the requested API resource identifier/Amazon Resource Name does not exist.</li> <li><code>tfawserr.ErrMessageContains(err, \"Code\", \"MessageContains\")</code>: Does simple substring matching for the error message.</li> </ul> <p>The recommendation for error message checking is to be just specific enough to capture the anticipated issue, but not include too much matching as the AWS API can change over time without notice. The maintainers have observed changes in wording and capitalization cause unexpected issues in the past.</p> <p>For example, given this error code and message:</p> <pre><code>InvalidParameterValueException: IAM Role arn:aws:iam::123456789012:role/XXX cannot be assumed by AWS Backup\n</code></pre> <p>An error check for this might be:</p> <pre><code>if tfawserr.ErrMessageContains(err, backup.ErrCodeInvalidParameterValueException, \"cannot be assumed\") { /* ... */ }\n</code></pre> <p>The Amazon Resource Name in the error message will be different for every environment and does not add value to the check. The AWS Backup suffix is also extraneous and could change should the service ever rename.</p>"},{"location":"error-handling/#use-aws-sdk-for-go-v1-error-code-constants","title":"Use AWS SDK for Go v1 Error Code Constants","text":"<p>Each AWS SDK for Go v1 service API typically implements common error codes, which get exported as public constants in the SDK. In the AWS SDK for Go v1 API Reference, these can be found in each of the service packages under the <code>Constants</code> section (typically named <code>ErrCode{ExceptionName}</code>).</p> <p>If an AWS SDK for Go service API is missing an error code constant, an AWS Support case should be submitted and a new constant can be added to <code>internal/service/{SERVICE}/errors.go</code> file (created if not present), e.g.</p> <pre><code>const(\nErrCodeInvalidParameterException = \"InvalidParameterException\"\n)\n</code></pre> <p>Then referencing code can use it via:</p> <pre><code>// imports\ntf{SERVICE} \"github.com/hashicorp/terraform-provider-aws/internal/service/{SERVICE}\"\n\n// logic\ntfawserr.ErrCodeEquals(err, tf{SERVICE}.ErrCodeInvalidParameterException)\n</code></pre> <p>e.g.</p> <pre><code>// imports\ntfec2 \"github.com/hashicorp/terraform-provider-aws/internal/service/ec2\"\n\n// logic\ntfawserr.ErrCodeEquals(err, tfec2.ErrCodeInvalidParameterException)\n</code></pre>"},{"location":"error-handling/#terraform-plugin-sdk-types-and-helpers","title":"Terraform Plugin SDK Types and Helpers","text":"<p>The Terraform Plugin SDK includes some error types which are used in certain operations and typically preferred over implementing new types:</p> <ul> <li><code>resource.NotFoundError</code></li> <li><code>resource.TimeoutError</code>: Returned from <code>resource.Retry()</code>, <code>resource.RetryContext()</code>, <code>(resource.StateChangeConf).WaitForState()</code>, and <code>(resource.StateChangeConf).WaitForStateContext()</code></li> </ul> <p>The Terraform AWS Provider codebase implements some additional helpers for working with these in the <code>github.com/hashicorp/terraform-provider-aws/internal/tfresource</code> package:</p> <ul> <li><code>tfresource.NotFound(err)</code>: Returns true if the error is a <code>resource.NotFoundError</code>.</li> <li><code>tfresource.TimedOut(err)</code>: Returns true if the error is a <code>resource.TimeoutError</code> and contains no <code>LastError</code>. This typically signifies that the retry logic was never signaled for a retry, which can happen when AWS API operations are automatically retrying before returning.</li> </ul>"},{"location":"error-handling/#resource-lifecycle-guidelines","title":"Resource Lifecycle Guidelines","text":"<p>Terraform CLI and the Terraform Plugin SDK have certain expectations and automatic behaviors depending on the lifecycle operation of a resource. This section highlights some common issues that can occur and their expected resolution.</p>"},{"location":"error-handling/#resource-creation","title":"Resource Creation","text":"<p>Invoked in the resource via the <code>schema.Resource</code> type <code>Create</code>/<code>CreateWithoutTimeout</code> function.</p>"},{"location":"error-handling/#disnewresource-checks","title":"d.IsNewResource() Checks","text":"<p>During resource creation, Terraform CLI expects either a properly applied state for the new resource or an error. To signal proper resource existence, the Terraform Plugin SDK uses an underlying resource identifier (set via <code>d.SetId(/* some value */)</code>). If for some reason the resource creation is returned without an error, but also without the resource identifier being set, Terraform CLI will return an error such as:</p> <pre><code>Error: Provider produced inconsistent result after apply\n\nWhen applying changes to aws_sns_topic_subscription.sqs,\nprovider \"registry.terraform.io/hashicorp/aws\" produced an unexpected new\nvalue: Root resource was present, but now absent.\n\nThis is a bug in the provider, which should be reported in the provider's own\nissue tracker.\n</code></pre> <p>A typical pattern in resource implementations in the <code>Create</code>/<code>CreateWithoutTimeout</code> function is to <code>return</code> the <code>Read</code>/<code>ReadWithoutTimeout</code> function at the end to fill in the Terraform State for all attributes. Another typical pattern in resource implementations in the <code>Read</code>/<code>ReadWithoutTimeout</code> function is to remove the resource from the Terraform State if the remote system returns an error or status that indicates the remote resource no longer exists by explicitly calling <code>d.SetId(\"\")</code> and returning no error. If the remote system is not strongly read-after-write consistent (eventually consistent), this means the resource creation can return no error and also return no resource state.</p> <p>To prevent this type of Terraform CLI error, the resource implementation should also check against <code>d.IsNewResource()</code> before removing from the Terraform State and returning no error. If that check is <code>true</code>, then remote operation error (or one synthesized from the non-existent status) should be returned instead. While adding this check will not fix the resource implementation to handle the eventually consistent nature of the remote system, the error being returned will be less opaque for operators and code maintainers to troubleshoot.</p> <p>In the Terraform AWS Provider, an initial fix for the Terraform CLI error will typically look like:</p> <pre><code>func resourceServiceThingCreate(d *schema.ResourceData, meta interface{}) error {\n/* ... */\n\nreturn resourceServiceThingRead(d, meta)\n}\n\nfunc resourceServiceThingRead(d *schema.ResourceData, meta interface{}) error {\n/* ... */\n\noutput, err := conn.DescribeServiceThing(input)\n\nif !d.IsNewResource() &amp;&amp; tfawserr.ErrCodeEquals(err, \"ResourceNotFoundException\") {\nlog.Printf(\"[WARN] {Service} {Thing} (%s) not found, removing from state\", d.Id())\nd.SetId(\"\")\nreturn nil\n}\n\nif err != nil {\nreturn fmt.Errorf(\"reading {Service} {Thing} (%s): %w\", d.Id(), err)\n}\n\n/* ... */\n}\n</code></pre> <p>If the remote system is not strongly read-after-write consistent, see the Retries and Waiters documentation on Resource Lifecycle Retries for how to prevent consistency-type errors.</p>"},{"location":"error-handling/#creation-error-message-context","title":"Creation Error Message Context","text":"<p>Returning errors during creation should include additional messaging about the location or cause of the error for operators and code maintainers by wrapping with <code>fmt.Errorf()</code>:</p> <pre><code>if err != nil {\nreturn fmt.Errorf(\"creating {SERVICE} {THING}: %w\", err)\n}\n</code></pre> <p>e.g.</p> <pre><code>if err != nil {\nreturn fmt.Errorf(\"creating EC2 VPC: %w\", err)\n}\n</code></pre> <p>Code that also uses waiters or other operations that return errors should follow a similar pattern, including the resource identifier since it has typically been set before this execution:</p> <pre><code>if _, err := VpcAvailable(conn, d.Id()); err != nil {\nreturn fmt.Errorf(\"waiting for EC2 VPC (%s) availability: %w\", d.Id(), err)\n}\n</code></pre>"},{"location":"error-handling/#resource-deletion","title":"Resource Deletion","text":"<p>Invoked in the resource via the <code>schema.Resource</code> type <code>Delete</code>/<code>DeleteWithoutTimeout</code> function.</p>"},{"location":"error-handling/#resource-already-deleted","title":"Resource Already Deleted","text":"<p>A typical pattern for resource deletion is to immediately perform the remote system deletion operation without checking existence. This is generally acceptable as operators are encouraged to always refresh their Terraform State prior to performing changes. However in certain scenarios, such as external systems modifying the remote system prior to the Terraform execution, it is certainly still possible that the remote system will return an error signifying that remote resource does not exist. In these cases, resources should implement logic that catches the error and returns no error.</p> <p>NOTE: The Terraform Plugin SDK automatically handles the equivalent of d.SetId(\"\") on deletion, so it is not necessary to include it.</p> <p>For example in the Terraform AWS Provider:</p> <pre><code>func resourceServiceThingDelete(d *schema.ResourceData, meta interface{}) error {\n/* ... */\n\noutput, err := conn.DeleteServiceThing(input)\n\nif tfawserr.ErrCodeEquals(err, \"ResourceNotFoundException\") {\nreturn nil\n}\n\nif err != nil {\nreturn fmt.Errorf(\"deleting {Service} {Thing} (%s): %w\", d.Id(), err)\n}\n\n/* ... */\n}\n</code></pre>"},{"location":"error-handling/#deletion-error-message-context","title":"Deletion Error Message Context","text":"<p>Returning errors during deletion should include the resource identifier and additional messaging about the location or cause of the error for operators and code maintainers by wrapping with <code>fmt.Errorf()</code>:</p> <pre><code>if err != nil {\nreturn fmt.Errorf(\"deleting {SERVICE} {THING} (%s): %w\", d.Id(), err)\n}\n</code></pre> <p>e.g.</p> <pre><code>if err != nil {\nreturn fmt.Errorf(\"deleting EC2 VPC (%s): %w\", d.Id(), err)\n}\n</code></pre> <p>Code that also uses waiters or other operations that return errors should follow a similar pattern:</p> <pre><code>if _, err := VpcDeleted(conn, d.Id()); err != nil {\nreturn fmt.Errorf(\"waiting for EC2 VPC (%s) deletion: %w\", d.Id(), err)\n}\n</code></pre>"},{"location":"error-handling/#resource-read","title":"Resource Read","text":"<p>Invoked in the resource via the <code>schema.Resource</code> type <code>Read</code>/<code>ReadWithoutTimeout</code> function.</p>"},{"location":"error-handling/#singular-data-source-errors","title":"Singular Data Source Errors","text":"<p>A data source which is expected to return Terraform State about a single remote resource is commonly referred to as a \"singular\" data source. Implementation-wise, it may use any available describe or listing functionality from the remote system to retrieve the information. In addition to any remote operation and other data handling errors that should be returned, these two additional cases should be covered:</p> <ul> <li>Returning an error when zero results are found.</li> <li>Returning an error when multiple results are found.</li> </ul> <p>For remote operations that are designed to return an error when the remote resource is not found, this error is typically just passed through similar to other remote operation errors. For remote operations that are designed to return a successful result whether there is zero, one, or multiple multiple results the error must be generated.</p> <p>For example in pseudo-code:</p> <pre><code>output, err := conn.ListServiceThings(input)\n\nif err != nil {\nreturn fmt.Errorf(\"listing {Service} {Thing}s: %w\", err)\n}\n\nif output == nil || len(output.Results) == 0 {\nreturn fmt.Errorf(\"no {Service} {Thing} found matching criteria; try different search\")\n}\n\nif len(output.Results) &gt; 1 {\nreturn fmt.Errorf(\"multiple {Service} {Thing} found matching criteria; try different search\")\n}\n</code></pre>"},{"location":"error-handling/#plural-data-source-errors","title":"Plural Data Source Errors","text":"<p>An emergent concept is a data source that returns multiple results, acting similar to any available listing functionality available from the remote system. These types of data sources should return no error if zero results are returned and no error if multiple results are found. Remote operation and other data handling errors should still be returned.</p>"},{"location":"error-handling/#read-error-message-context","title":"Read Error Message Context","text":"<p>Returning errors during read should include the resource identifier (for managed resources) and additional messaging about the location or cause of the error for operators and code maintainers by wrapping with <code>fmt.Errorf()</code>:</p> <pre><code>if err != nil {\nreturn fmt.Errorf(\"reading {SERVICE} {THING} (%s): %w\", d.Id(), err)\n}\n</code></pre> <p>e.g.</p> <pre><code>if err != nil {\nreturn fmt.Errorf(\"reading EC2 VPC (%s): %w\", d.Id(), err)\n}\n</code></pre>"},{"location":"error-handling/#resource-update","title":"Resource Update","text":"<p>Invoked in the resource via the <code>schema.Resource</code> type <code>Update</code>/<code>UpdateWithoutTimeout</code> function.</p>"},{"location":"error-handling/#update-error-message-context","title":"Update Error Message Context","text":"<p>Returning errors during update should include the resource identifier and additional messaging about the location or cause of the error for operators and code maintainers by wrapping with <code>fmt.Errorf()</code>:</p> <pre><code>if err != nil {\nreturn fmt.Errorf(\"updating {SERVICE} {THING} (%s): %w\", d.Id(), err)\n}\n</code></pre> <p>e.g.</p> <pre><code>if err != nil {\nreturn fmt.Errorf(\"updating EC2 VPC (%s): %w\", d.Id(), err)\n}\n</code></pre> <p>Code that also uses waiters or other operations that return errors should follow a similar pattern:</p> <pre><code>if _, err := VpcAvailable(conn, d.Id()); err != nil {\nreturn fmt.Errorf(\"waiting for EC2 VPC (%s) update: %w\", d.Id(), err)\n}\n</code></pre>"},{"location":"faq/","title":"Frequently Asked Questions","text":""},{"location":"faq/#who-are-the-maintainers","title":"Who are the maintainers?","text":"<p>The HashiCorp Terraform AWS provider team is :</p> <ul> <li>Marc Cosentino, Product Manager - GitHub @marcosentino</li> <li>Simon Davis, Engineering Manager - GitHub @breathingdust</li> <li>Justin Retzolk, Technical Community Manager - GitHub @justinretzolk</li> <li>Adrian Johnson, Engineer - GitHub @johnsonaj</li> <li>Dirk Avery, Engineer - GitHub @YakDriver</li> <li>Graham Davison, Engineer - GitHub @gdavison</li> <li>Jared Baker, Engineer - GitHub @jar-b</li> <li>Kerim Satirli, Developer Advocate - GitHub @ksatirli</li> <li>Kit Ewbank, Engineer - GitHub @ewbankkit</li> </ul>"},{"location":"faq/#why-isnt-my-pr-merged-yet","title":"Why isn\u2019t my PR merged yet?","text":"<p>Unfortunately, due to the volume of issues and new pull requests we receive, we are unable to give each one the full attention that we would like. We always focus on the contributions that provide the greatest value to the most community members. For more information on how we prioritize pull requests, see the prioritization guide.</p>"},{"location":"faq/#how-do-you-decide-what-gets-merged-for-each-release","title":"How do you decide what gets merged for each release?","text":"<p>We have a large backlog of pull requests to get through and the team are moving through them as quick as we can. All pull requests must be reviewed by a HashiCorp engineer before inclusion. This is to ensure that the design of the addition fits with what provider users have come to expect, and to ensure that testing and best practices are adhered to. This is particularly important for such a large codebase, to ensure that we sustain its maintainability as its grows.</p> <p>The number one factor we look at when deciding what issues to look at are your \ud83d\udc4d reactions to the original issue/PR description as these can be easily discovered. Comments that further explain desired use cases or poor user experience are also heavily factored. The items with the most support are always on our radar, and we commit to keep the community updated on their status and potential timelines.</p> <p>We publish a roadmap every quarter which describes major themes or specific product areas of focus. What is excluded from the public roadmap is work performed under NDA with AWS on new services, and any ad-hoc work we pick up during the quarter. This ad-hoc work can be responding to bugs, gardening day activity, customer prioritization, and technical debt items.</p> <p>We also are investing time to improve the contributing experience by improving documentation, adding more linter coverage to ensure that incoming PR's can be in as good shape as possible. This will allow us to get through them quicker.</p>"},{"location":"faq/#my-pr-hasnt-been-merged-and-it-now-has-merge-conflictsfailed-checks-should-i-keep-it-up-to-date","title":"My PR hasn't been merged and it now has merge conflicts/failed checks, should I keep it up to date?","text":"<p>We realize that sometimes pull requests sit for a considerable amount of time without being addressed. During this time period they may accumulate merge conflicts and failed linter checks as the provider codebase moves forward. As maintainers we have no expectation that you keep your PR up to date, these issues will be addressed at review time most often by the maintainers themselves. Obviously we would hope that your PR is mergeable when first raised! The mergeability of the PR does not affect its prioritization for review.</p>"},{"location":"faq/#how-often-do-you-release","title":"How often do you release?","text":"<p>We release weekly on Thursday. We release often to ensure we can bring value to the community at a frequent cadence and to ensure we are in a good place to react to AWS region launches and service announcements.</p>"},{"location":"faq/#backward-compatibility-promise","title":"Backward Compatibility Promise","text":"<p>Our policy is described on the Terraform website here. While we do our best to prevent breaking changes until major version releases of the provider, it is generally recommended to pin the provider version in your configuration.</p> <p>Due to the constant release pace of AWS and the relatively infrequent major version releases of the provider, there can be cases where a minor version update may contain unexpected changes depending on your configuration or environment. These may include items such as a resource requiring additional IAM permissions to support newer functionality. We typically base these decisions on a pragmatic compromise between introducing a relatively minor one-time inconvenience for a subset of the community versus better overall user experience for the entire community.</p>"},{"location":"faq/#once-a-major-release-is-published-will-new-features-and-fixes-be-backported-to-previous-versions","title":"Once a major release is published, will new features and fixes be backported to previous versions?","text":"<p>Generally new features and fixes will only be added to the most recent major version. Due to the high touch nature of provider development and the extensive regression testing required to ensure stability, maintaining multiple versions of the provider is not sustainable at this time. An exception to this could be a discovered security vulnerability for which backporting may be the most reasonable course of action. These would be reviewed on a case by case basis.</p>"},{"location":"faq/#aws-just-announced-a-new-region-when-will-i-see-it-in-the-provider","title":"AWS just announced a new region, when will I see it in the provider.","text":"<p>Normally pretty quickly. We usually see the region appear within the <code>aws-go-sdk</code> within a couple days of the announcement. Depending on when it lands, we can often get it out within the current or following weekly release. Comparatively, adding support for a new  region in the S3 backend can take a little longer, as it is shipped as part of Terraform Core and not via the AWS Provider.</p> <p>Please note that this new region requires a manual process to enable in your account. Once enabled in the console, it takes a few minutes for everything to work properly.</p> <p>If the region is not enabled properly, or the enablement process is still in progress, you may receive errors like these:</p> <pre><code>$ terraform apply\n\nError: error validating provider credentials: error calling sts:GetCallerIdentity: InvalidClientTokenId: The security token included in the request is invalid.\n    status code: 403, request id: 142f947b-b2c3-11e9-9959-c11ab17bcc63\n\n  on main.tf line 1, in provider \"aws\":\n   1: provider \"aws\" {\n</code></pre> <p>To use this new region before support has been added to the Terraform AWS Provider, you can disable the provider's automatic region validation via:</p> <pre><code>provider \"aws\" {\n  # ... potentially other configuration ...\n\nregion                 = \"af-south-1\"\nskip_region_validation = true\n}\n</code></pre>"},{"location":"faq/#how-can-i-help","title":"How can I help?","text":"<p>Great question, if you have contributed before check out issues with the <code>help-wanted</code> label. These are normally enhancement issues that will have a great impact, but the maintainers are unable to develop them in the near future. If you are just getting started, take a look at issues with the <code>good-first-issue</code> label. Items with these labels will always be given priority for response.</p> <p>Check out the Contributing Guide for additional information.</p>"},{"location":"faq/#how-can-i-become-a-maintainer","title":"How can I become a maintainer?","text":"<p>This is an area under active research. Stay tuned!</p>"},{"location":"issue-reporting-and-lifecycle/","title":"Issue Reporting and Lifecycle","text":""},{"location":"issue-reporting-and-lifecycle/#issue-reporting-checklists","title":"Issue Reporting Checklists","text":"<p>We welcome issues of all kinds including feature requests, bug reports, and general questions. Below you'll find checklists with guidelines for well-formed issues of each type.</p> <p>We encourage opening new issues rather than commenting on closed issues if a problem has not been completely solved or causes a regression. This ensures we are able to triage it effectively.</p>"},{"location":"issue-reporting-and-lifecycle/#bug-reports","title":"Bug Reports","text":"<ul> <li> <p>Test against latest release: Make sure you test against the latest    released version. It is possible we already fixed the bug you're experiencing.</p> </li> <li> <p>Search for possible duplicate reports: It's helpful to keep bug    reports consolidated to one thread, so do a quick search on existing bug    reports to check if anybody else has reported the same thing. You can scope       searches by the label \"bug\" to help narrow things down.</p> </li> <li> <p>Include steps to reproduce: Provide steps to reproduce the issue,    along with your <code>.tf</code> files, with secrets removed, so we can try to    reproduce it. Without this, it makes it much harder to fix the issue.</p> </li> <li> <p>For panics, include <code>crash.log</code>: If you experienced a panic, please    create a gist of the entire generated crash log    for us to look at. Double check no sensitive items were in the log.</p> </li> </ul>"},{"location":"issue-reporting-and-lifecycle/#feature-requests","title":"Feature Requests","text":"<ul> <li> <p>Search for possible duplicate requests: It's helpful to keep requests    consolidated to one thread, so do a quick search on existing requests to    check if anybody else has reported the same thing. You can scope searches by       the label \"enhancement\" to help narrow things down.</p> </li> <li> <p>Include a use case description: In addition to describing the    behavior of the feature you'd like to see added, it's helpful to also lay    out the reason why the feature would be important and how it would benefit    Terraform users.</p> </li> </ul>"},{"location":"issue-reporting-and-lifecycle/#questions","title":"Questions","text":"<ul> <li>Search for answers in Terraform documentation: We're happy to answer    questions in GitHub Issues, but it helps reduce issue churn and maintainer    workload if you work to find answers to common questions in the    documentation. Oftentimes Question issues result in documentation updates    to help future users, so if you don't find an answer, you can give us    pointers for where you'd expect to see it in the docs.</li> </ul>"},{"location":"issue-reporting-and-lifecycle/#issue-lifecycle","title":"Issue Lifecycle","text":"<p>Note: For detailed information on how issues are prioritized, see the prioritization guide.</p> <ol> <li> <p>The issue is reported.</p> </li> <li> <p>The issue is verified and categorized by a Terraform collaborator.    Categorization is done via GitHub labels. We generally use a two-label    system of (1) issue/PR type, and (2) section of the codebase. Type is    one of \"bug\", \"enhancement\", \"documentation\", or \"question\", and section    is usually the AWS service name.</p> </li> <li> <p>An initial triage process determines whether the issue is critical and must    be addressed immediately, or can be left open for community discussion.</p> </li> <li> <p>The issue is addressed in a pull request or commit. The issue number will be    referenced in the commit message so that the code that fixes it is clearly    linked.</p> </li> <li> <p>The issue is closed. Sometimes, valid issues will be closed because they are    tracked elsewhere or non-actionable. The issue is still indexed and    available for future viewers, or can be re-opened if necessary.</p> </li> <li> <p>30 days after the issue has been closed it is locked, preventing further comments.</p> </li> </ol>"},{"location":"naming/","title":"Naming Conventions for the AWS Provider","text":""},{"location":"naming/#service-identifier","title":"Service Identifier","text":"<p>In the AWS Provider, a service identifier should consistently identify an AWS service from code to documentation to provider use by a practitioner. Prominent places you will see service identifiers:</p> <ul> <li>The package name (e.g., <code>internal/service/&lt;serviceidentifier&gt;</code>)</li> <li>In resource and data source names (e.g., <code>aws_&lt;serviceidentifier&gt;_thing</code>)</li> <li>Documentation file names (e.g., <code>website/docs/r/&lt;serviceidentifier&gt;_thing</code>)</li> </ul> <p>Typically, choosing the AWS Provider identifier for a service is simple. AWS consistently uses one name and we use that name as the identifier. However, some services are not simple. To provide consistency, and to help contributors and practitioners know what to expect, we provide this rule for defining a service identifier:</p>"},{"location":"naming/#rule","title":"Rule","text":"<ol> <li>Determine the service package name for AWS Go SDK v2.</li> <li>Determine the AWS CLI v2 command corresponding to the service (i.e., the word following <code>aws</code> in CLI commands; e.g., for <code>aws sts get-caller-identity</code>, <code>sts</code> is the command, <code>get-caller-identity</code> is the subcommand).</li> <li>If the SDK and CLI agree, use that. If the service only exists in one, use that.</li> <li>If they differ, use the shorter of the two.</li> <li>Use lowercase letters and do not include any underscores (<code>_</code>).</li> </ol>"},{"location":"naming/#how-well-is-it-followed","title":"How Well Is It Followed?","text":"<p>With 156+ services having some level of implementation, the following is a summary of how well this rule is currently followed.</p> <p>For AWS provider service package names, only five packages violate this rule: <code>appautoscaling</code> should be <code>applicationautoscaling</code>, <code>codedeploy</code> should be <code>deploy</code>, <code>elasticsearch</code> should be <code>es</code>, <code>cloudwatchlogs</code> should be <code>logs</code>, and <code>simpledb</code> should be <code>sdb</code>.</p> <p>For the service identifiers used in resource and data source configuration names (e.g., <code>aws_acmpca_certificate_authority</code>), 32 wholly or partially violate the rule.</p> <ul> <li>EC2, ELB, ELBv2, and RDS have legacy but heavily used resources and data sources that do not or inconsistently use service identifiers.</li> <li>The remaining 28 services violate the rule in a consistent way: <code>appautoscaling</code> should be <code>applicationautoscaling</code>, <code>codedeploy</code> should be <code>deploy</code>, <code>elasticsearch</code> should be <code>es</code>, <code>cloudwatch_log</code> should be <code>logs</code>, <code>simpledb</code> should be <code>sdb</code>, <code>prometheus</code> should be <code>amp</code>, <code>api_gateway</code> should be <code>apigateway</code>, <code>cloudcontrolapi</code> should be <code>cloudcontrol</code>, <code>cognito_identity</code> should be <code>cognitoidentity</code>, <code>cognito</code> should be <code>cognitoidp</code>, <code>config</code> should be <code>configservice</code>, <code>dx</code> should be <code>directconnect</code>, <code>directory_service</code> should be <code>ds</code>, <code>elastic_beanstalk</code> should be <code>elasticbeanstalk</code>, <code>cloudwatch_event</code> should be <code>events</code>, <code>kinesis_firehose</code> should be <code>firehose</code>, <code>msk</code> should be <code>kafka</code>, <code>mskconnect</code> should be <code>kafkaconnect</code>, <code>kinesis_analytics</code> should be <code>kinesisanalytics</code>, <code>kinesis_video</code> should be <code>kinesisvideo</code>, <code>lex</code> should be <code>lexmodels</code>, <code>media_convert</code> should be <code>mediaconvert</code>, <code>media_package</code> should be <code>mediapackage</code>, <code>media_store</code> should be <code>mediastore</code>, <code>route53_resolver</code> should be <code>route53resolver</code>, relevant <code>s3</code> should be <code>s3control</code>, <code>serverlessapplicationrepository</code> should be <code>serverlessrepo</code>, and <code>service_discovery</code> should be <code>servicediscovery</code>.</li> </ul>"},{"location":"naming/#packages","title":"Packages","text":"<p>Package names are not seen or used by practitioners. However, they should still be carefully considered.</p>"},{"location":"naming/#rule_1","title":"Rule","text":"<ol> <li>For service packages (i.e., packages under <code>internal/service</code>), use the AWS Provider service identifier as the package name.</li> <li>For other packages, use a short name for the package. Common Go lengths are 3-9 characters.</li> <li>Use a descriptive name. The name should capture the key purpose of the package.</li> <li>Use lowercase letters and do not include any underscores (<code>_</code>).</li> <li>Avoid useless names like <code>helper</code>. These names convey zero information. Everything in the AWS Provider is helping something or someone do something so the name <code>helper</code> doesn't narrow down the purpose of the package within the codebase.</li> <li>Use a name that is not too narrow or too broad as Go packages should not be too big or too small. Tiny packages can be combined using a broader name encompassing both. For example, <code>verify</code> is a good name because it tells you what the package does and allows a broad set of validation, comparison, and checking functionality.</li> </ol>"},{"location":"naming/#resources-and-data-sources","title":"Resources and Data Sources","text":"<p>When creating a new resource or data source, it is important to get names right. Once practitioners rely on names, we can only change them through breaking changes. If you are unsure about what to call a resource or data source, discuss it with the community and maintainers.</p>"},{"location":"naming/#rule_2","title":"Rule","text":"<ol> <li>Follow the AWS SDK for Go v2. Almost always, the API operations make determining the name simple. For example, the Amazon CloudWatch Evidently service includes <code>CreateExperiment</code>, <code>GetExperiment</code>, <code>UpdateExperiment</code>, and <code>DeleteExperiment</code>. Thus, the resource (or data source) name is \"Experiment.\"</li> <li>Give a resource its Terraform configuration (i.e., HCL) name (e.g., <code>aws_imagebuilder_image_pipeline</code>) by joining these three parts with underscores:<ul> <li><code>aws</code> prefix</li> <li>Service identifier (service identifiers do not include underscores), all lower case (e.g., <code>imagebuilder</code>)</li> <li>Resource (or data source) name in snake case (spaces replaced with underscores, if any), all lower case (e.g., <code>image_pipeline</code>)</li> </ul> </li> <li>Name the main resource function <code>Resource&lt;ResourceName&gt;()</code>, with the resource name in MixedCaps. Do not include the service name or identifier. For example, define <code>ResourceImagePipeline()</code> in a file called <code>internal/service/imagebuilder/image_pipeline.go</code>.</li> <li>Similarly, name the main data source function <code>DataSource&lt;ResourceName&gt;()</code>, with the data source name in MixedCaps. Do not include the service name or identifier. For example, define <code>DataSourceImagePipeline()</code> in a file called <code>internal/service/imagebuilder/image_pipeline_data_source.go</code>.</li> </ol>"},{"location":"naming/#files","title":"Files","text":"<p>File names should follow Go and Markdown conventions with these additional points.</p>"},{"location":"naming/#resource-and-data-source-documentation-rule","title":"Resource and Data Source Documentation Rule","text":"<ol> <li>Resource markdown goes in the <code>website/docs/r</code> directory. Data source markdown goes in the <code>website/docs/d</code> directory.</li> <li>Use the service identifier and resource or data source name, separated by an underscore (<code>_</code>).</li> <li>All letters are lowercase.</li> <li>Use <code>.html.markdown</code> as the extension.</li> <li>Do not include \"aws\" in the name.</li> </ol> <p>A correct example is <code>accessanalyzer_analyzer.html.markdown</code>. An incorrect example is <code>service_discovery_instance.html.markdown</code> because the service identifier should not include an underscore.</p>"},{"location":"naming/#go-file-rule","title":"Go File Rule","text":"<ol> <li>Resource and data source files are in the <code>internal/service/&lt;service&gt;</code> directory.</li> <li>Do not include the service as part of the file name.</li> <li>Data sources should include <code>_data_source</code> after the data source name (e.g., <code>application_data_source.go</code>).</li> <li>Put unit and acceptance tests in a file ending with <code>_test.go</code> (e.g., <code>custom_domain_association_test.go</code>).</li> <li>Use snake case for multiword names (i.e., all letters are lowercase, words separated by underscores).</li> <li>Use the <code>.go</code> extension.</li> <li>Idiomatic names for common non-resource, non-data-source files include <code>consts.go</code> (service-wide constants), <code>find.go</code> (finders), <code>flex.go</code> (FLatteners and EXpanders), <code>generate.go</code> (directives for code generation), <code>id.go</code> (ID creators and parsers), <code>status.go</code> (status functions), <code>sweep.go</code> (sweepers), <code>tags_gen.go</code> (generated tag code), <code>validate.go</code> (validators), and <code>wait.go</code> (waiters).</li> </ol>"},{"location":"naming/#mixedcaps","title":"MixedCaps","text":"<p>Write multiword names in Go using MixedCaps (or mixedCaps) rather than underscores.</p> <p>For more details on capitalizations we enforce with CI Semgrep tests, see the Caps List.</p> <p>Initialisms and other abbreviations are a key difference between many camel/Pascal case interpretations and mixedCaps. Abbreviations in mixedCaps should be the correct, human-readable case. After all, names in code are for humans. (The mixedCaps convention aligns with HashiCorp's emphasis on pragmatism and beauty.)</p> <p>For example, an initialism such as \"VPC\" should either be all capitalized (\"VPC\") or all lower case (\"vpc\"), never \"Vpc\" or \"vPC.\" Similarly, in mixedCaps, \"DynamoDB\" should either be \"DynamoDB\" or \"dynamoDB\", depending on whether an initial cap is needed or not, and never \"dynamoDb\" or \"DynamoDb.\"</p>"},{"location":"naming/#rule_3","title":"Rule","text":"<ol> <li>Use mixedCaps for function, type, method, variable, and constant names in the Terraform AWS Provider Go code.</li> </ol>"},{"location":"naming/#functions","title":"Functions","text":"<p>In general, follow Go best practices for good function naming. This rule is for functions defined outside of the test context (i.e., not in a file ending with <code>_test.go</code>). For test functions, see Test Support Functions or Acceptance Test Configurations below.</p>"},{"location":"naming/#rule_4","title":"Rule","text":"<ol> <li>Only export functions (capitalize) when necessary, i.e., when the function is used outside the current package, including in the <code>_test</code> (<code>.test</code>) package.</li> <li>Use MixedCaps (exported) or mixedCaps (not exported). Do not use underscores for multiwords.</li> <li>Do not include the service name in the function name. (If functions are used outside the current package, the import package clarifies a function's origin. For example, the EC2 function <code>FindVPCEndpointByID()</code> is used outside the <code>internal/service/ec2</code> package but where it is used, the call is <code>tfec2.FindVPCEndpointByID()</code>.)</li> <li>For CRUD functions for resources, use this format: <code>resource&lt;ResourceName&gt;&lt;CRUDFunction&gt;</code>. For example, <code>resourceImageRecipeUpdate()</code>, <code>resourceBaiduChannelRead()</code>.</li> <li>For data sources, for Read functions, use this format: <code>dataSource&lt;DataSourceName&gt;Read</code>. For example, <code>dataSourceBrokerRead()</code>, <code>dataSourceEngineVersionRead()</code>.</li> <li>To improve readability, consider including the resource name in helper function names that pertain only to that resource. For example, for an expander function for an \"App\" resource and a \"Campaign Hook\" expander, use <code>expandAppCampaignHook()</code>.</li> <li>Do not include \"AWS\" or \"Aws\" in the name.</li> </ol>"},{"location":"naming/#variables-and-constants","title":"Variables and Constants","text":"<p>In general, follow Go best practices for good variable and constant naming.</p>"},{"location":"naming/#rule_5","title":"Rule","text":"<ol> <li>Only export variables and constants (capitalize) when necessary, i.e., the variable or constant is used outside the current package, including in the <code>_test</code> (<code>.test</code>) package.</li> <li>Use MixedCaps (exported) or mixedCaps (not exported). Do not use underscores for multiwords.</li> <li>Do not include the service name in variable or constant names. (If variables or constants are used outside the current package, the import package clarifies its origin. For example, IAM's <code>PropagationTimeout</code> is widely used outside of IAM but each instance is through the package import alias, <code>tfiam.PropagationTimeout</code>. \"IAM\" is unnecessary in the constant name.)</li> <li>To improve readability, consider including the resource name in variable and constant names that pertain only to that resource. For example, for a string constant for a \"Role\" resource and a \"not found\" status, use <code>roleStatusNotFound</code> or <code>RoleStatusNotFound</code>, if used outside the service's package.</li> <li>Do not include \"AWS\" or \"Aws\" in the name.</li> </ol> <p>NOTE: Give priority to using constants from the AWS SDK for Go rather than defining new constants for the same values.</p>"},{"location":"naming/#acceptance-and-unit-tests","title":"Acceptance and Unit Tests","text":"<p>With about 6000 acceptance and unit tests, following these naming conventions is essential to organization and (human) context switching between services.</p> <p>There are three types of tests in the AWS Provider: (regular) acceptance tests, serialized acceptance tests, and unit tests. All are functions that take a variable of type <code>*testing.T</code>. Acceptance tests and unit tests have exported (i.e., capitalized) names while serialized tests do not. Serialized tests are called by another exported acceptance test, often ending with <code>_serial</code>. The majority of tests in the AWS provider are acceptance tests.</p>"},{"location":"naming/#acceptance-test-rule","title":"Acceptance Test Rule","text":"<p>Acceptance test names have a minimum of two (e.g., <code>TestAccBackupPlan_tags</code>) or a maximum of three (e.g., <code>TestAccDynamoDBTable_Replica_multiple</code>) parts, joined with underscores:</p> <ol> <li>First part: All have a prefix (i.e., <code>TestAcc</code>), service name (e.g., <code>Backup</code>, <code>DynamoDB</code>), and resource name (e.g., <code>Plan</code>, <code>Table</code>), MixedCaps without underscores between. Do not include \"AWS\" or \"Aws\" in the name.</li> <li>Middle part (Optional): Test group (e.g., <code>Replica</code>), uppercase, MixedCaps. Consider a metaphor where tests are chapters in a book. If it is helpful, tests can be grouped together like chapters in a book that are sometimes grouped into parts or sections of the book.</li> <li>Last part: Test identifier (e.g., <code>basic</code>, <code>tags</code>, or <code>multiple</code>), lowercase, mixedCaps). The identifier should make the test's purpose clear but be concise. For example, the identifier <code>conflictsWithCloudFrontDefaultCertificate</code> (41 characters) conveys no more information than <code>conflictDefaultCertificate</code> (26 characters), since \"CloudFront\" is implied and \"with\" is always implicit. Avoid words that convey no meaning or whose meaning is implied. For example, \"with\" (e.g., <code>_withTags</code>) is not needed because we imply the name is telling us what the test is with. <code>withTags</code> can be simplified to <code>tags</code>.</li> </ol>"},{"location":"naming/#serialized-acceptance-test-rule","title":"Serialized Acceptance Test Rule","text":"<p>The names of serialized acceptance tests follow the regular acceptance test name rule except serialized acceptance test names:</p> <ol> <li>Start with <code>testAcc</code> instead of <code>TestAcc</code></li> <li>Do not include the name of the service (e.g., a serialized acceptance test would be called <code>testAccApp_basic</code> not <code>testAccAmplifyApp_basic</code>).</li> </ol>"},{"location":"naming/#unit-test-rule","title":"Unit Test Rule","text":"<p>Unit test names follow the same rule as acceptance test names except unit test names:</p> <ol> <li>Start with <code>Test</code>, not <code>TestAcc</code></li> <li>Do not include the name of the service</li> <li>Usually do not have any underscores</li> <li>If they test a function, should include the function name (e.g., a unit test of <code>ExpandListener()</code> should be called <code>TestExpandListener()</code>)</li> </ol>"},{"location":"naming/#test-support-functions","title":"Test Support Functions","text":"<p>This rule is for functions defined in the test context (i.e., in a file ending with <code>_test.go</code>) that do not return a string with Terraform configuration. For non-test functions, see Functions above. Or, see Acceptance Test Configurations below.</p>"},{"location":"naming/#rule_6","title":"Rule","text":"<ol> <li>Only export functions (capitalize) when necessary, i.e., when the function is used outside the current package. This is very rare.</li> <li>Use MixedCaps (exported) or mixedCaps (not exported). Do not use underscores for multiwords.</li> <li>Do not include the service name in the function name. For example, <code>testAccCheckAMPWorkspaceExists()</code> should be named <code>testAccCheckWorkspaceExists()</code> instead, dropping the service name.</li> <li>Several types of support functions occur commonly and should follow these patterns:<ul> <li>Destroy: <code>testAccCheck&lt;Resource&gt;Destroy</code></li> <li>Disappears: <code>testAccCheck&lt;Resource&gt;Disappears</code></li> <li>Exists: <code>testAccCheck&lt;Resource&gt;Exists</code></li> <li>Not Recreated: <code>testAccCheck&lt;Resource&gt;NotRecreated</code></li> <li>PreCheck: <code>testAccPreCheck</code> (often, only one PreCheck is needed per service so no resource name is needed)</li> <li>Recreated: <code>testAccCheck&lt;Resource&gt;Recreated</code></li> </ul> </li> <li>Do not include \"AWS\" or \"Aws\" in the name.</li> </ol>"},{"location":"naming/#acceptance-test-configurations","title":"Acceptance Test Configurations","text":"<p>This rule is for functions defined in the test context (i.e., in a file ending with <code>_test.go</code>) that return a string with Terraform configuration. For test support functions, see Test Support Functions above. Or, for non-test functions, see Functions above.</p> <p>NOTE: This rule is not widely used currently. However, new functions and functions you change should follow it.</p>"},{"location":"naming/#rule_7","title":"Rule","text":"<ol> <li>Only export functions (capitalize) when necessary, i.e., when the function is used outside the current package. This is very rare.</li> <li>Use MixedCaps (exported) or mixedCaps (not exported). Do not use underscores for multiwords.</li> <li>Do not include the service name in the function name.</li> <li>Follow this pattern: <code>testAccConfig&lt;Resource&gt;_&lt;TestGroup&gt;_&lt;configDescription&gt;</code><ul> <li><code>_&lt;TestGroup&gt;</code> is optional. Refer to the Acceptance Test Rule test group discussion.</li> <li>Especially when an acceptance test only uses one configuration, the <code>&lt;configDescription&gt;</code> should be the same as the test identifier discussed in the Acceptance Test Rule.</li> </ul> </li> <li>Do not include \"AWS\" or \"Aws\" in the name.</li> </ol>"},{"location":"prioritization/","title":"How We Prioritize","text":""},{"location":"prioritization/#intro","title":"Intro","text":""},{"location":"prioritization/#what-this-document-is","title":"What this document is","text":"<p>This document describes how we handle prioritization of work from a variety of input sources. Our focus is always to deliver tangible value to the practitioner on a predictable and frequent schedule, and we feel it is important to be transparent in how we weigh input in order to deliver on this goal.</p>"},{"location":"prioritization/#what-this-document-is-not","title":"What this document is not","text":"<p>Due to the variety of input sources, the scale of the provider, and resource constraints, it is impossible to give a hard number on how each of the factors outlined in this document are weighted. Instead, the goal of the document is to give a transparent, but generalized assessment of each of the sources of input so that the community has a better idea of why things are prioritized the way they are. Additional information may be found in the FAQ.</p>"},{"location":"prioritization/#prioritization","title":"Prioritization","text":"<p>We prioritze work based on a number of factors, including community feedback, issue/PR reactions, as well as the source of the request. While community feedback is heavily weighted, there are times where other factors take precedence. By their nature, some factors are less visible to the community, and so are outlined here as a way to be as transparent as possible. Each of the sources of input are detailed below.</p>"},{"location":"prioritization/#community","title":"Community","text":"<p>Our large community of practitioners are vocal and immensely productive in contributing to the provider codebase. Unfortunately our current team capacity means that we are unable to give every issue or pull request the same level of attention. This means we need to prioritize the issues that provide the most value to the greatest number of practitioners.</p> <p>We will always focus on the issues which have the most attention. The main rubric we have for assessing community wants is GitHub reactions. In addition to reactions, we look at comments, reactions to comments, and links to additional issues and PRs to help get a more holistic view of where the community stands. We try to ensure that for the issues where we have the most community support, we are responsive to that support and attempt to give timelines where-ever possible.</p>"},{"location":"prioritization/#customer","title":"Customer","text":"<p>Another source of work that must be weighted are escalations around particular feature requests and bugs from HashiCorp and AWS customers. Escalations typically come via several routes:</p> <ul> <li>Customer Support</li> <li>Sales Engineering</li> <li>AWS Solutions Architects contacting us on behalf of their clients.</li> </ul> <p>These reports flow into an internal board and are triaged on a weekly basis to determine whether the escalation request should be prioritized for an upcoming release or added to the backlog to monitor for additional community support. During triage, we verify whether a GitHub issue or PR exists for the request and will create one if it does not exist. In this way, these requests are visible to the community to some degree. An escalation coming from a customer does not necessarily guarantee that it will be prioritized over requests made by the community. Instead, we assess them based on the following rubric:</p> <ul> <li>Does the issue have considerable community support?</li> <li>Does the issue pertain to one of our Core Services?</li> </ul> <p>By weighing these factors, we can make a call to determine whether, and how it is to be prioritized.</p>"},{"location":"prioritization/#partner","title":"Partner","text":"<p>AWS Service Teams and Partner representatives regularly contact us to discuss upcoming features or new services. This work is often done under an NDA, so usually needs to be done in private. Often the ask is to enable Terraform support or an upcoming feature or service.</p> <p>As with customer escalations, a request from a partner does not necessarily mean that it will be prioritized over other efforts; capacity restraints require us to prioritize major releases or prefer offerings in line with our core services.</p>"},{"location":"prioritization/#internal","title":"Internal","text":""},{"location":"prioritization/#sdkcore-updates","title":"SDK/Core Updates","text":"<p>We endeavor to keep in step with all minor SDK releases, so these are automatically pulled in by our GitHub automation. Major releases normally include breaking changes and usually require us to bump the provider itself to a major version. We plan to make one major version change a year and try to avoid any more than that.</p>"},{"location":"prioritization/#technical-debt","title":"Technical Debt","text":"<p>We always include capacity for technical debt work in every iteration, but engineers are free to include minor tech debt work on their own recognizance. For larger items, these are discussed and prioritized in an internal meeting aimed at reviewing technical debt.</p>"},{"location":"prioritization/#adverse-user-experience-or-security-vulnerabilities","title":"Adverse User Experience or Security Vulnerabilities","text":"<p>Issues with the provider that provide a poor user experience (bugs, crashes), or involve a threat to security are always prioritized for inclusion. The severity of these will determine how soon they are included for release.</p>"},{"location":"provider-design/","title":"Provider Design","text":"<p>The Terraform AWS Provider follows the guidelines established in the HashiCorp Provider Design Principles. That general documentation provides many high-level design points gleaned from years of experience with Terraform's design and implementation concepts. Sections below will expand on specific design details between that documentation and this provider, while others will capture other pertinent information that may not be covered there. Other pages of the contributing guide cover implementation details such as code, testing, and documentation specifics.</p>"},{"location":"provider-design/#api-and-sdk-boundary","title":"API and SDK Boundary","text":"<p>The AWS provider implements support for the AWS service APIs using the AWS Go SDK. The API and SDK limits extend to the provider. In general, SDK operations manage the lifecycle of AWS components, such as creating, describing, updating, and deleting a database. Operations do not usually handle functionality within those components, such as executing a query on a database. If you are interested in other APIs/SDKs, we invite you to view the many Terraform Providers available, as each has a community of domain expertise.</p> <p>Some examples of functionality that is not expected in this provider:</p> <ul> <li>Raw HTTP(S) handling. See the Terraform HTTP Provider and Terraform TLS Provider instead.</li> <li>Kubernetes resource management beyond the EKS service APIs. See the Terraform Kubernetes Provider instead.</li> <li>Active Directory or other protocol clients. See the Terraform Active Directory Provider and other available provider instead.</li> <li>Functionality that requires additional software beyond the Terraform AWS Provider to be installed on the host executing Terraform. This currently includes the AWS CLI. See the Terraform External Provider and other available providers instead.</li> </ul>"},{"location":"provider-design/#infrastructure-as-code-suitability","title":"Infrastructure as Code Suitability","text":"<p>The provider maintainers' design goal is to cover as much of the AWS API as pragmatically possible. However, not every aspect of the API is compatible with an infrastructure-as-code (IaC) conception. Specifically: IaC is best suited for immutable rather than mutable infrastrcture -- i.e. for resources with a single desired state described in its entirety, as opposed to resources defined via a dynamic process.</p> <p>If such limits affect you, we recommend that you open an AWS Support case and encourage others to do the same. Request that AWS components be made more self-contained and compatible with IaC. These AWS Support cases can also yield insights into the AWS service and API that are not well documented.</p>"},{"location":"provider-design/#resource-type-considerations","title":"Resource Type Considerations","text":"<p>Terraform resources work best as the smallest infrastructure blocks on which practitioners can build more complex configurations and abstractions, such as Terraform Modules. The general heuristic guiding when to implement a new Terraform resource for an aspect of AWS is whether the AWS service API provides create, read, update, and delete (CRUD) operations. However, not all AWS service API functionality falls cleanly into CRUD lifecycle management. In these situations, there is extra consideration necessary for properly mapping API operations to Terraform resources.</p> <p>This section highlights design patterns when to consider an implementation within a singular Terraform resource or as separate Terraform resources.</p> <p>Please note: the overall design and implementation across all AWS functionality is federated: individual services may implement concepts and use terminology differently. As such, this guide is not exhaustive. The aim is to provide general concepts and basic terminology that points contributors in the right direction, especially in understanding previous implementations.</p>"},{"location":"provider-design/#authorization-and-acceptance-resources","title":"Authorization and Acceptance Resources","text":"<p>Some AWS services use an authorization-acceptance model for cross-account associations or access. Examples include:</p> <ul> <li>Direct Connect Association Proposals</li> <li>GuardDuty Member Invitations</li> <li>RAM Resource Share Associations</li> <li>Route 53 VPC Associations</li> <li>Security Hub Member Invitations</li> </ul> <p>Depending on the API and components, AWS uses two basic ways of creating cross-region and cross-account associations. One way is to generate an invitation (or proposal) identifier from one AWS account to another. Then in the other AWS account, that identifier is used to accept the invitation. The second way is configuring a reference to another AWS account identifier. These may not require explicit acceptance on the receiving account to finish creating the association or begin working.</p> <p>To model creating an association using an invitation or proposal, follow these guidelines.</p> <ul> <li>Follow the naming in the AWS service API to determine whether to use the term \"invitation\" or \"proposal.\"</li> <li>For the originating account, create an \"invitation\" or \"proposal\" resource. Make sure that the AWS service API has operations for creating and reading invitations.</li> <li>For the responding account, create an \"accepter\" resource. Ensure that the API has operations for accepting, reading, and rejecting invitations in the responding account. Map the operations as follows:<ul> <li>Create: Accepts the invitation.</li> <li>Read: Reads the invitation to determine its status. Note that in some APIs, invitations expire and disappear, complicating associations. If a resource does not find an invitation, the developer should implement a fall back to read the API resource associated with the invitation/proposal.</li> <li>Delete: Rejects or otherwise deletes the invitation.</li> </ul> </li> </ul> <p>To model the second type of association, implicit associations, create an \"association\" resource and, optionally, an \"authorization\" resource. Map create, read, and delete to the corresponding operations in the AWS service API.</p>"},{"location":"provider-design/#cross-service-functionality","title":"Cross-Service Functionality","text":"<p>Many AWS service APIs build on top of other AWS services. Some examples of these include:</p> <ul> <li>EKS Node Groups managing Auto Scaling Groups</li> <li>Lambda Functions managing EC2 ENIs</li> <li>Transfer Servers managing EC2 VPC Endpoints</li> </ul> <p>Some cross-service API implementations lack the management or description capabilities of the other service. The lack can make the Terraform resource implementation seem incomplete or unsuccessful in end-to-end configurations. Given the overall \u201cresources should represent a single API object\u201d goal from the HashiCorp Provider Design Principles, a resource must only communicate with a single AWS service API. As such, maintainers will not approve cross-service resources.</p> <p>The rationale behind this design decision includes the following:</p> <ul> <li>Unexpected IAM permissions being necessary for the resource. In high-security environments, all the service permissions may not be available or acceptable.</li> <li>Unexpected services generating CloudTrail logs for the resource.</li> <li>Needing extra and unexpected API endpoints configuration for organizations using custom endpoints, such as VPC endpoints.</li> <li>Unexpected changes to the AWS service internals for the cross-service implementations. Given that this functionality is not part of the primary service API, these details can change over time and may not be considered as a breaking change by the service team for an API upgrade.</li> </ul> <p>A poignant real-world example of the last point involved a Lambda resource. The resource helped clean up extra resources (ENIs) due to a common misconfiguration. Practitioners found the functionality helpful since the issue was hard to diagnose. Years later, AWS updated the Lambda API. Immediately, practitioners reported that Terraform executions were failing. Downgrading the provider was not possible since many configurations depended on recent releases. For environments running many versions behind, forcing an upgrade with the fix would likely cause unrelated and unexpected changes. In the end, HashiCorp and AWS performed a large-scale outreach to help upgrade and fixing the misconfigurations. Provider maintainers and practitioners lost considerable time.</p>"},{"location":"provider-design/#data-sources","title":"Data Sources","text":"<p>A separate class of Terraform resource types are data sources. These are typically intended as a configuration method to lookup or fetch data in a read-only manner. Data sources should not have side effects on the remote system.</p> <p>When discussing data sources, they are typically classified by the intended number of return objects or data. Singular data sources represent a one-to-one lookup or data operation. Plural data sources represent a one-to-many operation.</p>"},{"location":"provider-design/#plural-data-sources","title":"Plural Data Sources","text":"<p>These data sources are intended to return zero, one, or many results, usually associated with a managed resource type. Typically results are a set unless ordering guarantees are provided by the remote system. These should be named with a plural suffix (e.g., <code>s</code> or <code>es</code>) and should not include any specific attribute in the naming (e.g., prefer <code>aws_ec2_transit_gateways</code> instead of <code>aws_ec2_transit_gateway_ids</code>).</p>"},{"location":"provider-design/#singular-data-sources","title":"Singular Data Sources","text":"<p>These data sources are intended to return one result or an error. These should not include any specific attribute in the naming (e.g., prefer <code>aws_ec2_transit_gateway</code> instead of <code>aws_ec2_transit_gateway_id</code>).</p>"},{"location":"provider-design/#iam-resource-based-policy-resources","title":"IAM Resource-Based Policy Resources","text":"<p>For some AWS components, the AWS API allows specifying an IAM resource-based policy, the IAM policy to associate with a component. Some examples include:</p> <ul> <li>ECR Repository Policies</li> <li>EFS File System Policies</li> <li>SNS Topic Policies</li> </ul> <p>Provider developers should implement this capability in a new resource rather than adding it to the associated resource. Reasons for this include:</p> <ul> <li>Many of the policies must include the ARN of the resource. Working around this requirement with custom difference handling within a self-contained resource is unnecessarily cumbersome.</li> <li>Some policies involving multiple resources need to cross-reference each other's ARNs. Without a separate resource, this introduces a configuration cycle.</li> <li>Splitting the resources allows operators to logically split their configurations into purely operational and security boundaries. This allows environments to have distinct practitioners roles and permissions for IAM versus infrastructure changes.</li> </ul> <p>One rare exception to this guideline is where the policy is required during resource creation.</p>"},{"location":"provider-design/#managing-resource-running-state","title":"Managing Resource Running State","text":"<p>The AWS API provides the ability to start, stop, enable, or disable some AWS components. Some examples include:</p> <ul> <li>Batch Job Queues</li> <li>CloudFront Distributions</li> <li>RDS DB Event Subscriptions</li> </ul> <p>In this situation, provider developers should implement this ability within the resource instead of creating a separate resource. Since a practitioner cannot practically manage interaction with a resource's states in Terraform's declarative configuration, developers should implement the state management in the resource. This design provides consistency and future-proofing even where updating a resource in the current API is not problematic.</p>"},{"location":"provider-design/#task-execution-and-waiter-resources","title":"Task Execution and Waiter Resources","text":"<p>Some AWS operations are asynchronous. Terraform requests that AWS perform a task. Initially, AWS only notifies Terraform that it received the request. Terraform then requests the status while awaiting completion. Examples of this include:</p> <ul> <li>ACM Certificate validation</li> <li>EC2 AMI copying</li> <li>RDS DB Cluster Snapshot management</li> </ul> <p>In this situation, provider developers should create a separate resource representing the task, assuming that the AWS service API provides operations to start the task and read its status. Adding the task functionality to the parent resource muddies its infrastructure-management purpose. The maintainers prefer this approach even though there is some duplication of an existing resource. For example, the provider has a resource for copying an EC2 AMI in addition to the EC2 AMI resource itself. This modularity allows practitioners to manage the result of the task resource with another resource.</p> <p>For a related consideration, see the Managing Resource Running State section.</p>"},{"location":"provider-design/#versioned-resources","title":"Versioned Resources","text":"<p>AWS supports having multiple versions of some components. Examples of this include:</p> <ul> <li>ECS Task Definitions</li> <li>Lambda Functions</li> <li>Secrets Manager Secrets</li> </ul> <p>In general, provider developers should create a separate resource to represent a single version. For example, the provider has both the <code>aws_secretsmanager_secret</code> and <code>aws_secretsmanager_secret_version</code> resources. However, in some cases, developers should handle versioning in the main resource.</p> <p>In deciding when to create a separate resource, follow these guidelines:</p> <ul> <li>If AWS necessarily creates a version when you make a new AWS component, include version handling in the same Terraform resource. Creating an AWS component with one Terraform resource and later using a different resource for updates is confusing.</li> <li>If the AWS service API allows deleting versions and practitioners will want to delete versions, provider developers should implement a separate version resource.</li> <li>If the API only supports publishing new versions, either method is acceptable, however most current implementations are self-contained. Terraform's current configuration language does not natively support triggering resource updates or recreation across resources without a state value change. This can make the implementation more difficult for practitioners without special resource and configuration workarounds, such as a <code>triggers</code> attribute. If this changes in the future, then this guidance may be updated towards separate resources, following the Task Execution and Waiter Resources guidance.</li> </ul>"},{"location":"provider-design/#other-considerations","title":"Other Considerations","text":""},{"location":"provider-design/#aws-credential-exfiltration","title":"AWS Credential Exfiltration","text":"<p>In the interest of security, the maintainers will not approve data sources that provide the ability to reference or export the AWS credentials of the running provider. There are valid use cases for this information, such as to execute AWS CLI calls as part of the same Terraform configuration. However, this mechanism may allow credentials to be discovered and used outside of Terraform. Some specific concerns include:</p> <ul> <li>The values may be visible in Terraform user interface output or logging, allowing anyone with user interface or log access to see the credentials.</li> <li>The values are currently stored in plaintext in the Terraform state, allowing anyone with access to the state file or another Terraform configuration that references the state access to the credentials.</li> <li>Any new related functionality, while opt-in to implement, is also opt-in to prevent via security controls or policies. Adopting a weaker default security posture requires advance notice and prevents organizations that implement those controls from updating to a version with any such functionality.</li> </ul>"},{"location":"raising-a-pull-request/","title":"Raising a Pull Request","text":"<ol> <li> <p>Fork the GitHub repository allowing you to make the changes in your own copy of the repository.</p> </li> <li> <p>Create a branch using the following naming prefixes:</p> <ul> <li>f = feature</li> <li>b = bug fix</li> <li>d = documentation</li> <li>t = tests</li> <li>td = technical debt</li> <li>v = dependencies (\"vendoring\" previously)</li> </ul> <p>Some indicative example branch names would be <code>f-aws_emr_instance_group-refactor</code> or <code>td-staticcheck-st1008</code></p> </li> <li> <p>Make the changes you would like to include in the provider, add new tests as required, and make sure that all relevant existing tests are passing.</p> </li> <li> <p>Create a changelog entry following the process outlined here</p> </li> <li> <p>Create a pull request. Please ensure (if possible)   the 'Allow edits from maintainers' checkbox is checked. This will allow the maintainers to make changes and merge the PR without requiring action from the contributor.    You are welcome to submit your pull request for commentary or review before    it is fully completed by creating a draft pull request.    Please include specific questions or items you'd like feedback on.</p> </li> <li> <p>Once you believe your pull request is ready to be reviewed, ensure the    pull request is not a draft pull request by marking it ready for review    or removing <code>[WIP]</code> from the pull request title if necessary, and a    maintainer will review it. Follow the checklists below    to help ensure that your contribution can be easily reviewed and potentially    merged.</p> </li> <li> <p>One of Terraform's provider team members will look over your contribution and    either approve it or provide comments letting you know if there is anything    left to do. We'll try give you the opportunity to make the required changes yourself, but in some cases we may perform the changes ourselves if it makes sense to (minor changes, or for urgent issues).  We do our best to keep up with the volume of PRs waiting for review, but it may take some time depending on the complexity of the work.</p> </li> <li> <p>Once all outstanding comments and checklist items have been addressed, your    contribution will be merged! Merged PRs will be included in the next    Terraform release.</p> </li> <li> <p>In some cases, we might decide that a PR should be closed without merging.    We'll make sure to provide clear reasoning when this happens.</p> </li> </ol>"},{"location":"raising-a-pull-request/#go-coding-style","title":"Go Coding Style","text":"<p>All Go code is automatically checked for compliance with various linters, such as <code>gofmt</code>. These tools can be installed using the <code>GNUMakefile</code> in this repository.</p> <pre><code>$ cd terraform-provider-aws\n$ make tools\n</code></pre> <p>Check your code with the linters:</p> <pre><code>$ make lint\n</code></pre> <p>We use Semgrep to check for other code standards. This can be run directly on the command line, i.e.,</p> <pre><code>$ semgrep\n</code></pre> <p>or it can be run using Docker via the Makefile, i.e.,</p> <pre><code>$ make semgrep\n</code></pre> <p><code>gofmt</code> will also fix many simple formatting issues for you. The Makefile includes a target for this:</p> <pre><code>$ make fmt\n</code></pre> <p>The import statement in a Go file follows these rules (see #15903):</p> <ol> <li>Import declarations are grouped into a maximum of three groups with the following order:<ul> <li>Standard packages (also called short import path or built-in packages)</li> <li>Third-party packages (also called long import path packages)</li> <li>Local packages</li> </ul> </li> <li>Groups are separated by a single blank line</li> <li>Packages within each group are alphabetized</li> </ol> <p>Check your imports:</p> <pre><code>$ make importlint\n</code></pre> <p>For greater detail, the following Go language resources provide common coding preferences that may be referenced during review, if not automatically handled by the project's linting tools.</p> <ul> <li>Effective Go</li> <li>Go Code Review Comments</li> </ul>"},{"location":"raising-a-pull-request/#resource-contribution-guidelines","title":"Resource Contribution Guidelines","text":"<p>The following resource checks need to be addressed before your contribution can be merged. The exclusion of any applicable check may result in a delayed time to merge. Some of these are not handled by the automated code testing that occurs during submission, so reviewers (even those outside the maintainers) are encouraged to reach out to contributors about any issues to save time.</p> <p>This Contribution Guide also includes separate sections on topics such as Error Handling, which also applies to contributions.</p> <ul> <li>Passes Testing: All code and documentation changes must pass unit testing, code linting, and website link testing. Resource code changes must pass all acceptance testing for the resource.</li> <li>Avoids API Calls Across Account, Region, and Service Boundaries: Resources should not implement cross-account, cross-region, or cross-service API calls.</li> <li>Does Not Set Optional or Required for Non-Configurable Attributes: Resource schema definitions for read-only attributes must not include <code>Optional: true</code> or <code>Required: true</code>.</li> <li>Avoids resource.Retry() without resource.RetryableError(): Resource logic should only implement <code>resource.Retry()</code> if there is a retryable condition (e.g., <code>return resource.RetryableError(err)</code>).</li> <li>Avoids Reusing Resource Read Function in Data Source Read Function: Data sources should fully implement their own resource <code>Read</code> functionality including duplicating <code>d.Set()</code> calls.</li> <li>Avoids Reading Schema Structure in Resource Code: The resource <code>Schema</code> should not be read in resource <code>Create</code>/<code>Read</code>/<code>Update</code>/<code>Delete</code> functions to perform looping or otherwise complex attribute logic. Use <code>d.Get()</code> and <code>d.Set()</code> directly with individual attributes instead.</li> <li>Avoids ResourceData.GetOkExists(): Resource logic should avoid using <code>ResourceData.GetOkExists()</code> as its expected functionality is not guaranteed in all scenarios.</li> <li>Calls Read After Create and Update: Except where API eventual consistency prohibits immediate reading of resources or updated attributes,  resource <code>Create</code> and <code>Update</code> functions should return the resource <code>Read</code> function.</li> <li>Implements Immediate Resource ID Set During Create: Immediately after calling the API creation function, the resource ID should be set with <code>d.SetId()</code> before other API operations or returning the <code>Read</code> function.</li> <li>Implements Attribute Refreshes During Read: All attributes available in the API should have <code>d.Set()</code> called their values in the Terraform state during the <code>Read</code> function.</li> <li>Performs Error Checks with Non-Primitive Attribute Refreshes: When using <code>d.Set()</code> with non-primitive types (<code>schema.TypeList</code>, <code>schema.TypeSet</code>, or <code>schema.TypeMap</code>), perform error checking to prevent issues where the code is not properly able to refresh the Terraform state.</li> <li>Implements Import Acceptance Testing and Documentation: Support for resource import (<code>Importer</code> in resource schema) must include <code>ImportState</code> acceptance testing (see also the Acceptance Testing Guidelines) and <code>## Import</code> section in resource documentation.</li> <li>Implements Customizable Timeouts Documentation: Support for customizable timeouts (<code>Timeouts</code> in resource schema) must include <code>## Timeouts</code> section in resource documentation.</li> <li>Implements State Migration When Adding New Virtual Attribute: For new \"virtual\" attributes (those only in Terraform and not in the API), the schema should implement State Migration to prevent differences for existing configurations that upgrade.</li> <li>Uses AWS Go SDK Constants: Many AWS services provide string constants for value enumerations, error codes, and status types. See also the \"Constants\" sections under each of the service packages in the AWS Go SDK documentation.</li> <li>Uses AWS Go SDK Pointer Conversion Functions: Many APIs return pointer types and these functions return the zero value for the type if the pointer is <code>nil</code>. This prevents potential panics from unchecked <code>*</code> pointer dereferences and can eliminate boilerplate <code>nil</code> checking in many cases. See also the <code>aws</code> package in the AWS Go SDK documentation.</li> <li>Uses AWS Go SDK Types: Use available SDK structs instead of implementing custom types with indirection.</li> <li>Uses Existing Validation Functions: Schema definitions including <code>ValidateFunc</code> for attribute validation should use available Terraform <code>helper/validation</code> package functions. <code>All()</code>/<code>Any()</code> can be used for combining multiple validation function behaviors.</li> <li>Uses tfresource.TimedOut() with resource.Retry(): Resource logic implementing <code>resource.Retry()</code> should error check with <code>tfresource.TimedOut(err error)</code> and potentially unset the error before returning the error. For example:</li> </ul> <pre><code>var output *kms.CreateKeyOutput\nerr := resource.Retry(1*time.Minute, func() *resource.RetryError {\nvar err error\n\noutput, err = conn.CreateKey(input)\n\n/* ... */\n\nreturn nil\n})\n\nif tfresource.TimedOut(err) {\noutput, err = conn.CreateKey(input)\n}\n\nif err != nil {\nreturn fmt.Errorf(\"creating KMS External Key: %s\", err)\n}\n</code></pre> <ul> <li>Uses resource.UniqueId(): API fields for concurrency protection such as <code>CallerReference</code> and <code>IdempotencyToken</code> should use <code>resource.UniqueId()</code>. The implementation includes a monotonic counter which is safer for concurrent operations than solutions such as <code>time.Now()</code>.</li> <li>Skips id Attribute: The <code>id</code> attribute is implicit for all Terraform resources and does not need to be defined in the schema.</li> </ul> <p>The below are style-based items that may be noted during review and are recommended for simplicity, consistency, and quality assurance:</p> <ul> <li>Implements arn Attribute: APIs that return an ARN should implement <code>arn</code> as an attribute. Alternatively, the ARN can be synthesized using the AWS Go SDK <code>arn.ARN</code> structure. For example:</li> </ul> <pre><code>// Direct Connect Virtual Interface ARN.\n// See https://docs.aws.amazon.com/directconnect/latest/UserGuide/security_iam_service-with-iam.html#security_iam_service-with-iam-id-based-policies-resources.\narn := arn.ARN{\nPartition: meta.(*AWSClient).partition,\nRegion:    meta.(*AWSClient).region,\nService:   \"directconnect\",\nAccountID: meta.(*AWSClient).accountid,\nResource:  fmt.Sprintf(\"dxvif/%s\", d.Id()),\n}.String()\nd.Set(\"arn\", arn)\n</code></pre> <p>When the <code>arn</code> attribute is synthesized this way, add the resource to the list of those affected by the provider's <code>skip_requesting_account_id</code> attribute.</p> <ul> <li>Implements Warning Logging With Resource State Removal: If a resource is removed outside of Terraform (e.g., via different tool, API, or web UI), <code>d.SetId(\"\")</code> and <code>return nil</code> can be used in the resource <code>Read</code> function to trigger resource recreation. When this occurs, a warning log message should be printed beforehand: <code>log.Printf(\"[WARN] {SERVICE} {THING} (%s) not found, removing from state\", d.Id())</code></li> <li>Uses American English for Attribute Naming: For any ambiguity with attribute naming, prefer American English over British English. e.g., <code>color</code> instead of <code>colour</code>.</li> <li>Skips Timestamp Attributes: Generally, creation and modification dates from the API should be omitted from the schema.</li> <li>Uses Paginated AWS Go SDK Functions When Iterating Over a Collection of Objects: When the API for listing a collection of objects provides a paginated function, use it instead of looping until the next page token is not set. For example, with the EC2 API, <code>DescribeInstancesPages</code> should be used instead of <code>DescribeInstances</code> when more than one result is expected.</li> <li>Adds Paginated Functions Missing from the AWS Go SDK to Internal Service Package: If the AWS Go SDK does not define a paginated equivalent for a function to list a collection of objects, it should be added to a per-service internal package using the <code>listpages</code> generator. A support case should also be opened with AWS to have the paginated functions added to the AWS Go SDK.</li> </ul>"},{"location":"resource-filtering/","title":"Adding Resource Filtering Support","text":"<p>AWS provides server-side filtering across many services and resources, which can be used when listing resources of that type, for example in the implementation of a data source. See the EC2 Listing and filtering your resources page for information about how server-side filtering can be used with EC2 resources.</p> <p>To determine if the supporting AWS API supports this functionality:</p> <ul> <li>Open the AWS Go SDK documentation for the service, e.g., for <code>service/rds</code>. Note: there can be a delay between the AWS announcement and the updated AWS Go SDK documentation.</li> <li>Determine if the service API includes functionality for filtering resources (usually a <code>Filters</code> argument to a <code>DescribeThing</code> API call).</li> </ul> <p>Implementing server-side filtering support for Terraform AWS Provider resources requires the following, each with its own section below:</p> <ul> <li>Generated Service Filtering Code: In the internal code generators (e.g., <code>internal/generate/namevaluesfilters</code>), implementation and customization of how a service handles filtering, which is standardized for the resources.</li> <li>Resource Filtering Code Implementation: In the resource's equivalent data source code (e.g., <code>internal/service/{servicename}/thing_data_source.go</code>), implementation of <code>filter</code> schema attribute, along with handling in the <code>Read</code> function.</li> <li>Resource Filtering Documentation Implementation: In the resource's equivalent data source documentation (e.g., <code>website/docs/d/service_thing.html.markdown</code>), addition of <code>filter</code> argument</li> </ul>"},{"location":"resource-filtering/#adding-service-to-filter-generating-code","title":"Adding Service to Filter Generating Code","text":"<p>This step is only necessary for the first implementation and may have been previously completed. If so, move on to the next section.</p> <p>More details about this code generation can be found in the namevaluesfilters documentation.</p> <p>Add the AWS Go SDK service name (e.g., <code>rds</code>) to <code>sliceServiceNames</code> in <code>internal/generate/namevaluesfilters/generators/servicefilters/main.go</code>.</p> <ul> <li>Run <code>make gen</code> (<code>go generate ./...</code>) and ensure there are no errors via <code>make test</code> (<code>go test ./...</code>)</li> </ul>"},{"location":"resource-filtering/#resource-filter-code-implementation","title":"Resource Filter Code Implementation","text":"<ul> <li>In the resource's equivalent data source Go file (e.g., <code>internal/service/ec2/internet_gateway_data_source.go</code>), add the following Go import: <code>\"github.com/hashicorp/terraform-provider-aws/internal/generate/namevaluesfilters\"</code></li> <li>In the resource schema, add <code>\"filter\": namevaluesfilters.Schema(),</code></li> <li>Implement the logic to build the list of filters:</li> </ul> <pre><code>input := &amp;ec2.DescribeInternetGatewaysInput{}\n\n// Filters based on attributes.\nfilters := namevaluesfilters.New(map[string]string{\n\"internet-gateway-id\": d.Get(\"internet_gateway_id\").(string),\n})\n// Add filters based on keyvalue tags (N.B. Not applicable to all AWS services that support filtering)\nfilters.Add(namevaluesfilters.EC2Tags(keyvaluetags.New(d.Get(\"tags\").(map[string]interface{})).IgnoreAWS().IgnoreConfig(ignoreTagsConfig).Map()))\n// Add filters based on the custom filtering \"filter\" attribute.\nfilters.Add(d.Get(\"filter\").(*schema.Set))\n\ninput.Filters = filters.EC2Filters()\n</code></pre>"},{"location":"resource-filtering/#resource-filtering-documentation-implementation","title":"Resource Filtering Documentation Implementation","text":"<ul> <li>In the resource's equivalent data source documentation (e.g., <code>website/docs/d/internet_gateway.html.markdown</code>), add the following to the arguments reference:</li> </ul> <pre><code>* `filter` - (Optional) Custom filter block as described below.\n\nMore complex filters can be expressed using one or more `filter` sub-blocks, which take the following arguments:\n\n* `name` - (Required) Name of the field to filter by, as defined by\n  [the underlying AWS API](https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_DescribeInternetGateways.html).\n\n* `values` - (Required) Set of values that are accepted for the given field.\n  An Internet Gateway will be selected if any one of the given values matches.\n</code></pre>"},{"location":"resource-name-generation/","title":"Adding Resource Name Generation Support","text":"<p>Terraform AWS Provider resources can use shared logic to support and test name generation, where the operator can choose between an expected naming value, a generated naming value with a prefix, or a fully generated name.</p> <p>Implementing name generation support for Terraform AWS Provider resources requires the following, each with its own section below:</p> <ul> <li>Resource Name Generation Code Implementation: In the resource code (e.g., <code>internal/service/{service}/{thing}.go</code>), implementation of <code>name_prefix</code> attribute, along with handling in <code>Create</code> function.</li> <li>Resource Name Generation Testing Implementation: In the resource acceptance testing (e.g., <code>internal/service/{service}/{thing}_test.go</code>), implementation of new acceptance test functions and configurations to exercise new naming logic.</li> <li>Resource Name Generation Documentation Implementation: In the resource documentation (e.g., <code>website/docs/r/service_thing.html.markdown</code>), addition of <code>name_prefix</code> argument and update of <code>name</code> argument description.</li> </ul>"},{"location":"resource-name-generation/#resource-name-generation-code-implementation","title":"Resource name generation code implementation","text":"<ul> <li>In the resource Go file (e.g., <code>internal/service/{service}/{thing}.go</code>), add the following Go import: <code>\"github.com/hashicorp/terraform-provider-aws/internal/create\"</code></li> <li>In the resource schema, add the new <code>name_prefix</code> attribute and adjust the <code>name</code> attribute to be <code>Optional</code>, <code>Computed</code>, and <code>ConflictsWith</code> the <code>name_prefix</code> attribute. Ensure to keep any existing schema fields on <code>name</code> such as <code>ValidateFunc</code>. E.g.</li> </ul> <pre><code>\"name\": {\nType:          schema.TypeString,\nOptional:      true,\nComputed:      true,\nForceNew:      true,\nConflictsWith: []string{\"name_prefix\"},\n},\n\"name_prefix\": {\nType:          schema.TypeString,\nOptional:      true,\nComputed:      true,\nForceNew:      true,\nConflictsWith: []string{\"name\"},\n},\n</code></pre> <ul> <li>In the resource <code>Create</code> function, switch any calls from <code>d.Get(\"name\").(string)</code> to instead use the <code>create.Name()</code> function, e.g.</li> </ul> <pre><code>name := create.Name(d.Get(\"name\").(string), d.Get(\"name_prefix\").(string))\n\n// ... in AWS Go SDK Input types, etc. use aws.String(name)\n</code></pre> <ul> <li>If the resource supports import, in the resource <code>Read</code> function add a call to <code>d.Set(\"name_prefix\", ...)</code>, e.g.</li> </ul> <pre><code>d.Set(\"name\", resp.Name)\nd.Set(\"name_prefix\", create.NamePrefixFromName(aws.StringValue(resp.Name)))\n</code></pre>"},{"location":"resource-name-generation/#resource-name-generation-testing-implementation","title":"Resource name generation testing implementation","text":"<ul> <li>In the resource testing (e.g., <code>internal/service/{service}/{thing}_test.go</code>), add the following Go import: <code>\"github.com/hashicorp/terraform-provider-aws/internal/create\"</code></li> <li>In the resource testing, implement two new tests named <code>_Name_Generated</code> and <code>_NamePrefix</code> with associated configurations, that verifies creating the resource without <code>name</code> and <code>name_prefix</code> arguments (for the former) and with only the <code>name_prefix</code> argument (for the latter). E.g.</li> </ul> <pre><code>func TestAccServiceThing_nameGenerated(t *testing.T) {\nctx := acctest.Context(t)\nvar thing service.ServiceThing\nresourceName := \"aws_service_thing.test\"\n\nresource.ParallelTest(t, resource.TestCase{\nPreCheck:                 func() { acctest.PreCheck(t) },\nErrorCheck:               acctest.ErrorCheck(t, service.EndpointsID),\nProtoV5ProviderFactories: acctest.ProtoV5ProviderFactories,\nCheckDestroy:             testAccCheckThingDestroy(ctx),\nSteps: []resource.TestStep{\n{\nConfig: testAccThingConfig_nameGenerated(),\nCheck: resource.ComposeTestCheckFunc(\ntestAccCheckThingExists(ctx, resourceName, &amp;thing),\nacctest.CheckResourceAttrNameGenerated(resourceName, \"name\"),\nresource.TestCheckResourceAttr(resourceName, \"name_prefix\", resource.UniqueIdPrefix),\n),\n},\n// If the resource supports import:\n{\nResourceName:      resourceName,\nImportState:       true,\nImportStateVerify: true,\n},\n},\n})\n}\n\nfunc TestAccServiceThing_namePrefix(t *testing.T) {\nctx := acctest.Context(t)\nvar thing service.ServiceThing\nresourceName := \"aws_service_thing.test\"\n\nresource.ParallelTest(t, resource.TestCase{\nPreCheck:                 func() { acctest.PreCheck(t) },\nErrorCheck:               acctest.ErrorCheck(t, service.EndpointsID),\nProtoV5ProviderFactories: acctest.ProtoV5ProviderFactories,\nCheckDestroy:             testAccCheckThingDestroy(ctx),\nSteps: []resource.TestStep{\n{\nConfig: testAccThingConfig_namePrefix(\"tf-acc-test-prefix-\"),\nCheck: resource.ComposeTestCheckFunc(\ntestAccCheckThingExists(ctx, resourceName, &amp;thing),\nacctest.CheckResourceAttrNameFromPrefix(resourceName, \"name\", \"tf-acc-test-prefix-\"),\nresource.TestCheckResourceAttr(resourceName, \"name_prefix\", \"tf-acc-test-prefix-\"),\n),\n},\n// If the resource supports import:\n{\nResourceName:      resourceName,\nImportState:       true,\nImportStateVerify: true,\n},\n},\n})\n}\n\nfunc testAccThingConfig_nameGenerated() string {\nreturn fmt.Sprintf(`\nresource \"aws_service_thing\" \"test\" {\n  # ... other configuration ...\n}\n`)\n}\n\nfunc testAccThingConfig_namePrefix(namePrefix string) string {\nreturn fmt.Sprintf(`\nresource \"aws_service_thing\" \"test\" {\n  # ... other configuration ...\n\n  name_prefix = %[1]q\n}\n`, namePrefix)\n}\n</code></pre>"},{"location":"resource-name-generation/#resource-name-generation-documentation-implementation","title":"Resource name generation documentation implementation","text":"<ul> <li>In the resource documentation (e.g., <code>website/docs/r/service_thing.html.markdown</code>), add the following to the arguments reference:</li> </ul> <pre><code>* `name_prefix` - (Optional) Creates a unique name beginning with the specified prefix. Conflicts with `name`.\n</code></pre> <ul> <li>Adjust the existing <code>name</code> argument reference to ensure its denoted as <code>Optional</code>, includes a mention that it can be generated, and that it conflicts with <code>name_prefix</code>:</li> </ul> <pre><code>* `name` - (Optional) Name of the thing. If omitted, Terraform will assign a random, unique name. Conflicts with `name_prefix`.\n</code></pre>"},{"location":"resource-name-generation/#resource-name-generation-with-suffix","title":"Resource name generation with suffix","text":"<p>Some generated resource names require a fixed suffix (for example Amazon SNS FIFO topic names must end in <code>.fifo</code>). In these cases use <code>create.NameWithSuffix()</code> in the resource <code>Create</code> function and <code>create.NamePrefixFromNameWithSuffix()</code> in the resource <code>Read</code> function, e.g.</p> <pre><code>name := create.NameWithSuffix(d.Get(\"name\").(string), d.Get(\"name_prefix\").(string), \".fifo\")\n</code></pre> <p>and</p> <pre><code>d.Set(\"name\", resp.Name)\nd.Set(\"name_prefix\", create.NamePrefixFromNameWithSuffix(aws.StringValue(resp.Name), \".fifo\"))\n</code></pre> <p>There are also functions <code>acctest.CheckResourceAttrNameWithSuffixGenerated</code> and <code>acctest.CheckResourceAttrNameWithSuffixFromPrefix</code> for use in tests.</p>"},{"location":"resource-tagging/","title":"Adding Resource Tagging Support","text":"<p>AWS provides key-value metadata across many services and resources, which can be used for a variety of use cases including billing, ownership, and more. See the AWS Tagging Strategy page for more information about tagging at a high level.</p> <p>The Terraform AWS Provider supports default tags configured on the provider in addition to tags configured on the resource. Implementing tagging support for Terraform AWS Provider resources requires the following, each with its own section below:</p> <ul> <li>Generated Service Tagging Code: Each service has a <code>generate.go</code> file where generator directives live.   Through these directives and their flags, you can customize code generation for the service.   You can find the code that the tagging generator generates in a <code>tags_gen.go</code> file in a service, such as <code>internal/service/ec2/tags_gen.go</code>.   You should generally not need to edit the generator code itself (i.e., in <code>internal/generate/tags</code>).</li> <li>Resource Tagging Code Implementation: In the resource code (e.g., <code>internal/service/{service}/{thing}.go</code>),   implementation of <code>tags</code> and <code>tags_all</code> schema attributes,   along with implementation of <code>CustomizeDiff</code> in the resource definition and handling in <code>Create</code>, <code>Read</code>, and <code>Update</code> functions.</li> <li>Resource Tagging Acceptance Testing Implementation: In the resource acceptance testing (e.g., <code>internal/service/{service}/{thing}_test.go</code>),   implementation of new acceptance test function and configurations to exercise new tagging logic.</li> <li>Resource Tagging Documentation Implementation: In the resource documentation (e.g., <code>website/docs/r/service_thing.html.markdown</code>),   addition of <code>tags</code> argument and <code>tags_all</code> attributes.</li> </ul>"},{"location":"resource-tagging/#generating-tag-code-for-a-service","title":"Generating Tag Code for a Service","text":"<p>This step is generally only necessary for the first implementation and may have been previously completed.</p> <p>More details about this code generation, including fixes for potential error messages in this process, can be found in the <code>generate</code> package documentation.</p> <p>The generator will create several types of tagging-related code. All services that support tagging will generate the function <code>KeyValueTags</code>, which converts from service-specific structs returned by the AWS SDK into a common format used by the provider, and the function <code>Tags</code>, which converts from the common format back to the service-specific structs. In addition, many services have separate functions to list or update tags, so the corresponding <code>ListTags</code> and <code>UpdateTags</code> can be generated. Optionally, to retrieve a specific tag, you can generate the <code>GetTag</code> function.</p> <p>If the service directory does not contain a <code>generate.go</code> file, create one. This file must only contain generate directives and a package declaration (e.g., <code>package eks</code>). For examples of the <code>generate.go</code> file, many service directories contain one, e.g., <code>internal/service/eks/generate.go</code>.</p> <p>If the <code>generate.go</code> file does not contain a generate directive for tagging code, i.e., <code>//go:generate go run ../../generate/tags/main.go</code>, add it. Note that without flags, the directive itself will not do anything useful. You must not include more than one <code>generate/tags/main.go</code> directive, as subsequent directives will overwrite previous directives. To generate multiple types of tag code, use multiple flags with the directive.</p>"},{"location":"resource-tagging/#generating-tagging-types","title":"Generating Tagging Types","text":"<p>Determine how the service implements tagging: Some services will use a simple map style (<code>map[string]*string</code> in Go), while others will have a separate structure, often a <code>[]service.Tag</code> struct with <code>Key</code> and <code>Value</code> fields.</p> <p>If the service uses the simple map style, pass the flag <code>-ServiceTagsMap</code>.</p> <p>If the service uses a slice of structs, pass the flag <code>-ServiceTagsSlice</code>. If the name of the tag struct is not <code>Tag</code>, pass the flag <code>-TagType=&lt;struct name&gt;</code>. Note that the struct name is used without the package name. For example, the AppMesh service uses the struct <code>TagRef</code>, so the flag is <code>-TagType=TagRef</code>. If the key and value fields on the struct are not <code>Key</code> and <code>Value</code>, specify the names using the flags <code>-TagTypeKeyElem</code> and <code>-TagTypeValElem</code> respectively. For example, the KMS service uses the struct <code>Tag</code>, but the key and value fields are <code>TagKey</code> and <code>TagValue</code>, so the flags are <code>-TagTypeKeyElem=TagKey</code> and <code>-TagTypeValElem=TagValue</code>.</p> <p>Some services, such as EC2 and Auto Scaling, return a different type depending on the API call used to retrieve the tag. To indicate the additional type, include the flag <code>-TagType2=&lt;struct name&gt;</code>. For example, the Auto Scaling uses the struct <code>Tag</code> as part of resource calls, but returns the struct <code>TagDescription</code> from the <code>DescribeTags</code> API call. The flag used is <code>-TagType2=TagDescription</code>.</p> <p>For more details on flags for generating service keys, see the documentation for the tag generator</p>"},{"location":"resource-tagging/#generating-standalone-tag-listing-functions","title":"Generating Standalone Tag Listing Functions","text":"<p>If the service API uses a standalone function to retrieve tags instead of including them with the resource (usually a <code>ListTags</code> or <code>ListTagsForResource</code> API call), pass the flag <code>-ListTags</code>.</p> <p>If the API call is not <code>ListTagsForResource</code>, pass the flag <code>-ListTagsOp=&lt;API call name&gt;</code>. Note that this does not include the package name. For example, the Auto Scaling service uses the API call <code>DescribeTags</code>, so the flag is <code>-ListTagsOp=DescribeTags</code>.</p> <p>If the API call uses a field other than <code>ResourceArn</code> to identify the resource, pass the flag <code>-ListTagsInIDElem=&lt;field name&gt;</code>. For example, the CloudWatch service uses the field <code>ResourceARN</code>, so the flag is <code>-ListTagsInIDElem=ResourceARN</code>. Some API calls take a slice of identifiers instead of a single identifier. In this case, pass the flag <code>-ListTagsInIDNeedSlice=yes</code>.</p> <p>If the field containing the tags in the result of the API call is not named <code>Tags</code>, pass the flag <code>-ListTagsOutTagsElem=&lt;struct name&gt;</code>. For example, the CloudTrail service returns a nested structure, where the resulting flag is <code>-ListTagsOutTagsElem=ResourceTagList[0].TagsList</code>.</p> <p>In some cases, it can be useful to retrieve single tags. Pass the flag <code>-GetTag</code> to generate a function to do so.</p> <p>For more details on flags for generating tag listing functions, see the documentation for the tag generator</p>"},{"location":"resource-tagging/#generating-standalone-tag-updating-functions","title":"Generating Standalone Tag Updating Functions","text":"<p>If the service API uses a standalone function to update tags instead of including them when updating the resource (usually a <code>TagResource</code> and <code>UntagResource</code> API call), pass the flag <code>-UpdateTags</code>.</p> <p>If the API call to add tags is not <code>TagResource</code>, pass the flag <code>-TagOp=&lt;API call name&gt;</code>. Note that this does not include the package name. For example, the ElastiCache service uses the API call <code>AddTagsToResource</code>, so the flag is <code>-TagOp=AddTagsToResource</code>.</p> <p>If the API call to add tags uses a field other than <code>ResourceArn</code> to identify the resource, pass the flag <code>-TagInIDElem=&lt;field name&gt;</code>. For example, the EC2 service uses the field <code>Resources</code>, so the flag is <code>-TagInIDElem=Resources</code>. Some API calls take a slice of identifiers instead of a single identifier. In this case, pass the flag <code>-TagInIDNeedSlice=yes</code>.</p> <p>If the API call to remove tags is not <code>UntagResource</code>, pass the flag <code>-UntagOp=&lt;API call name&gt;</code>. Note that this does not include the package name. For example, the ElastiCache service uses the API call <code>RemoveTagsFromResource</code>, so the flag is <code>-UntagOp=RemoveTagsFromResource</code>.</p> <p>If the API call to remove tags uses a field other than <code>ResourceArn</code> to identify the resource, pass the flag <code>-UntagInTagsElem=&lt;field name&gt;</code>. For example, the Route 53 service uses the field <code>Keys</code>, so the flag is <code>-UntagInTagsElem=Keys</code>.</p> <p>For more details on flags for generating tag updating functions, see the documentation for the tag generator</p>"},{"location":"resource-tagging/#specifying-the-aws-sdk-for-go-version","title":"Specifying the AWS SDK for Go version","text":"<p>The vast majority of the Terraform AWS Provider is implemented using version 1 of the AWS SDK for Go. For new services, however, we have started to use version 2 of the SDK.</p> <p>By default, the generated code uses the AWS SDK for Go v1. To generate code using the AWS SDK for Go v2, pass the flag <code>-AwsSdkVersion=2</code>.</p> <p>For more information, see the documentation on AWS SDK versions.</p>"},{"location":"resource-tagging/#running-code-generation","title":"Running Code generation","text":"<p>Run the command <code>make gen</code> to run the code generators for the project. To ensure that the code compiles, run <code>make test</code>.</p>"},{"location":"resource-tagging/#resource-tagging-code-implementation","title":"Resource Tagging Code Implementation","text":""},{"location":"resource-tagging/#resource-schema","title":"Resource Schema","text":"<p>Add the following imports to the resource's Go source file:</p> <pre><code>imports (\n/* ... other imports ... */\ntftags \"github.com/hashicorp/terraform-provider-aws/internal/tags\"\n\"github.com/hashicorp/terraform-provider-aws/internal/verify\"\n)\n</code></pre> <p>Add the <code>tags</code> parameter and <code>tags_all</code> attribute to the schema. The <code>tags</code> parameter contains the tags set directly on the resource. The <code>tags_all</code> attribute contains union of the tags set directly on the resource and default tags configured on the provider.</p> <pre><code>func ResourceCluster() *schema.Resource {\nreturn &amp;schema.Resource{\n/* ... other configuration ... */\nSchema: map[string]*schema.Schema{\n/* ... other configuration ... */\n\"tags\":     tftags.TagsSchema(),\n\"tags_all\": tftags.TagsSchemaComputed(),\n},\n}\n}\n</code></pre> <p>The function <code>verify.SetTagsDiff</code> handles the combination of tags set on the resource and default tags, and must be added to the resource's <code>CustomizeDiff</code> function.</p> <p>If the resource has no other <code>CustomizeDiff</code> handler functions, set it directly:</p> <pre><code>func ResourceCluster() *schema.Resource {\nreturn &amp;schema.Resource{\n/* ... other configuration ... */\nCustomizeDiff: verify.SetTagsDiff,\n}\n}\n</code></pre> <p>Otherwise, if the resource already contains a <code>CustomizeDiff</code> function, append the <code>SetTagsDiff</code> via the <code>customdiff.All</code> method:</p> <pre><code>func ResourceExample() *schema.Resource {\nreturn &amp;schema.Resource{\n/* ... other configuration ... */\nCustomizeDiff: customdiff.All(\nresourceExampleCustomizeDiff,\nverify.SetTagsDiff,\n),\n}\n}\n</code></pre>"},{"location":"resource-tagging/#resource-create-operation","title":"Resource Create Operation","text":"<p>When creating a resource, some AWS APIs support passing tags in the Create call while others require setting the tags after the initial creation.</p> <p>If the API supports tagging on creation (e.g., the <code>Input</code> struct accepts a <code>Tags</code> field), implement the logic to convert the configuration tags into the service tags, e.g., with EKS Clusters:</p> <pre><code>// Typically declared near conn := /* ... */\ndefaultTagsConfig := meta.(*AWSClient).DefaultTagsConfig\ntags := defaultTagsConfig.MergeTags(tftags.New(ctx, d.Get(\"tags\").(map[string]interface{})))\n\ninput := &amp;eks.CreateClusterInput{\n/* ... other configuration ... */\nTags: Tags(tags.IgnoreAWS()),\n}\n</code></pre> <p>If the service API does not allow passing an empty list, the logic can be adjusted similar to:</p> <pre><code>// Typically declared near conn := /* ... */\ndefaultTagsConfig := meta.(*AWSClient).DefaultTagsConfig\ntags := defaultTagsConfig.MergeTags(tftags.New(ctx, d.Get(\"tags\").(map[string]interface{})))\n\ninput := &amp;eks.CreateClusterInput{\n/* ... other configuration ... */\n}\n\nif len(tags) &gt; 0 {\ninput.Tags = Tags(tags.IgnoreAWS())\n}\n</code></pre> <p>Otherwise, if the API does not support tagging on creation, implement the logic to convert the configuration tags into the service API call to tag a resource, e.g., with Device Farm device pools:</p> <pre><code>// Typically declared near conn := /* ... */\ndefaultTagsConfig := meta.(*AWSClient).DefaultTagsConfig\ntags := defaultTagsConfig.MergeTags(tftags.New(ctx, d.Get(\"tags\").(map[string]interface{})))\n\n/* ... creation steps ... */\n\nif len(tags) &gt; 0 {\nif err := UpdateTags(conn, d.Id(), nil, tags); err != nil {\nreturn fmt.Errorf(\"adding DeviceFarm Device Pool (%s) tags: %w\", d.Id(), err)\n}\n}\n</code></pre> <p>Some EC2 resources (e.g., <code>aws_ec2_fleet</code>) have a <code>TagSpecifications</code> field in the <code>InputStruct</code> instead of a <code>Tags</code> field. In these cases the <code>tagSpecificationsFromKeyValueTags()</code> helper function should be used. This example shows using <code>TagSpecifications</code>:</p> <pre><code>// Typically declared near conn := /* ... */\ndefaultTagsConfig := meta.(*AWSClient).DefaultTagsConfig\ntags := defaultTagsConfig.MergeTags(tftags.New(ctx, d.Get(\"tags\").(map[string]interface{})))\n\ninput := &amp;ec2.CreateFleetInput{\n/* ... other configuration ... */\nTagSpecifications: tagSpecificationsFromKeyValueTags(tags, ec2.ResourceTypeFleet),\n}\n</code></pre>"},{"location":"resource-tagging/#resource-read-operation","title":"Resource Read Operation","text":"<p>In the resource <code>Read</code> operation, implement the logic to convert the service tags to save them into the Terraform state for drift detection, e.g., with EKS Clusters:</p> <pre><code>// Typically declared near conn := /* ... */\ndefaultTagsConfig := meta.(*AWSClient).DefaultTagsConfig\nignoreTagsConfig := meta.(*AWSClient).IgnoreTagsConfig\n\n/* ... other d.Set(...) logic ... */\n\ntags := KeyValueTags(ctx, cluster.Tags).IgnoreAWS().IgnoreConfig(ignoreTagsConfig)\n\nif err := d.Set(\"tags\", tags.RemoveDefaultConfig(defaultTagsConfig).Map()); err != nil {\nreturn fmt.Errorf(\"setting tags: %w\", err)\n}\n\nif err := d.Set(\"tags_all\", tags.Map()); err != nil {\nreturn fmt.Errorf(\"setting tags_all: %w\", err)\n}\n</code></pre> <p>If the service API does not return the tags directly from reading the resource and requires a separate API call, use the generated <code>ListTags</code> function, e.g., with Athena Workgroups:</p> <pre><code>// Typically declared near conn := /* ... */\ndefaultTagsConfig := meta.(*AWSClient).DefaultTagsConfig\nignoreTagsConfig := meta.(*AWSClient).IgnoreTagsConfig\n\n/* ... other d.Set(...) logic ... */\n\ntags, err := ListTags(conn, arn.String())\n\nif err != nil {\nreturn fmt.Errorf(\"listing tags for resource (%s): %w\", arn, err)\n}\n\ntags = tags.IgnoreAWS().IgnoreConfig(ignoreTagsConfig)\n\nif err := d.Set(\"tags\", tags.RemoveDefaultConfig(defaultTagsConfig).Map()); err != nil {\nreturn fmt.Errorf(\"setting tags: %w\", err)\n}\n\nif err := d.Set(\"tags_all\", tags.Map()); err != nil {\nreturn fmt.Errorf(\"setting tags_all: %w\", err)\n}\n</code></pre>"},{"location":"resource-tagging/#resource-update-operation","title":"Resource Update Operation","text":"<p>In the resource <code>Update</code> operation, implement the logic to handle tagging updates, e.g., with EKS Clusters:</p> <pre><code>if d.HasChange(\"tags_all\") {\no, n := d.GetChange(\"tags_all\")\nif err := UpdateTags(conn, d.Get(\"arn\").(string), o, n); err != nil {\nreturn fmt.Errorf(\"updating tags: %w\", err)\n}\n}\n</code></pre> <p>If the resource <code>Update</code> function applies specific updates to attributes regardless of changes to tags, implement the following e.g., with IAM Policy:</p> <pre><code>if d.HasChangesExcept(\"tags\", \"tags_all\") {\n/* ... other logic ...*/\nrequest := &amp;iam.CreatePolicyVersionInput{\nPolicyArn:      aws.String(d.Id()),\nPolicyDocument: aws.String(d.Get(\"policy\").(string)),\nSetAsDefault:   aws.Bool(true),\n}\n\nif _, err := conn.CreatePolicyVersion(request); err != nil {\nreturn fmt.Errorf(\"updating IAM policy (%s): %w\", d.Id(), err)\n}\n}\n</code></pre>"},{"location":"resource-tagging/#resource-tagging-acceptance-testing-implementation","title":"Resource Tagging Acceptance Testing Implementation","text":"<p>In the resource testing (e.g., <code>internal/service/eks/cluster_test.go</code>), verify that existing resources without tagging are unaffected and do not have tags saved into their Terraform state. This should be done in the <code>_basic</code> acceptance test by adding one line similar to <code>resource.TestCheckResourceAttr(resourceName, \"tags.%\", \"0\"),</code> and one similar to <code>resource.TestCheckResourceAttr(resourceName, \"tags_all.%\", \"0\"),</code></p> <p>In the resource testing, implement a new test named <code>_tags</code> with associated configurations, that verifies creating the resource with tags and updating tags. E.g., EKS Clusters:</p> <pre><code>func TestAccEKSCluster_tags(t *testing.T) {\nctx := acctest.Context(t)\nvar cluster1, cluster2, cluster3 eks.Cluster\nrName := sdkacctest.RandomWithPrefix(acctest.ResourcePrefix)\nresourceName := \"aws_eks_cluster.test\"\n\nresource.ParallelTest(t, resource.TestCase{\nPreCheck:                 func() { acctest.PreCheck(t); testAccPreCheck(t) },\nErrorCheck:               acctest.ErrorCheck(t, eks.EndpointsID),\nProtoV5ProviderFactories: acctest.ProtoV5ProviderFactories,\nCheckDestroy:             testAccCheckClusterDestroy(ctx),\nSteps: []resource.TestStep{\n{\nConfig: testAccClusterConfig_tags1(rName, \"key1\", \"value1\"),\nCheck: resource.ComposeTestCheckFunc(\ntestAccCheckClusterExists(ctx, resourceName, &amp;cluster1),\nresource.TestCheckResourceAttr(resourceName, \"tags.%\", \"1\"),\nresource.TestCheckResourceAttr(resourceName, \"tags.key1\", \"value1\"),\n),\n},\n{\nResourceName:      resourceName,\nImportState:       true,\nImportStateVerify: true,\n},\n{\nConfig: testAccClusterConfig_tags2(rName, \"key1\", \"value1updated\", \"key2\", \"value2\"),\nCheck: resource.ComposeTestCheckFunc(\ntestAccCheckClusterExists(ctx, resourceName, &amp;cluster2),\nresource.TestCheckResourceAttr(resourceName, \"tags.%\", \"2\"),\nresource.TestCheckResourceAttr(resourceName, \"tags.key1\", \"value1updated\"),\nresource.TestCheckResourceAttr(resourceName, \"tags.key2\", \"value2\"),\n),\n},\n{\nConfig: testAccClusterConfig_tags1(rName, \"key2\", \"value2\"),\nCheck: resource.ComposeTestCheckFunc(\ntestAccCheckClusterExists(ctx, resourceName, &amp;cluster3),\nresource.TestCheckResourceAttr(resourceName, \"tags.%\", \"1\"),\nresource.TestCheckResourceAttr(resourceName, \"tags.key2\", \"value2\"),\n),\n},\n},\n})\n}\n\nfunc testAccClusterConfig_tags1(rName, tagKey1, tagValue1 string) string {\nreturn acctest.ConfigCompose(testAccClusterConfig_base(rName), fmt.Sprintf(`\nresource \"aws_eks_cluster\" \"test\" {\n  name     = %[1]q\n  role_arn = aws_iam_role.test.arn\n\n  tags = {\n    %[2]q = %[3]q\n  }\n\n  vpc_config {\n    subnet_ids = aws_subnet.test[*].id\n  }\n\n  depends_on = [aws_iam_role_policy_attachment.test-AmazonEKSClusterPolicy]\n}\n`, rName, tagKey1, tagValue1))\n}\n\nfunc testAccClusterConfig_tags2(rName, tagKey1, tagValue1, tagKey2, tagValue2 string) string {\nreturn acctest.ConfigCompose(testAccClusterConfig_base(rName), fmt.Sprintf(`\nresource \"aws_eks_cluster\" \"test\" {\n  name     = %[1]q\n  role_arn = aws_iam_role.test.arn\n\n  tags = {\n    %[2]q = %[3]q\n    %[4]q = %[5]q\n  }\n\n  vpc_config {\n    subnet_ids = aws_subnet.test[*].id\n  }\n\n  depends_on = [aws_iam_role_policy_attachment.test-AmazonEKSClusterPolicy]\n}\n`, rName, tagKey1, tagValue1, tagKey2, tagValue2))\n}\n</code></pre> <p>Verify all acceptance testing passes for the resource (e.g., <code>make testacc TESTS=TestAccEKSCluster_ PKG=eks</code>)</p>"},{"location":"resource-tagging/#resource-tagging-documentation-implementation","title":"Resource Tagging Documentation Implementation","text":"<p>In the resource documentation (e.g., <code>website/docs/r/eks_cluster.html.markdown</code>), add the following to the arguments reference:</p> <pre><code>* `tags` - (Optional) Key-value mapping of resource tags. If configured with a provider [`default_tags` configuration block](/docs/providers/aws/index.html#default_tags-configuration-block) present, tags with matching keys will overwrite those defined at the provider-level.\n</code></pre> <p>In the resource documentation (e.g., <code>website/docs/r/eks_cluster.html.markdown</code>), add the following to the attributes reference:</p> <pre><code>* `tags_all` - Map of tags assigned to the resource, including those inherited from the provider [`default_tags` configuration block](/docs/providers/aws/index.html#default_tags-configuration-block).\n</code></pre>"},{"location":"retries-and-waiters/","title":"Retries and Waiters","text":"<p>Terraform plugins may run into situations where calling the remote system after an operation may be necessary. These typically fall under three classes where:</p> <ul> <li>The request never reaches the remote system.</li> <li>The request reaches the remote system and responds that it cannot handle the request temporarily.</li> <li>The implementation of the remote system requires additional requests to ensure success.</li> </ul> <p>This guide describes the behavior of the Terraform AWS Provider and provides code implementations that help ensure success in each of these situations.</p>"},{"location":"retries-and-waiters/#terraform-plugin-sdk-functionality","title":"Terraform Plugin SDK Functionality","text":"<p>The Terraform Plugin SDK, which the AWS Provider uses, provides vital tools for handling consistency: the <code>resource.StateChangeConf{}</code> struct, and the retry functions, <code>resource.Retry()</code> and <code>resource.RetryContext()</code>. We will discuss these throughout the rest of this guide. Since they help keep the AWS Provider code consistent, we heavily prefer them over custom implementations.</p> <p>This guide goes beyond the Terraform Plugin SDK v2 documentation by providing additional context and emergent implementations specific to the Terraform AWS Provider.</p>"},{"location":"retries-and-waiters/#state-change-configuration-and-functions","title":"State Change Configuration and Functions","text":"<p>The <code>resource.StateChangeConf</code> type along with its receiver methods <code>WaitForState()</code> and <code>WaitForStateContext()</code> is a generic primitive for repeating operations in Terraform resource logic until desired value(s) are received. The \"state change\" in this case is generic to any value and not specific to the Terraform State. Among other functionality, it supports some of these desirable optional properties:</p> <ul> <li>Expecting specific value(s) while waiting for the target value(s) to be reached. Unexpected values are returned as an error which can be augmented with additional details.</li> <li>Expecting the target value(s) to be returned multiple times in succession.</li> <li>Allowing various polling configurations such as delaying the initial request and setting the time between polls.</li> </ul>"},{"location":"retries-and-waiters/#retry-functions","title":"Retry Functions","text":"<p>The <code>resource.Retry()</code> and <code>resource.RetryContext()</code> functions provide a simplified retry implementation around <code>resource.StateChangeConf</code>. Their most common use is for simple error-based retries.</p>"},{"location":"retries-and-waiters/#aws-request-handling","title":"AWS Request Handling","text":"<p>The Terraform AWS Provider's requests to AWS service APIs happen on top of Hypertext Transfer Protocol (HTTP). The following is a simplified description of the layers and handling that requests pass through:</p> <ul> <li>A Terraform resource calls an AWS Go SDK function.</li> <li>The AWS Go SDK generates an AWS-compatible HTTP request using the Go standard library <code>net/http</code> package. This includes the following:<ul> <li>Adding HTTP headers for authentication and signing of requests to ensure authenticity.</li> <li>Converting operation inputs into required HTTP URI parameters and/or request body type (XML or JSON).</li> <li>If debug logging is enabled, logging of the HTTP request.</li> </ul> </li> <li>The AWS Go SDK transmits the <code>net/http</code> request using Go's standard handling of the Operating System (OS) and Domain Name System (DNS) configuration.</li> <li>The AWS service potentially receives the request and responds, typically adding a request identifier HTTP header which can be used for AWS Support cases.</li> <li>The OS and Go <code>net/http</code> receive the response and pass it to the AWS Go SDK.</li> <li>The AWS Go SDK attempts to handle the response. This may include:<ul> <li>Parsing output</li> <li>Converting errors into operation errors (Go <code>error</code> type of wrapped <code>awserr.Error</code> type).</li> <li>Converting response elements into operation outputs (AWS Go SDK operation-specific types).</li> <li>Triggering automatic request retries based on default and custom logic.</li> </ul> </li> <li>The Terraform resource receives the response, including any output and errors, from the AWS Go SDK.</li> </ul> <p>The Terraform AWS Provider specific configuration for AWS Go SDK operation handling can be found in <code>aws/conns/conns.go</code> in this codebase and the <code>hashicorp/aws-sdk-go-base</code> codebase.</p> <p>NOTE: The section descibes the current handling with version 1 of the AWS Go SDK. In the future, this codebase will be migrated to version 2 of the AWS Go SDK. The newer version implements a very similar request flow but uses a simpler credential and request handling configuration. As such, the <code>aws-sdk-go-base</code> dependency will likely not receive further updates and will be removed after that migration.</p>"},{"location":"retries-and-waiters/#default-aws-go-sdk-retries","title":"Default AWS Go SDK Retries","text":"<p>In some situations, while handling a response, the AWS Go SDK automatically retries a request before returning the output and error. The retry mechanism implements an exponential backoff algorithm. The default conditions triggering automatic retries (implemented through <code>client.DefaultRetryer</code>) include:</p> <ul> <li>Certain network errors. A common exception to this is connection reset errors.</li> <li>HTTP status codes 429 and 5xx.</li> <li>Certain API error codes, which are common across various AWS services (e.g., <code>ThrottledException</code>). However, not all AWS services implement these error codes consistently. A common exception to this is certain expired credentials errors.</li> </ul> <p>By default, the Terraform AWS Provider sets the maximum number of AWS Go SDK retries based on the <code>max_retries</code> provider configuration. The provider configuration defaults to 25 and the exponential backoff roughly equates to one hour of retries. This very high default value was present before the Terraform AWS Provider codebase was split from Terraform CLI in version 0.10.</p> <p>NOTE: The section describes the current handling with version 1 of the AWS Go SDK. In the future, this codebase will be migrated to version 2 of the AWS Go SDK. The newer version implements additional retry conditions by default, such as consistently retrying all common network errors.</p> <p>NOTE: The section describes the current handling with Terraform Plugin SDK resource signatures without <code>context.Context</code>. In the future, this codebase will be migrated to the context-aware resource signatures which currently enforce a 20-minute default timeout that conflicts with the timeout with the default <code>max_retries</code> value. The Terraform Plugin SDK may be updated to support removing this default 20-minute timeout or the default retry mechanism described here will be updated to prevent context cancellation errors where possible.</p>"},{"location":"retries-and-waiters/#lower-network-error-retries","title":"Lower Network Error Retries","text":"<p>Given the very high default number of AWS Go SDK retries configured in the Terraform AWS Provider and the excessive wait that practitioners would face, the <code>hashicorp/aws-sdk-go-base</code> codebase lowers retries to 10 for certain network errors that typically cannot be remediated via retries. This roughly equates to 30 seconds of retries.</p>"},{"location":"retries-and-waiters/#terraform-aws-provider-service-retries","title":"Terraform AWS Provider Service Retries","text":"<p>The AWS Go SDK provides hooks for injecting custom logic into the service client handlers. We prefer this handling in situations where contributors would need to apply the retry behavior to many resources. For example, in cases where the AWS service API does not mark an error code as automatically retriable. The AWS Provider includes other retry-changing behaviors using this method. You can find them in the <code>aws/config.go</code> file. For example:</p> <pre><code>client.kafkaconn.Handlers.Retry.PushBack(func(r *request.Request) {\nif tfawserr.ErrMessageContains(r.Error, kafka.ErrCodeTooManyRequestsException, \"Too Many Requests\") {\nr.Retryable = aws.Bool(true)\n}\n})\n</code></pre>"},{"location":"retries-and-waiters/#eventual-consistency","title":"Eventual Consistency","text":"<p>Eventual consistency is a temporary condition where the remote system can return outdated information or errors due to not being strongly read-after-write consistent. This is a pattern found in remote systems that must be highly scaled for broad usage.</p> <p>Terraform expects any planned resource lifecycle change (create, update, destroy of the resource itself) and planned resource attribute value change to match after being applied. Conversely, operators typically expect that Terraform resources also implement the concept of drift detection for resources and their attributes, which requires reading information back from the remote system after an operation. A common implementation is refreshing the Terraform State information (<code>d.Set()</code>) during the <code>Read</code> function of a resource after <code>Create</code> and <code>Update</code>.</p> <p>These two concepts conflict with each other and require additional handling in Terraform resource logic as shown in the following sections. These issues are not reliably reproducible, especially in the case of writing acceptance testing, so they can be elusive with false positives to verify fixes.</p>"},{"location":"retries-and-waiters/#operation-specific-error-retries","title":"Operation Specific Error Retries","text":"<p>Even given a properly ordered Terraform configuration, eventual consistency can unexpectedly prevent downstream operations from succeeding. A simple retry after a few seconds resolves many of these issues. To reduce frustrating behavior for operators, wrap AWS Go SDK operations with the <code>resource.Retry()</code> or <code>resource.RetryContext()</code> functions. These retries should have a reasonably low timeout (typically two minutes but up to five minutes). Save them in a constant for reusability. These functions are preferably in line with the associated resource logic to remove any indirection with the code.</p> <p>Do not use this type of logic to overcome improperly ordered Terraform configurations. The approach may not work in larger environments.</p> <pre><code>// internal/service/example/wait.go (created if does not exist)\n\nconst (\n// Maximum amount of time to wait for Thing operation eventual consistency\nThingOperationTimeout = 2 * time.Minute\n)\n</code></pre> <pre><code>// internal/service/{service}/{thing}.go\n\n// ... Create, Read, Update, or Delete function ...\nerr := resource.Retry(ThingOperationTimeout, func() *resource.RetryError {\n_, err := conn./* ... AWS Go SDK operation with eventual consistency errors ... */\n\n// Retryable conditions which can be checked.\n// These must be updated to match the AWS service API error code and message.\nif tfawserr.ErrMessageContains(err, /* error code */, /* error message */) {\nreturn resource.RetryableError(err)\n}\n\nif err != nil {\nreturn resource.NonRetryableError(err)\n}\n\nreturn nil\n})\n\n// This check is important - it handles when the AWS Go SDK operation retries without returning.\n// e.g., any automatic retries due to network or throttling errors.\nif tfresource.TimedOut(err) {\n// The use of equals assignment (over colon equals) is also important here.\n// This overwrites the error variable to simplify logic.\n_, err = conn./* ... AWS Go SDK operation with IAM eventual consistency errors ... */\n}\n\nif err != nil {\nreturn fmt.Errorf(\"... error message context ... : %w\", err)\n}\n</code></pre> <p>NOTE: The section descibes the current handling with version 1 of the AWS Go SDK. In the future, this codebase will be migrated to version 2 of the AWS Go SDK. The newer version natively supports operation-specific retries in a more friendly manner, which may replace this type of implementation.</p>"},{"location":"retries-and-waiters/#iam-error-retries","title":"IAM Error Retries","text":"<p>A common eventual consistency issue is an error returned due to IAM permissions. The IAM service itself is eventually consistent along with the propagation of its components and permissions to other AWS services. For example, if the following operations occur in quick succession:</p> <ul> <li>Create an IAM Role</li> <li>Attach an IAM Policy to the IAM Role</li> <li>Reference the new IAM Role in another AWS service, such as creating a Lambda Function</li> </ul> <p>The last operation can receive varied API errors ranging from:</p> <ul> <li>IAM Role being reported as not existing</li> <li>IAM Role being reported as not having permissions for the other service to use it (assume role permissions)</li> <li>IAM Role being reported as not having sufficient permissions (inline or attached role permissions)</li> </ul> <p>Each AWS service API (and sometimes even operations within the same API) varies in the implementation of these errors. To handle them, it is recommended to use the Operation Specific Error Retries pattern. The Terraform AWS Provider implements a standard timeout constant of two minutes in the <code>internal/service/iam</code> package which should be used for all retry timeouts associated with IAM errors. This timeout was derived from years of Terraform operational experience with all AWS APIs.</p> <pre><code>// internal/service/{service}/{thing}.go\n\nimport (\n// ... other imports ...\ntfiam \"github.com/hashicorp/terraform-provider-aws/internal/service/iam\"\n)\n\n// ... Create and typically Update function ...\nerr := resource.Retry(iamwaiter.PropagationTimeout, func() *resource.RetryError {\n_, err := conn./* ... AWS Go SDK operation with IAM eventual consistency errors ... */\n\n// Example retryable condition\n// This must be updated to match the AWS service API error code and message.\nif tfawserr.ErrMessageContains(err, /* error code */, /* error message */) {\nreturn resource.RetryableError(err)\n}\n\nif err != nil {\nreturn resource.NonRetryableError(err)\n}\n\nreturn nil\n})\n\nif tfresource.TimedOut(err) {\n_, err = conn./* ... AWS Go SDK operation with IAM eventual consistency errors ... */\n}\n\nif err != nil {\nreturn fmt.Errorf(\"... error message context ... : %w\", err)\n}\n</code></pre>"},{"location":"retries-and-waiters/#asynchronous-operation-error-retries","title":"Asynchronous Operation Error Retries","text":"<p>Some remote system operations run asynchronously as detailed in the Asynchronous Operations section. In these cases, it is possible that the initial operation will immediately return as successful, but potentially return a retryable failure while checking the operation status that requires starting everything over. The handling for these is complicated by the fact that there are two timeouts, one for the retryable failure and one for the asynchronous operation status checking.</p> <p>The below code example highlights this situation for a resource creation that also exhibited IAM eventual consistency.</p> <pre><code>// internal/service/{service}/{thing}.go\n\nimport (\n// ... other imports ...\ntfiam \"github.com/hashicorp/terraform-provider-aws/internal/service/iam\"\n)\n\n// ... Create function ...\n\n// Underlying IAM eventual consistency errors can occur after the creation\n// operation. The goal is only retry these types of errors up to the IAM\n// timeout. Since the creation process is asynchronous and can take up to\n// its own timeout, we store a stop time upfront for checking.\niamwaiterStopTime := time.Now().Add(tfiam.PropagationTimeout)\n\n// Ensure to add IAM eventual consistency timeout in case of retries\nerr = resource.Retry(tfiam.PropagationTimeout+ThingOperationTimeout, func() *resource.RetryError {\n// Only retry IAM eventual consistency errors up to that timeout\niamwaiterRetry := time.Now().Before(iamwaiterStopTime)\n\n_, err := conn./* ... AWS Go SDK operation without eventual consistency errors ... */\n\nif err != nil {\nreturn resource.NonRetryableError(err)\n}\n\n_, err = ThingOperation(conn, d.Id())\n\nif err != nil {\nif iamwaiterRetry &amp;&amp; /* eventual consistency error checking */ {\nreturn resource.RetryableError(err)\n}\n\nreturn resource.NonRetryableError(err)\n}\n\nreturn nil\n})\n\nif tfresource.TimedOut(err) {\n_, err = conn./* ... AWS Go SDK operation without eventual consistency errors ... */\n\nif err != nil {\nreturn err\n}\n\n_, err = ThingOperation(conn, d.Id())\n\nif err != nil {\nreturn err\n}\n}\n</code></pre>"},{"location":"retries-and-waiters/#resource-lifecycle-retries","title":"Resource Lifecycle Retries","text":"<p>Resource lifecycle eventual consistency is a type of consistency issue that relates to the existence or state of an AWS infrastructure component. For example, if you create a resource and immediately try to get information about it, some AWS services and operations will return a \"not found\" error. Depending on the service and general AWS load, these errors can be frequent or rare.</p> <p>In order to avoid this issue, identify operations that make changes. Then, when calling any other operations that rely on the changes, account for the possibility that the AWS service has not yet fully realized them.</p> <p>A typical example is creating an AWS component. After creation, when attempting to read the component's information, provide logic to retry the read if the AWS service returns a \"not found\" error.</p> <p>The pattern that most resources should follow is to have the <code>Create</code> function return calling the <code>Read</code> function. This fills in computed attributes and ensures that the AWS service applied the configuration correctly. Add retry logic to the <code>Read</code> function to overcome the temporary condition on resource creation.</p> <p>Note that for eventually consistent resources, \"not found\" errors can still occur in the <code>Read</code> function even after implementing Resource Lifecycle Waiters for the Create function.</p> <pre><code>// internal/service/example/wait.go (created if does not exist)\n\nconst (\n// Maximum amount of time to wait for Thing eventual consistency on creation\nThingCreationTimeout = 2 * time.Minute\n)\n</code></pre> <pre><code>// internal/service/{service}/{thing}.go\n\nfunction ExampleThingCreate(d *schema.ResourceData, meta interface{}) error {\n// ...\nreturn ExampleThingRead(d, meta)\n}\n\nfunction ExampleThingRead(d *schema.ResourceData, meta interface{}) error {\nconn := meta.(*AWSClient).ExampleConn()\n\ninput := &amp;example.OperationInput{/* ... */}\n\nvar output *example.OperationOutput\nerr := resource.Retry(ThingCreationTimeout, func() *resource.RetryError {\nvar err error\noutput, err = conn.Operation(input)\n\n// Retry on any API \"not found\" errors, but only on new resources.\nif d.IsNewResource() &amp;&amp; tfawserr.ErrorCodeEquals(err, example.ErrCodeResourceNotFoundException) {\nreturn resource.RetryableError(err)\n}\n\nif err != nil {\nreturn resource.NonRetryableError(err)\n}\n\nreturn nil\n})\n\n// Retry AWS Go SDK operation if no response from automatic retries.\nif tfresource.TimedOut(err) {\noutput, err = exampleconn.Operation(input)\n}\n\n// Prevent confusing Terraform error messaging to operators by\n// Only ignoring API \"not found\" errors if not a new resource.\nif !d.IsNewResource() &amp;&amp; tfawserr.ErrorCodeEquals(err, example.ErrCodeNoSuchEntityException) {\nlog.Printf(\"[WARN] Example Thing (%s) not found, removing from state\", d.Id())\nd.SetId(\"\")\nreturn nil\n}\n\nif err != nil {\nreturn fmt.Errorf(\"reading Example Thing (%s): %w\", d.Id(), err)\n}\n\n// Prevent panics.\nif output == nil {\nreturn fmt.Errorf(\"reading Example Thing (%s): empty response\", d.Id())\n}\n\n// ... refresh Terraform state as normal ...\nd.Set(\"arn\", output.Arn)\n}\n</code></pre> <p>Some other general guidelines are:</p> <ul> <li>If the <code>Create</code> function uses <code>resource.StateChangeConf</code>, the underlying <code>resource.RefreshStateFunc</code> should <code>return nil, \"\", nil</code> instead of the API \"not found\" error. This way the <code>StateChangeConf</code> logic will automatically retry.</li> <li>If the <code>Create</code> function uses <code>resource.Retry()</code>, the API \"not found\" error should be caught and <code>return resource.RetryableError(err)</code> to automatically retry.</li> </ul> <p>In rare cases, it may be easier to duplicate all <code>Read</code> function logic in the <code>Create</code> function to handle all retries in one place.</p>"},{"location":"retries-and-waiters/#resource-attribute-value-waiters","title":"Resource Attribute Value Waiters","text":"<p>An emergent solution for handling eventual consistency with attribute values on updates is to introduce a custom <code>resource.StateChangeConf</code> and <code>resource.RefreshStateFunc</code> handlers. For example:</p> <pre><code>// internal/service/example/status.go (created if does not exist)\n\n// ThingAttribute fetches the Thing and its Attribute\nfunc ThingAttribute(conn *example.Example, id string) resource.StateRefreshFunc {\nreturn func() (interface{}, string, error) {\noutput, err := /* ... AWS Go SDK operation to fetch resource/value ... */\n\nif tfawserr.ErrCodeEquals(err, example.ErrCodeResourceNotFoundException) {\nreturn nil, \"\", nil\n}\n\nif err != nil {\nreturn nil, \"\", err\n}\n\nif output == nil {\nreturn nil, \"\", nil\n}\n\nreturn output, aws.StringValue(output.Attribute), nil\n}\n}\n</code></pre> <pre><code>// internal/service/example/wait.go (created if does not exist)\n\nconst (\nThingAttributePropagationTimeout = 2 * time.Minute\n)\n\n// ThingAttributeUpdated is an attribute waiter for ThingAttribute\nfunc ThingAttributeUpdated(conn *example.Example, id string, expectedValue string) (*example.Thing, error) {\nstateConf := &amp;resource.StateChangeConf{\nTarget:  []string{expectedValue},\nRefresh: ThingAttribute(conn, id),\nTimeout: ThingAttributePropagationTimeout,\n}\n\noutputRaw, err := stateConf.WaitForState()\n\nif output, ok := outputRaw.(*example.Thing); ok {\nreturn output, err\n}\n\nreturn nil, err\n}\n</code></pre> <pre><code>// internal/service/{service}/{thing}.go\n\nfunction ExampleThingUpdate(d *schema.ResourceData, meta interface{}) error {\n// ...\n\nd.HasChange(\"attribute\") {\n// ... AWS Go SDK logic to update attribute ...\n\nif _, err := ThingAttributeUpdated(conn, d.Id(), d.Get(\"attribute\").(string)); err != nil {\nreturn fmt.Errorf(\"waiting for Example Thing (%s) attribute update: %w\", d.Id(), err)\n}\n}\n\n// ...\n}\n</code></pre>"},{"location":"retries-and-waiters/#asynchronous-operations","title":"Asynchronous Operations","text":"<p>When you initiate a long-running operation, an AWS service may return a successful response immediately and continue working on the request asynchronously. A resource can track the status with a component-level field (e.g., <code>CREATING</code>, <code>UPDATING</code>, etc.) or an explicit tracking identifier.</p> <p>Terraform resources should wait for these background operations to complete. Failing to do so can introduce incomplete state information and downstream errors in other resources. In rare scenarios involving very long-running operations, operators may request a flag to skip the waiting. However, these should only be implemented case-by-case to prevent those previously mentioned confusing issues.</p>"},{"location":"retries-and-waiters/#aws-go-sdk-waiters","title":"AWS Go SDK Waiters","text":"<p>In limited cases, the AWS service API model includes the information to automatically generate a waiter function in the AWS Go SDK for an operation. These are typically named with the prefix <code>WaitUntil...</code>. If available, these functions can be used for an initial resource implementation. For example:</p> <pre><code>if err := conn.WaitUntilEndpointInService(input); err != nil {\nreturn fmt.Errorf(\"waiting for Example Thing (%s) ...: %w\", d.Id(), err)\n}\n</code></pre> <p>If it is necessary to customize the timeouts and polling, we generally prefer using Resource Lifecycle Waiters instead since they are more commonly used throughout the codebase.</p>"},{"location":"retries-and-waiters/#resource-lifecycle-waiters","title":"Resource Lifecycle Waiters","text":"<p>Most of the codebase uses <code>resource.StateChangeConf</code> and <code>resource.RefreshStateFunc</code> handlers for tracking either component level status fields or explicit tracking identifiers. These should be placed in the <code>internal/service/{SERVICE}</code> package and split into separate functions. For example:</p> <pre><code>// internal/service/example/status.go (created if does not exist)\n\n// ThingStatus fetches the Thing and its Status\nfunc ThingStatus(conn *example.Example, id string) resource.StateRefreshFunc {\nreturn func() (interface{}, string, error) {\noutput, err := /* ... AWS Go SDK operation to fetch resource/status ... */\n\nif tfawserr.ErrCodeEquals(err, example.ErrCodeResourceNotFoundException) {\nreturn nil, \"\", nil\n}\n\nif err != nil {\nreturn nil, \"\", err\n}\n\nif output == nil {\nreturn nil, \"\", nil\n}\n\nreturn output, aws.StringValue(output.Status), nil\n}\n}\n</code></pre> <pre><code>// internal/service/example/wait.go (created if does not exist)\n\nconst (\nThingCreationTimeout = 2 * time.Minute\nThingDeletionTimeout = 5 * time.Minute\n)\n\n// ThingCreated is a resource waiter for Thing creation\nfunc ThingCreated(conn *example.Example, id string) (*example.Thing, error) {\nstateConf := &amp;resource.StateChangeConf{\nPending: []string{example.StatusCreating},\nTarget:  []string{example.StatusCreated},\nRefresh: ThingStatus(conn, id),\nTimeout: ThingCreationTimeout,\n}\n\noutputRaw, err := stateConf.WaitForState()\n\nif output, ok := outputRaw.(*example.Thing); ok {\nreturn output, err\n}\n\nreturn nil, err\n}\n\n// ThingDeleted is a resource waiter for Thing deletion\nfunc ThingDeleted(conn *example.Example, id string) (*example.Thing, error) {\nstateConf := &amp;resource.StateChangeConf{\nPending: []string{example.StatusDeleting},\nTarget:  []string{}, // Use empty list if the resource disappears and does not have \"deleted\" status\nRefresh: ThingStatus(conn, id),\nTimeout: ThingDeletionTimeout,\n}\n\noutputRaw, err := stateConf.WaitForState()\n\nif output, ok := outputRaw.(*example.Thing); ok {\nreturn output, err\n}\n\nreturn nil, err\n}\n</code></pre> <pre><code>// internal/service/{service}/{thing}.go\n\nfunction ExampleThingCreate(d *schema.ResourceData, meta interface{}) error {\n// ... AWS Go SDK logic to create resource ...\n\nif _, err := ThingCreated(conn, d.Id()); err != nil {\nreturn fmt.Errorf(\"waiting for Example Thing (%s) creation: %w\", d.Id(), err)\n}\n\nreturn ExampleThingRead(d, meta)\n}\n\nfunction ExampleThingDelete(d *schema.ResourceData, meta interface{}) error {\n// ... AWS Go SDK logic to delete resource ...\n\nif _, err := ThingDeleted(conn, d.Id()); err != nil {\nreturn fmt.Errorf(\"waiting for Example Thing (%s) deletion: %w\", d.Id(), err)\n}\n\nreturn ExampleThingRead(d, meta)\n}\n</code></pre> <p>Typically, the AWS Go SDK should include constants for various status field values (e.g., <code>StatusCreating</code> for <code>CREATING</code>). If not, create them in a file named <code>internal/service/{SERVICE}/consts.go</code>.</p>"},{"location":"running-and-writing-acceptance-tests/","title":"Running and Writing Acceptance Tests","text":"<p>Terraform includes an acceptance test harness that does most of the repetitive work involved in testing a resource. For additional information about testing Terraform Providers, see the SDKv2 documentation.</p>"},{"location":"running-and-writing-acceptance-tests/#acceptance-tests-often-cost-money-to-run","title":"Acceptance Tests Often Cost Money to Run","text":"<p>Our acceptance test suite creates real resources, and as a result they cost real money to run. Because the resources only exist for a short period of time, the total amount of money required is usually a relatively small amount. That said there are particular services which are very expensive to run and its important to be prepared for those costs.</p> <p>Some services which can be cost prohibitive include (among others):</p> <ul> <li>WorkSpaces</li> <li>Glue</li> <li>OpenSearch</li> <li>RDS</li> <li>ACM (Amazon Certificate Manager)</li> <li>FSx</li> <li>Kinesis Analytics</li> <li>EC2</li> <li>ElastiCache</li> <li>Storage Gateway</li> </ul> <p>We don't want financial limitations to be a barrier to contribution, so if you are unable to pay to run acceptance tests for your contribution, mention this in your pull request. We will happily accept \"best effort\" implementations of acceptance tests and run them for you on our side. This might mean that your PR takes a bit longer to merge, but it most definitely is not a blocker for contributions.</p>"},{"location":"running-and-writing-acceptance-tests/#running-an-acceptance-test","title":"Running an Acceptance Test","text":"<p>Acceptance tests can be run using the <code>testacc</code> target in the Terraform <code>Makefile</code>. The individual tests to run can be controlled using a regular expression. Prior to running the tests provider configuration details such as access keys must be made available as environment variables.</p> <p>For example, to run an acceptance test against the Amazon Web Services provider, the following environment variables must be set:</p> <pre><code># Using a profile\nexport AWS_PROFILE=...\n# Otherwise\nexport AWS_ACCESS_KEY_ID=...\nexport AWS_SECRET_ACCESS_KEY=...\nexport AWS_DEFAULT_REGION=...\n</code></pre> <p>Please note that the default region for the testing is <code>us-west-2</code> and must be overridden via the <code>AWS_DEFAULT_REGION</code> environment variable, if necessary. This is especially important for testing AWS GovCloud (US), which requires:</p> <pre><code>export AWS_DEFAULT_REGION=us-gov-west-1\n</code></pre> <p>Tests can then be run by specifying a regular expression defining the tests to run and the package in which the tests are defined:</p> <pre><code>$ make testacc TESTS=TestAccCloudWatchDashboard_updateName PKG=cloudwatch\n==&gt; Checking that code complies with gofmt requirements...\nTF_ACC=1 go test ./internal/service/cloudwatch/... -v -count 1 -parallel 20 -run=TestAccCloudWatchDashboard_updateName -timeout 180m\n=== RUN   TestAccCloudWatchDashboard_updateName\n=== PAUSE TestAccCloudWatchDashboard_updateName\n=== CONT  TestAccCloudWatchDashboard_updateName\n--- PASS: TestAccCloudWatchDashboard_updateName (25.33s)\nPASS\nok      github.com/hashicorp/terraform-provider-aws/internal/service/cloudwatch 25.387s\n</code></pre> <p>Entire resource test suites can be targeted by using the naming convention to write the regular expression. For example, to run all tests of the <code>aws_cloudwatch_dashboard</code> resource rather than just the updateName test, you can start testing like this:</p> <pre><code>$ make testacc TESTS=TestAccCloudWatchDashboard PKG=cloudwatch\n==&gt; Checking that code complies with gofmt requirements...\nTF_ACC=1 go test ./internal/service/cloudwatch/... -v -count 1 -parallel 20 -run=TestAccCloudWatchDashboard -timeout 180m\n=== RUN   TestAccCloudWatchDashboard_basic\n=== PAUSE TestAccCloudWatchDashboard_basic\n=== RUN   TestAccCloudWatchDashboard_update\n=== PAUSE TestAccCloudWatchDashboard_update\n=== RUN   TestAccCloudWatchDashboard_updateName\n=== PAUSE TestAccCloudWatchDashboard_updateName\n=== CONT  TestAccCloudWatchDashboard_basic\n=== CONT  TestAccCloudWatchDashboard_updateName\n=== CONT  TestAccCloudWatchDashboard_update\n--- PASS: TestAccCloudWatchDashboard_basic (15.83s)\n--- PASS: TestAccCloudWatchDashboard_updateName (26.69s)\n--- PASS: TestAccCloudWatchDashboard_update (27.72s)\nPASS\nok      github.com/hashicorp/terraform-provider-aws/internal/service/cloudwatch 27.783s\n</code></pre> <p>Running acceptance tests requires version 0.12.26 or higher of the Terraform CLI to be installed.</p> <p>For advanced developers, the acceptance testing framework accepts some additional environment variables that can be used to control Terraform CLI binary selection, logging, and other behaviors. See the SDKv2 documentation for more information.</p> <p>Please Note: On macOS 10.14 and later (and some Linux distributions), the default user open file limit is 256. This may cause unexpected issues when running the acceptance testing since this can prevent various operations from occurring such as opening network connections to AWS. To view this limit, the <code>ulimit -n</code> command can be run. To update this limit, run <code>ulimit -n 1024</code>  (or higher).</p>"},{"location":"running-and-writing-acceptance-tests/#running-cross-account-tests","title":"Running Cross-Account Tests","text":"<p>Certain testing requires multiple AWS accounts. This additional setup is not typically required and the testing will return an error (shown below) if your current setup does not have the secondary AWS configuration:</p> <pre><code>$ make testacc TESTS=TestAccRDSInstance_DBSubnetGroupName_ramShared PKG=rds\nTF_ACC=1 go test ./internal/service/rds/... -v -count 1 -parallel 20 -run=TestAccRDSInstance_DBSubnetGroupName_ramShared -timeout 180m\n=== RUN   TestAccRDSInstance_DBSubnetGroupName_ramShared\n=== PAUSE TestAccRDSInstance_DBSubnetGroupName_ramShared\n=== CONT  TestAccRDSInstance_DBSubnetGroupName_ramShared\n    acctest.go:674: skipping test because at least one environment variable of [AWS_ALTERNATE_PROFILE AWS_ALTERNATE_ACCESS_KEY_ID] must be set. Usage: credentials for running acceptance testing in alternate AWS account.\n--- SKIP: TestAccRDSInstance_DBSubnetGroupName_ramShared (0.85s)\nPASS\nok      github.com/hashicorp/terraform-provider-aws/internal/service/rds        0.888s\n</code></pre> <p>Running these acceptance tests is the same as before, except the following additional AWS credential information is required:</p> <pre><code># Using a profile\nexport AWS_ALTERNATE_PROFILE=...\n# Otherwise\nexport AWS_ALTERNATE_ACCESS_KEY_ID=...\nexport AWS_ALTERNATE_SECRET_ACCESS_KEY=...\n</code></pre>"},{"location":"running-and-writing-acceptance-tests/#running-cross-region-tests","title":"Running Cross-Region Tests","text":"<p>Certain testing requires multiple AWS regions. Additional setup is not typically required because the testing defaults the second AWS region to <code>us-east-1</code> and the third AWS region to <code>us-east-2</code>.</p> <p>Running these acceptance tests is the same as before, but if you wish to override the second and third regions:</p> <pre><code>export AWS_ALTERNATE_REGION=...\nexport AWS_THIRD_REGION=...\n</code></pre>"},{"location":"running-and-writing-acceptance-tests/#running-only-short-tests","title":"Running Only Short Tests","text":"<p>Some tests have been manually marked as long-running (longer than 300 seconds) and can be skipped using the <code>-short</code> flag. However, we are adding long-running guards little by little and many services have no guarded tests.</p> <p>Where guards have been implemented, do not always skip long-running tests. However, for intermediate test runs during development, or to verify functionality unrelated to the specific long-running tests, skipping long-running tests makes work more efficient. We recommend that for the final test run before submitting a PR that you run affected tests without the <code>-short</code> flag.</p> <p>If you want to run only short-running tests, you can use either one of these equivalent statements. Note the use of <code>-short</code>.</p> <p>For example:</p> <pre><code>$ make testacc TESTS='TestAccECSTaskDefinition_' PKG=ecs TESTARGS=-short\n</code></pre> <p>Or:</p> <pre><code>$ TF_ACC=1 go test ./internal/service/ecs/... -v -count 1 -parallel 20 -run='TestAccECSTaskDefinition_' -short -timeout 180m\n</code></pre>"},{"location":"running-and-writing-acceptance-tests/#writing-an-acceptance-test","title":"Writing an Acceptance Test","text":"<p>Terraform has a framework for writing acceptance tests which minimizes the amount of boilerplate code necessary to use common testing patterns. This guide is meant to augment the general SDKv2 documentation with Terraform AWS Provider specific conventions and helpers.</p>"},{"location":"running-and-writing-acceptance-tests/#anatomy-of-an-acceptance-test","title":"Anatomy of an Acceptance Test","text":"<p>This section describes in detail how the Terraform acceptance testing framework operates with respect to the Terraform AWS Provider. We recommend those unfamiliar with this provider, or Terraform resource testing in general, take a look here first to generally understand how we interact with AWS and the resource code to verify functionality.</p> <p>The entry point to the framework is the <code>resource.ParallelTest()</code> function. This wraps our testing to work with the standard Go testing framework, while also preventing unexpected usage of AWS by requiring the <code>TF_ACC=1</code> environment variable. This function accepts a <code>TestCase</code> parameter, which has all the details about the test itself. For example, this includes the test steps (<code>TestSteps</code>) and how to verify resource deletion in the API after all steps have been run (<code>CheckDestroy</code>).</p> <p>Each <code>TestStep</code> proceeds by applying some Terraform configuration using the provider under test, and then verifying that results are as expected by making assertions using the provider API. It is common for a single test function to exercise both the creation of and updates to a single resource. Most tests follow a similar structure.</p> <ol> <li>Pre-flight checks are made to ensure that sufficient provider configuration    is available to be able to proceed - for example in an acceptance test    targeting AWS, <code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code> must be set prior    to running acceptance tests. This is common to all tests exercising a single    provider.</li> </ol> <p>Most assertion functions are defined out of band with the tests. This keeps the tests readable, and allows reuse of assertion functions across different tests of the same type of resource. The definition of a complete test looks like this:</p> <pre><code>func TestAccCloudWatchDashboard_basic(t *testing.T) {\nctx := acctest.Context(t)\nvar dashboard cloudwatch.GetDashboardOutput\nrInt := acctest.RandInt()\nresource.ParallelTest(t, resource.TestCase{\nPreCheck:                 func() { acctest.PreCheck(t) },\nErrorCheck:               acctest.ErrorCheck(t, cloudwatch.EndpointsID),\nProtoV5ProviderFactories: acctest.ProtoV5ProviderFactories,\nCheckDestroy:             testAccCheckDashboardDestroy(ctx),\nSteps: []resource.TestStep{\n{\nConfig: testAccDashboardConfig(rInt),\nCheck: resource.ComposeTestCheckFunc(\ntestAccCheckDashboardExists(ctx, \"aws_cloudwatch_dashboard.foobar\", &amp;dashboard),\nresource.TestCheckResourceAttr(\"aws_cloudwatch_dashboard.foobar\", \"dashboard_name\", testAccDashboardName(rInt)),\n),\n},\n},\n})\n}\n</code></pre> <p>When executing the test, the following steps are taken for each <code>TestStep</code>:</p> <ol> <li> <p>The Terraform configuration required for the test is applied. This is    responsible for configuring the resource under test, and any dependencies it    may have. For example, to test the <code>aws_cloudwatch_dashboard</code> resource, a valid configuration with the requisite fields is required. This results in configuration which looks like this:</p> <pre><code>resource \"aws_cloudwatch_dashboard\" \"foobar\" {\ndashboard_name = \"terraform-test-dashboard-%d\"\ndashboard_body = &lt;&lt;EOF\n  {\n    \"widgets\": [{\n      \"type\": \"text\",\n      \"x\": 0,\n      \"y\": 0,\n      \"width\": 6,\n      \"height\": 6,\n      \"properties\": {\n        \"markdown\": \"Hi there from Terraform: CloudWatch\"\n      }\n    }]\n  }\n  EOF\n}\n</code></pre> </li> <li> <p>Assertions are run using the provider API. These use the provider API    directly rather than asserting against the resource state. For example, to    verify that the <code>aws_cloudwatch_dashboard</code> described above was created    successfully, a test function like this is used:</p> <pre><code>func testAccCheckDashboardExists(ctx context.Context, n string, dashboard *cloudwatch.GetDashboardOutput) resource.TestCheckFunc {\nreturn func(s *terraform.State) error {\nrs, ok := s.RootModule().Resources[n]\nif !ok {\nreturn fmt.Errorf(\"Not found: %s\", n)\n}\n\nconn := acctest.Provider.Meta().(*conns.AWSClient).CloudWatchConn()\nparams := cloudwatch.GetDashboardInput{\nDashboardName: aws.String(rs.Primary.ID),\n}\n\nresp, err := conn.GetDashboardWithContext(ctx, &amp;params)\nif err != nil {\nreturn err\n}\n\n*dashboard = *resp\n\nreturn nil\n}\n}\n</code></pre> </li> </ol> <p>Notice that the only information used from the Terraform state is the ID of    the resource. For computed properties, we instead assert that the value saved in the Terraform state was the    expected value if possible. The testing framework provides helper functions    for several common types of check - for example:</p> <pre><code>```go\nresource.TestCheckResourceAttr(\"aws_cloudwatch_dashboard.foobar\", \"dashboard_name\", testAccDashboardName(rInt)),\n```\n</code></pre> <ol> <li> <p>The resources created by the test are destroyed. This step happens    automatically, and is the equivalent of calling <code>terraform destroy</code>.</p> </li> <li> <p>Assertions are made against the provider API to verify that the resources    have indeed been removed. If these checks fail, the test fails and reports    \"dangling resources\". The code to ensure that the <code>aws_cloudwatch_dashboard</code> shown    above has been destroyed looks like this:</p> <pre><code>func testAccCheckDashboardDestroy(ctx context.Context) resource.TestCheckFunc {\nreturn func(s *terraform.State) error {\nconn := acctest.Provider.Meta().(*conns.AWSClient).CloudWatchConn()\n\nfor _, rs := range s.RootModule().Resources {\nif rs.Type != \"aws_cloudwatch_dashboard\" {\ncontinue\n}\n\nparams := cloudwatch.GetDashboardInput{\nDashboardName: aws.String(rs.Primary.ID),\n}\n\n_, err := conn.GetDashboardWithContext(ctx, &amp;params)\nif err == nil {\nreturn fmt.Errorf(\"Dashboard still exists: %s\", rs.Primary.ID)\n}\nif !isDashboardNotFoundErr(err) {\nreturn err\n}\n}\n\nreturn nil\n}\n}\n</code></pre> </li> </ol> <p>These functions usually test only for the resource directly under test.</p>"},{"location":"running-and-writing-acceptance-tests/#resource-acceptance-testing","title":"Resource Acceptance Testing","text":"<p>Most resources that implement standard Create, Read, Update, and Delete functionality should follow the pattern below. Each test type has a section that describes them in more detail:</p> <ul> <li>basic: This represents the bare minimum verification that the resource can be created, read, deleted, and optionally imported.</li> <li>disappears: A test that verifies Terraform will offer to recreate a resource if it is deleted outside of Terraform (e.g., via the Console) instead of returning an error that it cannot be found.</li> <li>Per Attribute: A test that verifies the resource with a single additional argument can be created, read, optionally updated (or force resource recreation), deleted, and optionally imported.</li> </ul> <p>The leading sections below highlight additional recommended patterns.</p>"},{"location":"running-and-writing-acceptance-tests/#test-configurations","title":"Test Configurations","text":"<p>Most of the existing test configurations you will find in the Terraform AWS Provider are written in the following function-based style:</p> <pre><code>func TestAccExampleThing_basic(t *testing.T) {\n// ... omitted for brevity ...\n\nresource.ParallelTest(t, resource.TestCase{\n// ... omitted for brevity ...\nSteps: []resource.TestStep{\n{\nConfig: testAccExampleThingConfig(),\n// ... omitted for brevity ...\n},\n},\n})\n}\n\nfunc testAccExampleThingConfig() string {\nreturn `\nresource \"aws_example_thing\" \"test\" {\n  # ... omitted for brevity ...\n}\n`\n}\n</code></pre> <p>Even when no values need to be passed in to the test configuration, we have found this setup to be the most flexible for allowing that to be easily implemented. Any configurable values are handled via <code>fmt.Sprintf()</code>. Using <code>text/template</code> or other templating styles is explicitly forbidden.</p> <p>For consistency, resources in the test configuration should be named <code>resource \"...\" \"test\"</code> unless multiple of that resource are necessary.</p> <p>We discourage re-using test configurations across test files (except for some common configuration helpers we provide) as it is much harder to discover potential testing regressions.</p> <p>Please also note that the newline on the first line of the configuration (before <code>resource</code>) and the newline after the last line of configuration (after <code>}</code>) are important to allow test configurations to be easily combined without generating Terraform configuration language syntax errors.</p>"},{"location":"running-and-writing-acceptance-tests/#combining-test-configurations","title":"Combining Test Configurations","text":"<p>We include a helper function, <code>acctest.ConfigCompose()</code> for iteratively building and chaining test configurations together. It accepts any number of configurations to combine them. This simplifies a single resource's testing by allowing the creation of a \"base\" test configuration for all the other test configurations (if necessary) and also allows the maintainers to curate common configurations. Each of these is described in more detail in below sections.</p> <p>Please note that we do discourage excessive chaining of configurations such as implementing multiple layers of \"base\" configurations. Usually these configurations are harder for maintainers and other future readers to understand due to the multiple levels of indirection.</p>"},{"location":"running-and-writing-acceptance-tests/#base-test-configurations","title":"Base Test Configurations","text":"<p>If a resource requires the same Terraform configuration as a prerequisite for all test configurations, then a common pattern is implementing a \"base\" test configuration that is combined with each test configuration.</p> <p>For example:</p> <pre><code>func testAccExampleThingConfigBase() string {\nreturn `\nresource \"aws_iam_role\" \"test\" {\n  # ... omitted for brevity ...\n}\n\nresource \"aws_iam_role_policy\" \"test\" {\n  # ... omitted for brevity ...\n}\n`\n}\n\nfunc testAccExampleThingConfig() string {\nreturn acctest.ConfigCompose(\ntestAccExampleThingConfigBase(),\n`\nresource \"aws_example_thing\" \"test\" {\n  # ... omitted for brevity ...\n}\n`)\n}\n</code></pre>"},{"location":"running-and-writing-acceptance-tests/#available-common-test-configurations","title":"Available Common Test Configurations","text":"<p>These test configurations are typical implementations we have found or allow testing to implement best practices easier, since the Terraform AWS Provider testing is expected to run against various AWS Regions and Partitions.</p> <ul> <li><code>acctest.AvailableEC2InstanceTypeForRegion(\"type1\", \"type2\", ...)</code>: Typically used to replace hardcoded EC2 Instance Types. Uses <code>aws_ec2_instance_type_offering</code> data source to return an available EC2 Instance Type in preferred ordering. Reference the instance type via: <code>data.aws_ec2_instance_type_offering.available.instance_type</code>. Use <code>acctest.AvailableEC2InstanceTypeForRegionNamed(\"name\", \"type1\", \"type2\", ...)</code> to specify a name for the data source</li> <li><code>acctest.ConfigLatestAmazonLinuxHVMEBSAMI()</code>: Typically used to replace hardcoded EC2 Image IDs (<code>ami-12345678</code>). Uses <code>aws_ami</code> data source to find the latest Amazon Linux image. Reference the AMI ID via: <code>data.aws_ami.amzn-ami-minimal-hvm-ebs.id</code></li> </ul>"},{"location":"running-and-writing-acceptance-tests/#randomized-naming","title":"Randomized Naming","text":"<p>For AWS resources that require unique naming, the tests should implement a randomized name, typically coded as a <code>rName</code> variable in the test and passed as a parameter to creating the test configuration.</p> <p>For example:</p> <pre><code>func TestAccExampleThing_basic(t *testing.T) {\nrName := sdkacctest.RandomWithPrefix(acctest.ResourcePrefix)\n// ... omitted for brevity ...\n\nresource.ParallelTest(t, resource.TestCase{\n// ... omitted for brevity ...\nSteps: []resource.TestStep{\n{\nConfig: testAccExampleThingConfigName(rName),\n// ... omitted for brevity ...\n},\n},\n})\n}\n\nfunc testAccExampleThingConfigName(rName string) string {\nreturn fmt.Sprintf(`\nresource \"aws_example_thing\" \"test\" {\n  name = %[1]q\n}\n`, rName)\n}\n</code></pre> <p>Typically the <code>rName</code> is always the first argument to the test configuration function, if used, for consistency.</p> <p>Note that if <code>rName</code> (or any other variable) is used multiple times in the <code>fmt.Sprintf()</code> statement, do not repeat <code>rName</code> in the <code>fmt.Sprintf()</code> arguments. Using <code>fmt.Sprintf(..., rName, rName)</code>, for example, would not be correct. Instead, use the indexed <code>%[1]q</code> (or <code>%[x]q</code>, <code>%[x]s</code>, <code>%[x]t</code>, or <code>%[x]d</code>, where <code>x</code> represents the index number) verb multiple times. For example:</p> <pre><code>func testAccExampleThingConfigName(rName string) string {\nreturn fmt.Sprintf(`\nresource \"aws_example_thing\" \"test\" {\n  name = %[1]q\n\n  tags = {\n    Name = %[1]q\n  }\n}\n`, rName)\n}\n</code></pre>"},{"location":"running-and-writing-acceptance-tests/#other-recommended-variables","title":"Other Recommended Variables","text":"<p>We also typically recommend saving a <code>resourceName</code> variable in the test that contains the resource reference, e.g., <code>aws_example_thing.test</code>, which is repeatedly used in the checks.</p> <p>For example:</p> <pre><code>func TestAccExampleThing_basic(t *testing.T) {\n// ... omitted for brevity ...\nresourceName := \"aws_example_thing.test\"\n\nresource.ParallelTest(t, resource.TestCase{\n// ... omitted for brevity ...\nSteps: []resource.TestStep{\n{\n// ... omitted for brevity ...\nCheck: resource.ComposeTestCheckFunc(\ntestAccCheckExampleThingExists(ctx, resourceName),\nacctest.CheckResourceAttrRegionalARN(resourceName, \"arn\", \"example\", fmt.Sprintf(\"thing/%s\", rName)),\nresource.TestCheckResourceAttr(resourceName, \"description\", \"\"),\nresource.TestCheckResourceAttr(resourceName, \"name\", rName),\n),\n},\n{\nResourceName:      resourceName,\nImportState:       true,\nImportStateVerify: true,\n},\n},\n})\n}\n\n// below all TestAcc functions\n\nfunc testAccExampleThingConfigName(rName string) string {\nreturn fmt.Sprintf(`\nresource \"aws_example_thing\" \"test\" {\n  name = %[1]q\n}\n`, rName)\n}\n</code></pre>"},{"location":"running-and-writing-acceptance-tests/#basic-acceptance-tests","title":"Basic Acceptance Tests","text":"<p>Usually this test is implemented first. The test configuration should contain only required arguments (<code>Required: true</code> attributes) and it should check the values of all read-only attributes (<code>Computed: true</code> without <code>Optional: true</code>). If the resource supports it, it verifies import. It should NOT perform other <code>TestStep</code> such as updates or verify recreation.</p> <p>These are typically named <code>TestAcc{SERVICE}{THING}_basic</code>, e.g., <code>TestAccCloudWatchDashboard_basic</code></p> <p>For example:</p> <pre><code>func TestAccExampleThing_basic(t *testing.T) {\nctx := acctest.Context(t)\nrName := sdkacctest.RandomWithPrefix(acctest.ResourcePrefix)\nresourceName := \"aws_example_thing.test\"\n\nresource.ParallelTest(t, resource.TestCase{\nPreCheck:                 func() { acctest.PreCheck(t) },\nErrorCheck:               acctest.ErrorCheck(t, service.EndpointsID),\nProtoV5ProviderFactories: acctest.ProtoV5ProviderFactories,\nCheckDestroy:             testAccCheckExampleThingDestroy(ctx),\nSteps: []resource.TestStep{\n{\nConfig: testAccExampleThingConfigName(rName),\nCheck: resource.ComposeTestCheckFunc(\ntestAccCheckExampleThingExists(ctx, resourceName),\nacctest.CheckResourceAttrRegionalARN(resourceName, \"arn\", \"example\", fmt.Sprintf(\"thing/%s\", rName)),\nresource.TestCheckResourceAttr(resourceName, \"description\", \"\"),\nresource.TestCheckResourceAttr(resourceName, \"name\", rName),\n),\n},\n{\nResourceName:      resourceName,\nImportState:       true,\nImportStateVerify: true,\n},\n},\n})\n}\n\n// below all TestAcc functions\n\nfunc testAccExampleThingConfigName(rName string) string {\nreturn fmt.Sprintf(`\nresource \"aws_example_thing\" \"test\" {\n  name = %[1]q\n}\n`, rName)\n}\n</code></pre>"},{"location":"running-and-writing-acceptance-tests/#prechecks","title":"PreChecks","text":"<p>Acceptance test cases have a PreCheck. The PreCheck ensures that the testing environment meets certain preconditions. If the environment does not meet the preconditions, Go skips the test. Skipping a test avoids reporting a failure and wasting resources where the test cannot succeed.</p> <p>Here is an example of the default PreCheck:</p> <pre><code>func TestAccExampleThing_basic(t *testing.T) {\nrName := sdkacctest.RandomWithPrefix(acctest.ResourcePrefix)\nresourceName := \"aws_example_thing.test\"\n\nresource.ParallelTest(t, resource.TestCase{\nPreCheck:     func() { acctest.PreCheck(t) },\n// ... additional checks follow ...\n})\n}\n</code></pre> <p>Extend the default PreCheck by adding calls to functions in the anonymous PreCheck function. The functions can be existing functions in the provider or custom functions you add for new capabilities.</p>"},{"location":"running-and-writing-acceptance-tests/#standard-provider-prechecks","title":"Standard Provider PreChecks","text":"<p>If you add a new test that has preconditions which are checked by an existing provider function, use that standard PreCheck instead of creating a new one. Some existing tests are missing standard PreChecks and you can help by adding them where appropriate.</p> <p>These are some of the standard provider PreChecks:</p> <ul> <li><code>acctest.PreCheckPartitionHasService(t *testing.T, serviceID string)</code> checks whether the current partition lists the service as part of its offerings. Note: AWS may not add new or public preview services to the service list immediately. This function will return a false positive in that case.</li> <li><code>acctest.PreCheckOrganizationsAccount(ctx context.Context, t *testing.T)</code> checks whether the current account can perform AWS Organizations tests.</li> <li><code>acctest.PreCheckAlternateAccount(t *testing.T)</code> checks whether the environment is set up for tests across accounts.</li> <li><code>acctest.PreCheckMultipleRegion(t *testing.T, regions int)</code> checks whether the environment is set up for tests across regions.</li> </ul> <p>This is an example of using a standard PreCheck function. For an established service, such as WAF or FSx, use <code>acctest.PreCheckPartitionHasService()</code> and the service endpoint ID to check that a partition supports the service.</p> <pre><code>func TestAccExampleThing_basic(t *testing.T) {\nrName := sdkacctest.RandomWithPrefix(acctest.ResourcePrefix)\nresourceName := \"aws_example_thing.test\"\n\nresource.ParallelTest(t, resource.TestCase{\nPreCheck:     func() { acctest.PreCheck(t); acctest.PreCheckPartitionHasService(t, waf.EndpointsID) },\n// ... additional checks follow ...\n})\n}\n</code></pre>"},{"location":"running-and-writing-acceptance-tests/#custom-prechecks","title":"Custom PreChecks","text":"<p>In situations where standard PreChecks do not test for the required preconditions, create a custom PreCheck.</p> <p>Below is an example of adding a custom PreCheck function. For a new or preview service that AWS does not include in the partition service list yet, you can verify the existence of the service with a simple read-only request (e.g., list all X service things). (For acceptance tests of established services, use <code>acctest.PreCheckPartitionHasService()</code> instead.)</p> <pre><code>func TestAccExampleThing_basic(t *testing.T) {\nctx := acctest.Context(t)\nrName := sdkacctest.RandomWithPrefix(acctest.ResourcePrefix)\nresourceName := \"aws_example_thing.test\"\n\nresource.ParallelTest(t, resource.TestCase{\nPreCheck:     func() { acctest.PreCheck(t), testAccPreCheckExample(ctx, t) },\n// ... additional checks follow ...\n})\n}\n\nfunc testAccPreCheckExample(ctx context.Context, t *testing.T) {\nconn := acctest.Provider.Meta().(*conns.AWSClient).ExampleConn()\ninput := &amp;example.ListThingsInput{}\n_, err := conn.ListThingsWithContext(ctx, input)\nif testAccPreCheckSkipError(err) {\nt.Skipf(\"skipping acceptance testing: %s\", err)\n}\nif err != nil {\nt.Fatalf(\"unexpected PreCheck error: %s\", err)\n}\n}\n</code></pre>"},{"location":"running-and-writing-acceptance-tests/#errorchecks","title":"ErrorChecks","text":"<p>Acceptance test cases have an ErrorCheck. The ErrorCheck provides a chance to take a look at errors before the test fails. While most errors should result in test failure, some should not. For example, an error that indicates an API operation is not supported in a particular region should cause the test to skip instead of fail. Since errors should flow through the ErrorCheck, do not handle the vast majority of failing conditions. Instead, in ErrorCheck, focus on the rare errors that should cause a test to skip, or in other words, be ignored.</p>"},{"location":"running-and-writing-acceptance-tests/#common-errorcheck","title":"Common ErrorCheck","text":"<p>In many situations, the common ErrorCheck is sufficient. It will skip tests for several normal occurrences such as when AWS reports a feature is not supported in the current region.</p> <p>Here is an example of the common ErrorCheck:</p> <pre><code>func TestAccExampleThing_basic(t *testing.T) {\nrName := sdkacctest.RandomWithPrefix(acctest.ResourcePrefix)\nresourceName := \"aws_example_thing.test\"\n\nresource.ParallelTest(t, resource.TestCase{\n// PreCheck\nErrorCheck:   acctest.ErrorCheck(t, service.EndpointsID),\n// ... additional checks follow ...\n})\n}\n</code></pre>"},{"location":"running-and-writing-acceptance-tests/#service-specific-errorchecks","title":"Service-Specific ErrorChecks","text":"<p>However, some services have special conditions that aren't caught by the common ErrorCheck. In these cases, you can create a service-specific ErrorCheck.</p> <p>To add a service-specific ErrorCheck, follow these steps:</p> <ol> <li>Make sure there is not already an ErrorCheck for the service you have in mind. For example, search the codebase for <code>acctest.RegisterServiceErrorCheckFunc(service.EndpointsID</code> replacing \"service\" with the package name of the service you're working on (e.g., <code>ec2</code>). If there is already an ErrorCheck for the service, add to the existing service-specific ErrorCheck.</li> <li>Create the service-specific ErrorCheck in an <code>_test.go</code> file for the service. See the example below.</li> <li>Register the new service-specific ErrorCheck in the <code>init()</code> at the top of the <code>_test.go</code> file. See the example below.</li> </ol> <p>An example of adding a service-specific ErrorCheck:</p> <pre><code>// just after the imports, create or add to the init() function\nfunc init() {\nacctest.RegisterServiceErrorCheck(service.EndpointsID, testAccErrorCheckSkipService)\n}\n\n// ... additional code and tests ...\n\n// this is the service-specific ErrorCheck\nfunc testAccErrorCheckSkipService(t *testing.T) resource.ErrorCheckFunc {\nreturn acctest.ErrorCheckSkipMessagesContaining(t,\n\"Error message specific to the service that indicates unsupported features\",\n\"You can include from one to many portions of error messages\",\n\"Be careful to not inadvertently capture errors that should not be skipped\",\n)\n}\n</code></pre>"},{"location":"running-and-writing-acceptance-tests/#long-running-test-guards","title":"Long-Running Test Guards","text":"<p>For any acceptance tests that typically run longer than 300 seconds (5 minutes), add a <code>-short</code> test guard at the top of the test function.</p> <p>For example:</p> <pre><code>func TestAccExampleThing_longRunningTest(t *testing.T) {\nif testing.Short() {\nt.Skip(\"skipping long-running test in short mode\")\n}\n\n// ... omitted for brevity ...\n\nresource.ParallelTest(t, resource.TestCase{\n// ... omitted for brevity ...\n})\n}\n</code></pre> <p>When running acceptances tests, tests with these guards can be skipped using the Go <code>-short</code> flag. See Running Only Short Tests for examples.</p>"},{"location":"running-and-writing-acceptance-tests/#disappears-acceptance-tests","title":"Disappears Acceptance Tests","text":"<p>This test is generally implemented second. It is straightforward to setup once the basic test is passing since it can reuse that test configuration. It prevents a common bug report with Terraform resources that error when they can not be found (e.g., deleted outside Terraform).</p> <p>These are typically named <code>TestAcc{SERVICE}{THING}_disappears</code>, e.g., <code>TestAccCloudWatchDashboard_disappears</code></p> <p>For example:</p> <pre><code>func TestAccExampleThing_disappears(t *testing.T) {\nctx := acctest.Context(t)\nrName := sdkacctest.RandomWithPrefix(acctest.ResourcePrefix)\nresourceName := \"aws_example_thing.test\"\n\nresource.ParallelTest(t, resource.TestCase{\nPreCheck:                 func() { acctest.PreCheck(t) },\nErrorCheck:               acctest.ErrorCheck(t, service.EndpointsID),\nProtoV5ProviderFactories: acctest.ProtoV5ProviderFactories,\nCheckDestroy:             testAccCheckExampleThingDestroy(ctx),\nSteps: []resource.TestStep{\n{\nConfig: testAccExampleThingConfigName(rName),\nCheck: resource.ComposeTestCheckFunc(\ntestAccCheckExampleThingExists(ctx, resourceName, &amp;job),\nacctest.CheckResourceDisappears(ctx, acctest.Provider, ResourceExampleThing(), resourceName),\n),\nExpectNonEmptyPlan: true,\n},\n},\n})\n}\n</code></pre> <p>If this test does fail, the fix for this is generally adding error handling immediately after the <code>Read</code> API call that catches the error and tells Terraform to remove the resource before returning the error:</p> <pre><code>output, err := conn.GetThing(input)\n\nif !d.IsNewResource() &amp;&amp; tfresource.NotFound(err) {\nlog.Printf(\"[WARN] Example Thing (%s) not found, removing from state\", d.Id())\nd.SetId(\"\")\nreturn nil\n}\n\nif err != nil {\nreturn fmt.Errorf(\"reading Example Thing (%s): %w\", d.Id(), err)\n}\n</code></pre> <p>For children resources that are encapsulated by a parent resource, it is also preferable to verify that removing the parent resource will not generate an error either. These are typically named <code>TestAcc{SERVICE}{THING}_disappears_{PARENT}</code>, e.g., <code>TestAccRoute53ZoneAssociation_disappears_Vpc</code></p> <pre><code>func TestAccExampleChildThing_disappears_ParentThing(t *testing.T) {\nctx := acctest.Context(t)\nrName := sdkacctest.RandomWithPrefix(acctest.ResourcePrefix)\nparentResourceName := \"aws_example_parent_thing.test\"\nresourceName := \"aws_example_child_thing.test\"\n\nresource.ParallelTest(t, resource.TestCase{\nPreCheck:                 func() { acctest.PreCheck(t) },\nErrorCheck:               acctest.ErrorCheck(t, service.EndpointsID),\nProtoV5ProviderFactories: acctest.ProtoV5ProviderFactories,\nCheckDestroy:             testAccCheckExampleChildThingDestroy(ctx),\nSteps: []resource.TestStep{\n{\nConfig: testAccExampleThingConfigName(rName),\nCheck: resource.ComposeTestCheckFunc(\ntestAccCheckExampleThingExists(ctx, resourceName),\nacctest.CheckResourceDisappears(ctx, acctest.Provider, ResourceExampleParentThing(), parentResourceName),\n),\nExpectNonEmptyPlan: true,\n},\n},\n})\n}\n</code></pre>"},{"location":"running-and-writing-acceptance-tests/#per-attribute-acceptance-tests","title":"Per Attribute Acceptance Tests","text":"<p>These are typically named <code>TestAcc{SERVICE}{THING}_{ATTRIBUTE}</code>, e.g., <code>TestAccCloudWatchDashboard_Name</code></p> <p>For example:</p> <pre><code>func TestAccExampleThing_Description(t *testing.T) {\nctx := acctest.Context(t)\nrName := sdkacctest.RandomWithPrefix(acctest.ResourcePrefix)\nresourceName := \"aws_example_thing.test\"\n\nresource.ParallelTest(t, resource.TestCase{\nPreCheck:                 func() { acctest.PreCheck(t) },\nErrorCheck:               acctest.ErrorCheck(t, service.EndpointsID),\nProtoV5ProviderFactories: acctest.ProtoV5ProviderFactories,\nCheckDestroy:             testAccCheckExampleThingDestroy(ctx),\nSteps: []resource.TestStep{\n{\nConfig: testAccExampleThingConfigDescription(rName, \"description1\"),\nCheck: resource.ComposeTestCheckFunc(\ntestAccCheckExampleThingExists(ctx, resourceName),\nresource.TestCheckResourceAttr(resourceName, \"description\", \"description1\"),\n),\n},\n{\nResourceName:      resourceName,\nImportState:       true,\nImportStateVerify: true,\n},\n{\nConfig: testAccExampleThingConfigDescription(rName, \"description2\"),\nCheck: resource.ComposeTestCheckFunc(\ntestAccCheckExampleThingExists(ctx, resourceName),\nresource.TestCheckResourceAttr(resourceName, \"description\", \"description2\"),\n),\n},\n},\n})\n}\n\n// below all TestAcc functions\n\nfunc testAccExampleThingConfigDescription(rName string, description string) string {\nreturn fmt.Sprintf(`\nresource \"aws_example_thing\" \"test\" {\n  description = %[2]q\n  name        = %[1]q\n}\n`, rName, description)\n}\n</code></pre>"},{"location":"running-and-writing-acceptance-tests/#cross-account-acceptance-tests","title":"Cross-Account Acceptance Tests","text":"<p>When testing requires AWS infrastructure in a second AWS account, the below changes to the normal setup will allow the management or reference of resources and data sources across accounts:</p> <ul> <li>In the <code>PreCheck</code> function, include <code>acctest.PreCheckOrganizationsAccount(ctx, t)</code> to ensure a standardized set of information is required for cross-account testing credentials</li> <li>Switch usage of <code>ProtoV5ProviderFactories: acctest.ProtoV5ProviderFactories</code> to <code>ProtoV5ProviderFactories: acctest.ProtoV5FactoriesAlternate(ctx, t)</code></li> <li>Add <code>acctest.ConfigAlternateAccountProvider()</code> to the test configuration and use <code>provider = awsalternate</code> for cross-account resources. The resource that is the focus of the acceptance test should not use the alternate provider identification to simplify the testing setup.</li> <li>For any <code>TestStep</code> that includes <code>ImportState: true</code>, add the <code>Config</code> that matches the previous <code>TestStep</code> <code>Config</code></li> </ul> <p>An example acceptance test implementation can be seen below:</p> <pre><code>func TestAccExample_basic(t *testing.T) {\nctx := acctest.Context(t)\nresourceName := \"aws_example.test\"\n\nresource.ParallelTest(t, resource.TestCase{\nPreCheck: func() {\nacctest.PreCheck(t)\nacctest.PreCheckOrganizationsAccount(ctx, t)\n},\nErrorCheck:               acctest.ErrorCheck(t, service.EndpointsID),\nProtoV5ProviderFactories: acctest.ProtoV5FactoriesAlternate(ctx, t),\nCheckDestroy:             testAccCheckExampleDestroy(ctx),\nSteps: []resource.TestStep{\n{\nConfig: testAccExampleConfig(),\nCheck: resource.ComposeTestCheckFunc(\ntestAccCheckExampleExists(ctx, resourceName),\n// ... additional checks ...\n),\n},\n{\nConfig:            testAccExampleConfig(),\nResourceName:      resourceName,\nImportState:       true,\nImportStateVerify: true,\n},\n},\n})\n}\n\nfunc testAccExampleConfig() string {\nreturn acctest.ConfigAlternateAccountProvider() + fmt.Sprintf(`\n# Cross account resources should be handled by the cross account provider.\n# The standardized provider block to use is awsalternate as seen below.\nresource \"aws_cross_account_example\" \"test\" {\n  provider = awsalternate\n\n  # ... configuration ...\n}\n\n# The resource that is the focus of the testing should be handled by the default provider,\n# which is automatically done by not specifying the provider configuration in the resource.\nresource \"aws_example\" \"test\" {\n  # ... configuration ...\n}\n`)\n}\n</code></pre> <p>Searching for usage of <code>acctest.PreCheckOrganizationsAccount</code> in the codebase will yield real world examples of this setup in action.</p>"},{"location":"running-and-writing-acceptance-tests/#cross-region-acceptance-tests","title":"Cross-Region Acceptance Tests","text":"<p>When testing requires AWS infrastructure in a second or third AWS region, the below changes to the normal setup will allow the management or reference of resources and data sources across regions:</p> <ul> <li>In the <code>PreCheck</code> function, include <code>acctest.PreCheckMultipleRegion(t, ###)</code> to ensure a standardized set of information is required for cross-region testing configuration. If the infrastructure in the second AWS region is also in a second AWS account also include <code>acctest.PreCheckOrganizationsAccount(ctx, t)</code></li> <li>Switch usage of <code>ProtoV5ProviderFactories: acctest.ProtoV5ProviderFactories</code> to <code>ProtoV5ProviderFactories: acctest.ProtoV5FactoriesMultipleRegions(ctx, t, 2)</code> (where the last parameter is number of regions, 2 or 3)</li> <li>Add <code>acctest.ConfigMultipleRegionProvider(###)</code> to the test configuration and use <code>provider = awsalternate</code> (and potentially <code>provider = awsthird</code>) for cross-region resources. The resource that is the focus of the acceptance test should not use the alternative providers to simplify the testing setup. If the infrastructure in the second AWS region is also in a second AWS account use <code>testAccAlternateAccountAlternateRegionProviderConfig()</code> (EC2) instead</li> <li>For any <code>TestStep</code> that includes <code>ImportState: true</code>, add the <code>Config</code> that matches the previous <code>TestStep</code> <code>Config</code></li> </ul> <p>An example acceptance test implementation can be seen below:</p> <pre><code>func TestAccExample_basic(t *testing.T) {\nctx := acctest.Context(t)\nvar providers []*schema.Provider\nresourceName := \"aws_example.test\"\n\nresource.ParallelTest(t, resource.TestCase{\nPreCheck: func() {\nacctest.PreCheck(t)\nacctest.PreCheckMultipleRegion(t, 2)\n},\nErrorCheck:               acctest.ErrorCheck(t, service.EndpointsID),\nProtoV5ProviderFactories: acctest.ProtoV5FactoriesMultipleRegions(ctx, t, 2),\nCheckDestroy:             testAccCheckExampleDestroy(ctx),\nSteps: []resource.TestStep{\n{\nConfig: testAccExampleConfig(),\nCheck: resource.ComposeTestCheckFunc(\ntestAccCheckExampleExists(ctx, resourceName),\n// ... additional checks ...\n),\n},\n{\nConfig:            testAccExampleConfig(),\nResourceName:      resourceName,\nImportState:       true,\nImportStateVerify: true,\n},\n},\n})\n}\n\nfunc testAccExampleConfig() string {\nreturn acctest.ConfigMultipleRegionProvider(2) + fmt.Sprintf(`\n# Cross region resources should be handled by the cross region provider.\n# The standardized provider is awsalternate as seen below.\nresource \"aws_cross_region_example\" \"test\" {\n  provider = awsalternate\n\n  # ... configuration ...\n}\n\n# The resource that is the focus of the testing should be handled by the default provider,\n# which is automatically done by not specifying the provider configuration in the resource.\nresource \"aws_example\" \"test\" {\n  # ... configuration ...\n}\n`)\n}\n</code></pre> <p>Searching for usage of <code>acctest.PreCheckMultipleRegion</code> in the codebase will yield real world examples of this setup in action.</p>"},{"location":"running-and-writing-acceptance-tests/#service-specific-region-acceptance-testing","title":"Service-Specific Region Acceptance Testing","text":"<p>Certain AWS service APIs are only available in specific AWS regions. For example as of this writing, the <code>pricing</code> service is available in <code>ap-south-1</code> and <code>us-east-1</code>, but no other regions or partitions. When encountering these types of services, the acceptance testing can be setup to automatically detect the correct region(s), while skipping the testing in unsupported partitions.</p> <p>To prepare the shared service functionality, create a file named <code>internal/service/{SERVICE}/acc_test.go</code>. A starting example with the Pricing service (<code>internal/service/pricing/acc_test.go</code>):</p> <pre><code>package aws\n\nimport (\n\"context\"\n\"sync\"\n\"testing\"\n\n\"github.com/aws/aws-sdk-go/aws/endpoints\"\n\"github.com/aws/aws-sdk-go/service/pricing\"\n\"github.com/hashicorp/terraform-plugin-sdk/v2/diag\"\n\"github.com/hashicorp/terraform-plugin-sdk/v2/helper/schema\"\n\"github.com/hashicorp/terraform-plugin-sdk/v2/terraform\"\n\"github.com/hashicorp/terraform-provider-aws/internal/acctest\"\n\"github.com/hashicorp/terraform-provider-aws/internal/provider\"\n)\n\n// testAccPricingRegion is the chosen Pricing testing region\n//\n// Cached to prevent issues should multiple regions become available.\nvar testAccPricingRegion string\n\n// testAccProviderPricing is the Pricing provider instance\n//\n// This Provider can be used in testing code for API calls without requiring\n// the use of saving and referencing specific ProviderFactories instances.\n//\n// testAccPreCheckPricing(t) must be called before using this provider instance.\nvar testAccProviderPricing *schema.Provider\n\n// testAccProviderPricingConfigure ensures the provider is only configured once\nvar testAccProviderPricingConfigure sync.Once\n\n// testAccPreCheckPricing verifies AWS credentials and that Pricing is supported\nfunc testAccPreCheckPricing(t *testing.T) {\nacctest.PreCheckPartitionHasService(t, pricing.EndpointsID)\n\n// Since we are outside the scope of the Terraform configuration we must\n// call Configure() to properly initialize the provider configuration.\ntestAccProviderPricingConfigure.Do(func() {\ntestAccProviderPricing = provider.Provider()\n\nconfig := map[string]interface{}{\n\"region\": testAccGetPricingRegion(),\n}\n\ndiags := testAccProviderPricing.Configure(context.Background(), terraform.NewResourceConfigRaw(config))\n\nif diags != nil &amp;&amp; diags.HasError() {\nfor _, d := range diags {\nif d.Severity == diag.Error {\nt.Fatalf(\"configuring Pricing provider: %s\", d.Summary)\n}\n}\n}\n})\n}\n\n// testAccPricingRegionProviderConfig is the Terraform provider configuration for Pricing region testing\n//\n// Testing Pricing assumes no other provider configurations\n// are necessary and overwrites the \"aws\" provider configuration.\nfunc testAccPricingRegionProviderConfig() string {\nreturn acctest.ConfigRegionalProvider(testAccGetPricingRegion())\n}\n\n// testAccGetPricingRegion returns the Pricing region for testing\nfunc testAccGetPricingRegion() string {\nif testAccPricingRegion != \"\" {\nreturn testAccPricingRegion\n}\n\nif rs, ok := endpoints.RegionsForService(endpoints.DefaultPartitions(), testAccGetPartition(), pricing.ServiceName); ok {\n// return available region (random if multiple)\nfor regionID := range rs {\ntestAccPricingRegion = regionID\nreturn testAccPricingRegion\n}\n}\n\ntestAccPricingRegion = testAccGetRegion()\n\nreturn testAccPricingRegion\n}\n</code></pre> <p>For the resource or data source acceptance tests, the key items to adjust are:</p> <ul> <li>Ensure <code>TestCase</code> uses <code>ProtoV5ProviderFactories: acctest.ProtoV5ProviderFactories</code> instead of <code>ProviderFactories: acctest.ProviderFactories</code> or <code>Providers: acctest.Providers</code></li> <li>Add the call for the new <code>PreCheck</code> function (keeping <code>acctest.PreCheck(t)</code>), e.g. <code>PreCheck: func() { acctest.PreCheck(t); testAccPreCheckPricing(t) },</code></li> <li>If the testing is for a managed resource with a <code>CheckDestroy</code> function, ensure it uses the new provider instance, e.g. <code>testAccProviderPricing</code>, instead of <code>acctest.Provider</code>.</li> <li>If the testing is for a managed resource with a <code>Check...Exists</code> function, ensure it uses the new provider instance, e.g. <code>testAccProviderPricing</code>, instead of <code>acctest.Provider</code>.</li> <li>In each <code>TestStep</code> configuration, ensure the new provider configuration function is called, e.g.</li> </ul> <pre><code>func testAccDataSourcePricingProductConfigRedshift() string {\nreturn acctest.ConfigCompose(\ntestAccPricingRegionProviderConfig(),\n`\n# ... test configuration ...\n`)\n}\n</code></pre> <p>If the testing configurations require more than one region, reach out to the maintainers for further assistance.</p>"},{"location":"running-and-writing-acceptance-tests/#acceptance-test-concurrency","title":"Acceptance Test Concurrency","text":"<p>Certain AWS service APIs allow a limited number of a certain component, while the acceptance testing runs at a default concurrency of twenty tests at a time. For example as of this writing, the SageMaker service only allows one SageMaker Domain per AWS Region. Running the tests with the default concurrency will fail with API errors relating to the component quota being exceeded.</p> <p>When encountering these types of components, the acceptance testing can be setup to limit the available concurrency of that particular component. When limited to one component at a time, this may also be referred to as serializing the acceptance tests.</p> <p>To convert to serialized (one test at a time) acceptance testing:</p> <ul> <li>Convert all existing capital <code>T</code> test functions with the limited component to begin with a lowercase <code>t</code>, e.g., <code>TestAccSageMakerDomain_basic</code> becomes <code>testDomain_basic</code>. This will prevent the test framework from executing these tests directly as the prefix <code>Test</code> is required.<ul> <li>In each of these test functions, convert <code>resource.ParallelTest</code> to <code>resource.Test</code></li> </ul> </li> <li>Create a capital <code>T</code> <code>TestAcc{Service}{Thing}_serial</code> test function that then references all the lowercase <code>t</code> test functions. If multiple test files are referenced, this new test be created in a new shared file such as <code>internal/service/{SERVICE}/{SERVICE}_test.go</code>. The contents of this test can be setup like the following:</li> </ul> <pre><code>func TestAccExampleThing_serial(t *testing.T) {\nt.Parallel()\n\ntestCases := map[string]map[string]func(t *testing.T){\n\"Thing\": {\n\"basic\":        testAccThing_basic,\n\"disappears\":   testAccThing_disappears,\n// ... potentially other resource tests ...\n},\n// ... potentially other top level resource test groups ...\n}\n\nacctest.RunSerialTests2Levels(t, testCases, 0)\n}\n</code></pre> <p>NOTE: Future iterations of these acceptance testing concurrency instructions will include the ability to handle more than one component at a time including service quota lookup, if supported by the service API.</p>"},{"location":"running-and-writing-acceptance-tests/#data-source-acceptance-testing","title":"Data Source Acceptance Testing","text":"<p>Writing acceptance testing for data sources is similar to resources, with the biggest changes being:</p> <ul> <li>Adding <code>DataSource</code> to the test and configuration naming, such as <code>TestAccExampleThingDataSource_Filter</code></li> <li>The basic test may be named after the easiest lookup attribute instead, e.g., <code>TestAccExampleThingDataSource_Name</code></li> <li>No disappears testing</li> <li>Almost all checks should be done with <code>resource.TestCheckResourceAttrPair()</code> to compare the data source attributes to the resource attributes</li> <li>The usage of an additional <code>dataSourceName</code> variable to store a data source reference, e.g., <code>data.aws_example_thing.test</code></li> </ul> <p>Data sources testing should still use the <code>CheckDestroy</code> function of the resource, just to continue verifying that there are no dangling AWS resources after a test is run.</p> <p>Please note that we do not recommend re-using test configurations between resources and their associated data source as it is harder to discover testing regressions. Authors are encouraged to potentially implement similar \"base\" configurations though.</p> <p>For example:</p> <pre><code>func TestAccExampleThingDataSource_Name(t *testing.T) {\nctx := acctest.Context(t)\nrName := sdkacctest.RandomWithPrefix(acctest.ResourcePrefix)\ndataSourceName := \"data.aws_example_thing.test\"\nresourceName := \"aws_example_thing.test\"\n\nresource.ParallelTest(t, resource.TestCase{\nPreCheck:                 func() { acctest.PreCheck(t) },\nErrorCheck:               acctest.ErrorCheck(t, service.EndpointsID),\nProtoV5ProviderFactories: acctest.ProtoV5ProviderFactories,\nCheckDestroy:             testAccCheckExampleThingDestroy(ctx),\nSteps: []resource.TestStep{\n{\nConfig: testAccExampleThingDataSourceConfigName(rName),\nCheck: resource.ComposeTestCheckFunc(\ntestAccCheckExampleThingExists(ctx, resourceName),\nresource.TestCheckResourceAttrPair(resourceName, \"arn\", dataSourceName, \"arn\"),\nresource.TestCheckResourceAttrPair(resourceName, \"description\", dataSourceName, \"description\"),\nresource.TestCheckResourceAttrPair(resourceName, \"name\", dataSourceName, \"name\"),\n),\n},\n},\n})\n}\n\n// below all TestAcc functions\n\nfunc testAccExampleThingDataSourceConfigName(rName string) string {\nreturn fmt.Sprintf(`\nresource \"aws_example_thing\" \"test\" {\n  name = %[1]q\n}\n\ndata \"aws_example_thing\" \"test\" {\n  name = aws_example_thing.test.name\n}\n`, rName)\n}\n</code></pre>"},{"location":"running-and-writing-acceptance-tests/#acceptance-test-sweepers","title":"Acceptance Test Sweepers","text":"<p>When running the acceptance tests, especially when developing or troubleshooting Terraform resources, its possible for code bugs or other issues to prevent the proper destruction of AWS infrastructure. To prevent lingering resources from consuming quota or causing unexpected billing, the Terraform Plugin SDK supports the test sweeper framework to clear out an AWS region of all resources. This section is meant to augment the SDKv2 documentation on test sweepers with Terraform AWS Provider specific details.</p>"},{"location":"running-and-writing-acceptance-tests/#running-test-sweepers","title":"Running Test Sweepers","text":"<p>WARNING: Test Sweepers will destroy AWS infrastructure and backups in the target AWS account and region! These are designed to override any API deletion protection. Never run these outside a development AWS account that should be completely empty of resources.</p> <p>To run the sweepers for all resources in <code>us-west-2</code> and <code>us-east-1</code> (default testing regions):</p> <pre><code>$ make sweep\n</code></pre> <p>To run a specific resource sweeper:</p> <pre><code>$ SWEEPARGS=-sweep-run=aws_example_thing make sweep\n</code></pre> <p>To run sweepers with an assumed role, use the following additional environment variables:</p> <ul> <li><code>TF_AWS_ASSUME_ROLE_ARN</code> - Required.</li> <li><code>TF_AWS_ASSUME_ROLE_DURATION</code> - Optional, defaults to 1 hour (3600).</li> <li><code>TF_AWS_ASSUME_ROLE_EXTERNAL_ID</code> - Optional.</li> <li><code>TF_AWS_ASSUME_ROLE_SESSION_NAME</code> - Optional.</li> </ul>"},{"location":"running-and-writing-acceptance-tests/#sweeper-checklists","title":"Sweeper Checklists","text":"<ul> <li>Add Resource Sweeper Implementation: See Writing Test Sweepers.</li> <li>Add Service To Sweeper List: Once a <code>sweep.go</code> file is present in the service subdirectory, run <code>make gen</code> to regenerate the list of imports in <code>internal/sweep/sweep_test.go</code>.</li> </ul>"},{"location":"running-and-writing-acceptance-tests/#writing-test-sweepers","title":"Writing Test Sweepers","text":"<p>Sweeper logic should be written to a file called <code>sweep.go</code> in the appropriate service subdirectory (<code>internal/service/{serviceName}</code>). This file should include the following build tags above the package declaration:</p> <pre><code>//go:build sweep\n// +build sweep\n\npackage example\n</code></pre> <p>Next, initialize the resource into the test sweeper framework:</p> <pre><code>func init() {\nresource.AddTestSweepers(\"aws_example_thing\", &amp;resource.Sweeper{\nName: \"aws_example_thing\",\nF:    sweepThings,\n// Optionally\nDependencies: []string{\n\"aws_other_thing\",\n},\n})\n}\n</code></pre> <p>Then add the actual implementation. Preferably, if a paginated SDK call is available:</p> <pre><code>func sweepThings(region string) error {\nctx := sweep.Context(region)\nclient, err := sweep.SharedRegionalSweepClient(region)\n\nif err != nil {\nreturn fmt.Errorf(\"getting client: %w\", err)\n}\n\nconn := client.(*conns.AWSClient).ExampleConn()\nsweepResources := make([]sweep.Sweepable, 0)\nvar errs *multierror.Error\n\ninput := &amp;example.ListThingsInput{}\n\nerr = conn.ListThingsPages(input, func(page *example.ListThingsOutput, lastPage bool) bool {\nif page == nil {\nreturn !lastPage\n}\n\nfor _, thing := range page.Things {\nr := ResourceThing()\nd := r.Data(nil)\n\nid := aws.StringValue(thing.Id)\nd.SetId(id)\n\n// Perform resource specific pre-sweep setup.\n// For example, you may need to perform one or more of these types of pre-sweep tasks, specific to the resource:\n//\n// err := sweep.ReadResource(ctx, r, d, client) // fill in data\n// d.Set(\"skip_final_snapshot\", true)           // set an argument in order to delete\n\n// This \"if\" is only needed if the pre-sweep setup can produce errors.\n// Otherwise, do not include it.\nif err != nil {\nerr := fmt.Errorf(\"reading Example Thing (%s): %w\", id, err)\nlog.Printf(\"[ERROR] %s\", err)\nerrs = multierror.Append(errs, err)\ncontinue\n}\n\nsweepResources = append(sweepResources, sweep.NewSweepResource(r, d, client))\n}\n\nreturn !lastPage\n})\n\nif err != nil {\nerrs = multierror.Append(errs, fmt.Errorf(\"listing Example Thing for %s: %w\", region, err))\n}\n\nif err := sweep.SweepOrchestrator(sweepResources); err != nil {\nerrs = multierror.Append(errs, fmt.Errorf(\"sweeping Example Thing for %s: %w\", region, err))\n}\n\nif sweep.SkipSweepError(err) {\nlog.Printf(\"[WARN] Skipping Example Thing sweep for %s: %s\", region, errs)\nreturn nil\n}\n\nreturn errs.ErrorOrNil()\n}\n</code></pre> <p>Otherwise, if no paginated SDK call is available:</p> <pre><code>func sweepThings(region string) error {\nctx := sweep.Context(region)\nclient, err := sweep.SharedRegionalSweepClient(region)\n\nif err != nil {\nreturn fmt.Errorf(\"getting client: %w\", err)\n}\n\nconn := client.(*conns.AWSClient).ExampleConn()\nsweepResources := make([]sweep.Sweepable, 0)\nvar errs *multierror.Error\n\ninput := &amp;example.ListThingsInput{}\n\nfor {\noutput, err := conn.ListThings(input)\n\nfor _, thing := range output.Things {\nr := ResourceThing()\nd := r.Data(nil)\n\nid := aws.StringValue(thing.Id)\nd.SetId(id)\n\n// Perform resource specific pre-sweep setup.\n// For example, you may need to perform one or more of these types of pre-sweep tasks, specific to the resource:\n//\n// err := sweep.ReadResource(ctx, r, d, client) // fill in data\n// d.Set(\"skip_final_snapshot\", true)           // set an argument in order to delete\n\n// This \"if\" is only needed if the pre-sweep setup can produce errors.\n// Otherwise, do not include it.\nif err != nil {\nerr := fmt.Errorf(\"reading Example Thing (%s): %w\", id, err)\nlog.Printf(\"[ERROR] %s\", err)\nerrs = multierror.Append(errs, err)\ncontinue\n}\n\nsweepResources = append(sweepResources, sweep.NewSweepResource(r, d, client))\n}\n\nif aws.StringValue(output.NextToken) == \"\" {\nbreak\n}\n\ninput.NextToken = output.NextToken\n}\n\nif err := sweep.SweepOrchestrator(sweepResources); err != nil {\nerrs = multierror.Append(errs, fmt.Errorf(\"sweeping Example Thing for %s: %w\", region, err))\n}\n\nif sweep.SkipSweepError(err) {\nlog.Printf(\"[WARN] Skipping Example Thing sweep for %s: %s\", region, errs)\nreturn nil\n}\n\nreturn errs.ErrorOrNil()\n}\n</code></pre>"},{"location":"running-and-writing-acceptance-tests/#acceptance-test-checklists","title":"Acceptance Test Checklists","text":"<p>There are several aspects to writing good acceptance tests. These checklists will help ensure effective testing from the design stage through to implementation details.</p>"},{"location":"running-and-writing-acceptance-tests/#basic-acceptance-test-design","title":"Basic Acceptance Test Design","text":"<p>These are basic principles to help guide the creation of acceptance tests.</p> <ul> <li>Covers Changes: Every line of resource or data source code added or changed should be covered by one or more tests. For example, if a resource has two ways of functioning, tests should cover both possible paths. Nearly every codebase change needs test coverage to ensure functionality and prevent future regressions. If a bug or other problem prompted a fix, a test should be added that previously would have failed, especially if the report included a configuration.</li> <li>Follows the Single Responsibility Principle: Every test should have a single responsibility and effectively test that responsibility. This may include individual tests for verifying basic functionality of the resource (Create, Read, Delete), separately verifying using and updating a single attribute in a resource, or separately changing between two attributes to verify two \"modes\"/\"types\" possible with a resource configuration. In following this principle, test configurations should be as simple as possible. For example, not including extra configuration unless it is necessary for the specific test.</li> </ul>"},{"location":"running-and-writing-acceptance-tests/#test-implementation","title":"Test Implementation","text":"<p>The below are required items that will be noted during submission review and prevent immediate merging:</p> <ul> <li>Implements CheckDestroy: Resource testing should include a <code>CheckDestroy</code> function (typically named <code>testAccCheck{SERVICE}{RESOURCE}Destroy</code>) that calls the API to verify that the Terraform resource has been deleted or disassociated as appropriate. More information about <code>CheckDestroy</code> functions can be found in the SDKv2 TestCase documentation.</li> <li>Implements Exists Check Function: Resource testing should include a <code>TestCheckFunc</code> function (typically named <code>testAccCheck{SERVICE}{RESOURCE}Exists</code>) that calls the API to verify that the Terraform resource has been created or associated as appropriate. Preferably, this function will also accept a pointer to an API object representing the Terraform resource from the API response that can be set for potential usage in later <code>TestCheckFunc</code>. More information about these functions can be found in the SDKv2 Custom Check Functions documentation.</li> <li>Excludes Provider Declarations: Test configurations should not include <code>provider \"aws\" {...}</code> declarations. If necessary, only the provider declarations in <code>acctest.go</code> should be used for multiple account/region or otherwise specialized testing.</li> <li>Passes in us-west-2 Region: Tests default to running in <code>us-west-2</code> and at a minimum should pass in that region or include necessary <code>PreCheck</code> functions to skip the test when ran outside an expected environment.</li> <li>Includes ErrorCheck: All acceptance tests should include a call to the common ErrorCheck (<code>ErrorCheck:   acctest.ErrorCheck(t, service.EndpointsID),</code>).</li> <li>Uses resource.ParallelTest: Tests should use <code>resource.ParallelTest()</code> instead of <code>resource.Test()</code> except where serialized testing is absolutely required.</li> <li>[ ] Uses fmt.Sprintf(): Test configurations preferably should to be separated into their own functions (typically named <code>testAcc{SERVICE}{RESOURCE}Config{PURPOSE}</code>) that call <code>fmt.Sprintf()</code> for variable injection or a string <code>const</code> for completely static configurations. Test configurations should avoid <code>var</code> or other variable injection functionality such as <code>text/template</code>.</li> <li>Uses Randomized Infrastructure Naming: Test configurations that use resources where a unique name is required should generate a random name. Typically this is created via <code>rName := sdkacctest.RandomWithPrefix(acctest.ResourcePrefix)</code> in the acceptance test function before generating the configuration.</li> <li>Prevents S3 Bucket Deletion Errors: Test configurations that use <code>aws_s3_bucket</code> resources as a logging destination should include the <code>force_destroy = true</code> configuration. This is to prevent race conditions where logging objects may be written during the testing duration which will cause <code>BucketNotEmpty</code> errors during deletion.</li> </ul> <p>For resources that support import, the additional item below is required that will be noted during submission review and prevent immediate merging:</p> <ul> <li>Implements ImportState Testing: Tests should include an additional <code>TestStep</code> configuration that verifies resource import via <code>ImportState: true</code> and <code>ImportStateVerify: true</code>. This <code>TestStep</code> should be added to all possible tests for the resource to ensure that all infrastructure configurations are properly imported into Terraform.</li> </ul> <p>The below are style-based items that may be noted during review and are recommended for simplicity, consistency, and quality assurance:</p> <ul> <li>Uses Builtin Check Functions: Tests should use already available check functions, e.g. <code>resource.TestCheckResourceAttr()</code>, to verify values in the Terraform state over creating custom <code>TestCheckFunc</code>. More information about these functions can be found in the SDKv2 Builtin Check Functions documentation.</li> <li>Uses TestCheckResourceAttrPair() for Data Sources: Tests should use <code>resource.TestCheckResourceAttrPair()</code> to verify values in the Terraform state for data sources attributes to compare them with their expected resource attributes.</li> <li>Excludes Timeouts Configurations: Test configurations should not include <code>timeouts {...}</code> configuration blocks except for explicit testing of customizable timeouts (typically very short timeouts with <code>ExpectError</code>).</li> <li>Implements Default and Zero Value Validation: The basic test for a resource (typically named <code>TestAcc{SERVICE}{RESOURCE}_basic</code>) should use available check functions, e.g. <code>resource.TestCheckResourceAttr()</code>, to verify default and zero values in the Terraform state for all attributes. Empty/missing configuration blocks can be verified with <code>resource.TestCheckResourceAttr(resourceName, \"{ATTRIBUTE}.#\", \"0\")</code> and empty maps with <code>resource.TestCheckResourceAttr(resourceName, \"{ATTRIBUTE}.%\", \"0\")</code></li> </ul>"},{"location":"running-and-writing-acceptance-tests/#avoid-hard-coding","title":"Avoid Hard Coding","text":"<p>Avoid hard coding values in acceptance test checks and configurations for consistency and testing flexibility. Resource testing is expected to pass across multiple AWS environments supported by the Terraform AWS Provider (e.g., AWS Standard and AWS GovCloud (US)). Contributors are not expected or required to perform testing outside of AWS Standard, e.g., running only in the <code>us-west-2</code> region is perfectly acceptable. However, contributors are expected to avoid hard coding with these guidelines.</p>"},{"location":"running-and-writing-acceptance-tests/#hardcoded-account-ids","title":"Hardcoded Account IDs","text":"<ul> <li>Uses Account Data Sources: Any hardcoded account numbers in configuration, e.g., <code>137112412989</code>, should be replaced with a data source. Depending on the situation, there are several data sources for account IDs including:<ul> <li><code>aws_caller_identity</code> data source,</li> <li><code>aws_canonical_user_id</code> data source,</li> <li><code>aws_billing_service_account</code> data source, and</li> <li><code>aws_sagemaker_prebuilt_ecr_image</code> data source.</li> </ul> </li> <li>Uses Account Test Checks: Any check required to verify an AWS Account ID of the current testing account or another account should use one of the following available helper functions over the usage of <code>resource.TestCheckResourceAttrSet()</code> and <code>resource.TestMatchResourceAttr()</code>:<ul> <li><code>acctest.CheckResourceAttrAccountID()</code>: Validates the state value equals the AWS Account ID of the current account running the test. This is the most common implementation.</li> <li><code>acctest.MatchResourceAttrAccountID()</code>: Validates the state value matches any AWS Account ID (e.g. a 12 digit number). This is typically only used in data source testing of AWS managed components.</li> </ul> </li> </ul> <p>Here's an example of using <code>aws_caller_identity</code>:</p> <pre><code>data \"aws_caller_identity\" \"current\" {}\n\nresource \"aws_backup_selection\" \"test\" {\nplan_id      = aws_backup_plan.test.id\nname         = \"tf_acc_test_backup_selection_%[1]d\"\niam_role_arn = \"arn:${data.aws_partition.current.partition}:iam::${data.aws_caller_identity.current.account_id}:role/service-role/AWSBackupDefaultServiceRole\"\n}\n</code></pre>"},{"location":"running-and-writing-acceptance-tests/#hardcoded-ami-ids","title":"Hardcoded AMI IDs","text":"<ul> <li>Uses aws_ami Data Source: Any hardcoded AMI ID configuration, e.g. <code>ami-12345678</code>, should be replaced with the <code>aws_ami</code> data source pointing to an Amazon Linux image. The package <code>internal/acctest</code> includes test configuration helper functions to simplify these lookups:<ul> <li><code>acctest.ConfigLatestAmazonLinuxHVMEBSAMI()</code>: The recommended AMI for most situations, using Amazon Linux, HVM virtualization, and EBS storage. To reference the AMI ID in the test configuration: <code>data.aws_ami.amzn-ami-minimal-hvm-ebs.id</code>.</li> <li><code>testAccLatestAmazonLinuxHVMInstanceStoreAMIConfig()</code> (EC2): AMI lookup using Amazon Linux, HVM virtualization, and Instance Store storage. Should only be used in testing that requires Instance Store storage rather than EBS. To reference the AMI ID in the test configuration: <code>data.aws_ami.amzn-ami-minimal-hvm-instance-store.id</code>.</li> <li><code>testAccLatestAmazonLinuxPVEBSAMIConfig()</code> (EC2): AMI lookup using Amazon Linux, Paravirtual virtualization, and EBS storage. Should only be used in testing that requires Paravirtual over Hardware Virtual Machine (HVM) virtualization. To reference the AMI ID in the test configuration: <code>data.aws_ami.amzn-ami-minimal-pv-ebs.id</code>.</li> <li><code>configLatestAmazonLinuxPvInstanceStoreAmi</code> (EC2): AMI lookup using Amazon Linux, Paravirtual virtualization, and Instance Store storage. Should only be used in testing that requires Paravirtual virtualization over HVM and Instance Store storage over EBS. To reference the AMI ID in the test configuration: <code>data.aws_ami.amzn-ami-minimal-pv-instance-store.id</code>.</li> <li><code>testAccLatestWindowsServer2016CoreAMIConfig()</code> (EC2): AMI lookup using Windows Server 2016 Core, HVM virtualization, and EBS storage. Should only be used in testing that requires Windows. To reference the AMI ID in the test configuration: <code>data.aws_ami.win2016core-ami.id</code>.</li> </ul> </li> </ul> <p>Here's an example of using <code>acctest.ConfigLatestAmazonLinuxHVMEBSAMI()</code> and <code>data.aws_ami.amzn-ami-minimal-hvm-ebs.id</code>:</p> <pre><code>func testAccLaunchConfigurationDataSourceConfig_basic(rName string) string {\nreturn acctest.ConfigCompose(\nacctest.ConfigLatestAmazonLinuxHVMEBSAMI(),\nfmt.Sprintf(`\nresource \"aws_launch_configuration\" \"test\" {\n  name          = %[1]q\n  image_id      = data.aws_ami.amzn-ami-minimal-hvm-ebs.id\n  instance_type = \"m1.small\"\n}\n`, rName))\n}\n</code></pre>"},{"location":"running-and-writing-acceptance-tests/#hardcoded-availability-zones","title":"Hardcoded Availability Zones","text":"<ul> <li>Uses aws_availability_zones Data Source: Any hardcoded AWS Availability Zone configuration, e.g. <code>us-west-2a</code>, should be replaced with the <code>aws_availability_zones</code> data source. Use the convenience function called <code>acctest.ConfigAvailableAZsNoOptIn()</code> (defined in <code>internal/acctest/acctest.go</code>) to declare <code>data \"aws_availability_zones\" \"available\" {...}</code>. You can then reference the data source via <code>data.aws_availability_zones.available.names[0]</code> or <code>data.aws_availability_zones.available.names[count.index]</code> in resources using <code>count</code>.</li> </ul> <p>Here's an example of using <code>acctest.ConfigAvailableAZsNoOptIn()</code> and <code>data.aws_availability_zones.available.names[0]</code>:</p> <pre><code>func testAccInstanceVpcConfigBasic(rName string) string {\nreturn acctest.ConfigCompose(\nacctest.ConfigAvailableAZsNoOptIn(),\nfmt.Sprintf(`\nresource \"aws_subnet\" \"test\" {\n  availability_zone = data.aws_availability_zones.available.names[0]\n  cidr_block        = \"10.0.0.0/24\"\n  vpc_id            = aws_vpc.test.id\n}\n`, rName))\n}\n</code></pre>"},{"location":"running-and-writing-acceptance-tests/#hardcoded-database-versions","title":"Hardcoded Database Versions","text":"<ul> <li>Uses Database Version Data Sources: Hardcoded database versions, e.g., RDS MySQL Engine Version <code>5.7.42</code>, should be removed (which means the AWS-defined default version will be used) or replaced with a list of preferred versions using a data source. Because versions change over times and version offerings vary from region to region and partition to partition, using the default version or providing a list of preferences ensures a version will be available. Depending on the situation, there are several data sources for versions, including:<ul> <li><code>aws_rds_engine_version</code> data source,</li> <li><code>aws_docdb_engine_version</code> data source, and</li> <li><code>aws_neptune_engine_version</code> data source.</li> </ul> </li> </ul> <p>Here's an example of using <code>aws_rds_engine_version</code> and <code>data.aws_rds_engine_version.default.version</code>:</p> <pre><code>data \"aws_rds_engine_version\" \"default\" {\nengine = \"mysql\"\n}\n\ndata \"aws_rds_orderable_db_instance\" \"test\" {\nengine                     = data.aws_rds_engine_version.default.engine\nengine_version             = data.aws_rds_engine_version.default.version\npreferred_instance_classes = [\"db.t3.small\", \"db.t2.small\", \"db.t2.medium\"]\n}\n\nresource \"aws_db_instance\" \"bar\" {\nengine               = data.aws_rds_engine_version.default.engine\nengine_version       = data.aws_rds_engine_version.default.version\ninstance_class       = data.aws_rds_orderable_db_instance.test.instance_class\nskip_final_snapshot  = true\nparameter_group_name = \"default.${data.aws_rds_engine_version.default.parameter_group_family}\"\n}\n</code></pre>"},{"location":"running-and-writing-acceptance-tests/#hardcoded-direct-connect-locations","title":"Hardcoded Direct Connect Locations","text":"<ul> <li>Uses aws_dx_locations Data Source: Hardcoded AWS Direct Connect locations, e.g., <code>EqSe2</code>, should be replaced with the <code>aws_dx_locations</code> data source.</li> </ul> <p>Here's an example using <code>data.aws_dx_locations.test.location_codes</code>:</p> <pre><code>data \"aws_dx_locations\" \"test\" {}\n\nresource \"aws_dx_lag\" \"test\" {\nname                  = \"Test LAG\"\nconnections_bandwidth = \"1Gbps\"\nlocation              = tolist(data.aws_dx_locations.test.location_codes)[0]\nforce_destroy         = true\n}\n</code></pre>"},{"location":"running-and-writing-acceptance-tests/#hardcoded-instance-types","title":"Hardcoded Instance Types","text":"<ul> <li>Uses Instance Type Data Source: Singular hardcoded instance types and classes, e.g., <code>t2.micro</code> and <code>db.t2.micro</code>, should be replaced with a list of preferences using a data source. Because offerings vary from region to region and partition to partition, providing a list of preferences dramatically improves the likelihood that one of the options will be available. Depending on the situation, there are several data sources for instance types and classes, including:<ul> <li><code>aws_ec2_instance_type_offering</code> data source - Convenience functions declare configurations that are referenced with <code>data.aws_ec2_instance_type_offering.available</code> including:<ul> <li>The <code>acctest.AvailableEC2InstanceTypeForAvailabilityZone()</code> function for test configurations using an EC2 Subnet which is inherently within a single Availability Zone</li> <li>The <code>acctest.AvailableEC2InstanceTypeForRegion()</code> function for test configurations that do not include specific Availability Zones</li> </ul> </li> <li><code>aws_rds_orderable_db_instance</code> data source,</li> <li><code>aws_neptune_orderable_db_instance</code> data source, and</li> <li><code>aws_docdb_orderable_db_instance</code> data source.</li> </ul> </li> </ul> <p>Here's an example of using <code>acctest.AvailableEC2InstanceTypeForRegion()</code> and <code>data.aws_ec2_instance_type_offering.available.instance_type</code>:</p> <pre><code>func testAccSpotInstanceRequestConfig(rInt int) string {\nreturn acctest.ConfigCompose(\nacctest.AvailableEC2InstanceTypeForRegion(\"t3.micro\", \"t2.micro\"),\nfmt.Sprintf(`\nresource \"aws_spot_instance_request\" \"test\" {\n  instance_type        = data.aws_ec2_instance_type_offering.available.instance_type\n  spot_price           = \"0.05\"\n  wait_for_fulfillment = true\n}\n`, rInt))\n}\n</code></pre> <p>Here's an example of using <code>aws_rds_orderable_db_instance</code> and <code>data.aws_rds_orderable_db_instance.test.instance_class</code>:</p> <pre><code>data \"aws_rds_orderable_db_instance\" \"test\" {\nengine                     = \"mysql\"\nengine_version             = \"5.7.31\"\npreferred_instance_classes = [\"db.t3.micro\", \"db.t2.micro\", \"db.t3.small\"]\n}\n\nresource \"aws_db_instance\" \"test\" {\nengine              = data.aws_rds_orderable_db_instance.test.engine\nengine_version      = data.aws_rds_orderable_db_instance.test.engine_version\ninstance_class      = data.aws_rds_orderable_db_instance.test.instance_class\nskip_final_snapshot = true\nusername            = \"test\"\n}\n</code></pre>"},{"location":"running-and-writing-acceptance-tests/#hardcoded-partition-dns-suffix","title":"Hardcoded Partition DNS Suffix","text":"<ul> <li>Uses aws_partition Data Source: Any hardcoded DNS suffix configuration, e.g., the <code>amazonaws.com</code> in a <code>ec2.amazonaws.com</code> service principal, should be replaced with the <code>aws_partition</code> data source. A common pattern is declaring <code>data \"aws_partition\" \"current\" {}</code> and referencing it via <code>data.aws_partition.current.dns_suffix</code>.</li> </ul> <p>Here's an example of using <code>aws_partition</code> and <code>data.aws_partition.current.dns_suffix</code>:</p> <pre><code>data \"aws_partition\" \"current\" {}\n\nresource \"aws_iam_role\" \"test\" {\nassume_role_policy = &lt;&lt;POLICY\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"\",\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"Service\": \"cloudtrail.${data.aws_partition.current.dns_suffix}\"\n      },\n      \"Action\": \"sts:AssumeRole\"\n    }\n  ]\n}\nPOLICY\n}\n</code></pre>"},{"location":"running-and-writing-acceptance-tests/#hardcoded-partition-in-arn","title":"Hardcoded Partition in ARN","text":"<ul> <li>Uses aws_partition Data Source: Any hardcoded AWS Partition configuration, e.g. the <code>aws</code> in a <code>arn:aws:SERVICE:REGION:ACCOUNT:RESOURCE</code> ARN, should be replaced with the <code>aws_partition</code> data source. A common pattern is declaring <code>data \"aws_partition\" \"current\" {}</code> and referencing it via <code>data.aws_partition.current.partition</code>.</li> <li> <p>Uses Builtin ARN Check Functions: Tests should use available ARN check functions to validate ARN attribute values in the Terraform state over <code>resource.TestCheckResourceAttrSet()</code> and <code>resource.TestMatchResourceAttr()</code>:</p> <ul> <li><code>acctest.CheckResourceAttrRegionalARN()</code> verifies that an ARN matches the account ID and region of the test execution with an exact resource value</li> <li><code>acctest.MatchResourceAttrRegionalARN()</code> verifies that an ARN matches the account ID and region of the test execution with a regular expression of the resource value</li> <li><code>acctest.CheckResourceAttrGlobalARN()</code> verifies that an ARN matches the account ID of the test execution with an exact resource value</li> <li><code>acctest.MatchResourceAttrGlobalARN()</code> verifies that an ARN matches the account ID of the test execution with a regular expression of the resource value</li> <li><code>acctest.CheckResourceAttrRegionalARNNoAccount()</code> verifies than an ARN has no account ID and matches the current region of the test execution with an exact resource value</li> <li><code>acctest.CheckResourceAttrGlobalARNNoAccount()</code> verifies than an ARN has no account ID and matches an exact resource value</li> <li><code>acctest.CheckResourceAttrRegionalARNAccountID()</code> verifies than an ARN matches a specific account ID and the current region of the test execution with an exact resource value</li> <li><code>acctest.CheckResourceAttrGlobalARNAccountID()</code> verifies than an ARN matches a specific account ID with an exact resource value</li> </ul> </li> </ul> <p>Here's an example of using <code>aws_partition</code> and <code>data.aws_partition.current.partition</code>:</p> <pre><code>data \"aws_partition\" \"current\" {}\n\nresource \"aws_iam_role_policy_attachment\" \"test\" {\npolicy_arn = \"arn:${data.aws_partition.current.partition}:iam::aws:policy/service-role/AmazonRDSEnhancedMonitoringRole\"\nrole       = aws_iam_role.test.name\n}\n</code></pre>"},{"location":"running-and-writing-acceptance-tests/#hardcoded-region","title":"Hardcoded Region","text":"<ul> <li>Uses aws_region Data Source: Any hardcoded AWS Region configuration, e.g., <code>us-west-2</code>, should be replaced with the <code>aws_region</code> data source. A common pattern is declaring <code>data \"aws_region\" \"current\" {}</code> and referencing it via <code>data.aws_region.current.name</code></li> </ul> <p>Here's an example of using <code>aws_region</code> and <code>data.aws_region.current.name</code>:</p> <pre><code>data \"aws_region\" \"current\" {}\n\nresource \"aws_route53_zone\" \"test\" {\nvpc {\nvpc_id     = aws_vpc.test.id\nvpc_region = data.aws_region.current.name\n}\n}\n</code></pre>"},{"location":"running-and-writing-acceptance-tests/#hardcoded-spot-price","title":"Hardcoded Spot Price","text":"<ul> <li>Uses aws_ec2_spot_price Data Source: Any hardcoded spot prices, e.g., <code>0.05</code>, should be replaced with the <code>aws_ec2_spot_price</code> data source. A common pattern is declaring <code>data \"aws_ec2_spot_price\" \"current\" {}</code> and referencing it via <code>data.aws_ec2_spot_price.current.spot_price</code>.</li> </ul> <p>Here's an example of using <code>aws_ec2_spot_price</code> and <code>data.aws_ec2_spot_price.current.spot_price</code>:</p> <pre><code>data \"aws_ec2_spot_price\" \"current\" {\ninstance_type = \"t3.medium\"\n\nfilter {\nname   = \"product-description\"\nvalues = [\"Linux/UNIX\"]\n}\n}\n\nresource \"aws_spot_fleet_request\" \"test\" {\nspot_price      = data.aws_ec2_spot_price.current.spot_price\ntarget_capacity = 2\n}\n</code></pre>"},{"location":"running-and-writing-acceptance-tests/#hardcoded-ssh-keys","title":"Hardcoded SSH Keys","text":"<ul> <li>Uses acctest.RandSSHKeyPair() or RandSSHKeyPairSize() Functions: Any hardcoded SSH keys should be replaced with random SSH keys generated by either the acceptance testing framework's function <code>RandSSHKeyPair()</code> or the provider function <code>RandSSHKeyPairSize()</code>. <code>RandSSHKeyPair()</code> generates 1024-bit keys.</li> </ul> <p>Here's an example using <code>aws_key_pair</code></p> <pre><code>func TestAccKeyPair_basic(t *testing.T) {\n...\n\nrName := sdkacctest.RandomWithPrefix(acctest.ResourcePrefix)\npublicKey, _, err := acctest.RandSSHKeyPair(acctest.DefaultEmailAddress)\nif err != nil {\nt.Fatalf(\"generating random SSH key: %s\", err)\n}\n\nresource.ParallelTest(t, resource.TestCase{\n...\nSteps: []resource.TestStep{\n{\nConfig: testAccKeyPairConfig(rName, publicKey),\n...\n},\n},\n})\n}\n\nfunc testAccKeyPairConfig(rName, publicKey string) string {\nreturn fmt.Sprintf(`\nresource \"aws_key_pair\" \"test\" {\n  key_name   = %[1]q\n  public_key = %[2]q\n}\n`, rName, publicKey)\n}\n</code></pre>"},{"location":"running-and-writing-acceptance-tests/#hardcoded-email-addresses","title":"Hardcoded Email Addresses","text":"<ul> <li>Uses either acctest.DefaultEmailAddress Constant or acctest.RandomEmailAddress() Function: Any hardcoded email addresses should replaced with either the constant <code>acctest.DefaultEmailAddress</code> or the function <code>acctest.RandomEmailAddress()</code>.</li> </ul> <p>Using <code>acctest.DefaultEmailAddress</code> is preferred when using a single email address in an acceptance test.</p> <p>Here's an example using <code>acctest.DefaultEmailAddress</code></p> <pre><code>func TestAccSNSTopicSubscription_email(t *testing.T) {\n...\n\nrName := sdkacctest.RandomWithPrefix(acctest.ResourcePrefix)\n\nresource.ParallelTest(t, resource.TestCase{\n...\nSteps: []resource.TestStep{\n{\nConfig: testAccTopicSubscriptionEmailConfig(rName, acctest.DefaultEmailAddress),\nCheck: resource.ComposeTestCheckFunc(\n...\nresource.TestCheckResourceAttr(resourceName, \"endpoint\", acctest.DefaultEmailAddress),\n),\n},\n},\n})\n}\n</code></pre> <p>Here's an example using <code>acctest.RandomEmailAddress()</code></p> <pre><code>func TestAccPinpointEmailChannel_basic(t *testing.T) {\n...\n\ndomain := acctest.RandomDomainName()\naddress1 := acctest.RandomEmailAddress(domain)\naddress2 := acctest.RandomEmailAddress(domain)\n\nresource.ParallelTest(t, resource.TestCase{\n...\nSteps: []resource.TestStep{\n{\nConfig: testAccEmailChannelConfig_FromAddress(domain, address1),\nCheck: resource.ComposeTestCheckFunc(\n...\nresource.TestCheckResourceAttr(resourceName, \"from_address\", address1),\n),\n},\n{\nConfig: testAccEmailChannelConfig_FromAddress(domain, address2),\nCheck: resource.ComposeTestCheckFunc(\n...\nresource.TestCheckResourceAttr(resourceName, \"from_address\", address2),\n),\n},\n},\n})\n}\n</code></pre>"},{"location":"service-package-pullrequest-guide/","title":"Service Package Refactor Pull Request Guide","text":"<p>Pull request #21306 has significantly refactored the AWS provider codebase. Specifically, the code for all AWS resources and data sources has been relocated from a single <code>aws</code> directory to a large number of separate directories in <code>internal/service</code>, each corresponding to a particular AWS service. In addition to vastly simplifying the codebase's overall structure, this change has also allowed us to simplify the names of a number of underlying functions -- without encountering namespace collisions. Issue #20000 contains a more complete description of these changes.</p> <p>As a result, nearly every pull request opened prior to the refactoring has merge conflicts; they are attempting to apply changes to files that have since been relocated. Furthermore, any new files or functions introduced must be brought into line with the codebase's new conventions. The following steps are intended to resolve such a conflict -- though it should be noted that this guide is an active work in progress as additional pull requests are amended.</p> <p>These fixes, however, in no way affect the prioritization of a particular pull request. Once a pull request has been selected for review, the necessary changes will be made by a maintainer -- either directly or in collaboration with the pull request author.</p>"},{"location":"service-package-pullrequest-guide/#fixing-a-pre-refactor-pull-request","title":"Fixing a Pre-Refactor Pull Request","text":"<ol> <li> <p><code>git checkout</code> the branch pertaining to the pull request you wish to amend</p> </li> <li> <p>Begin a merge of the latest version of <code>main</code> branch into your local branch:    <code>git pull origin main</code>. Merge conflicts are expected.</p> </li> <li> <p>For any new file, rename and move the file to its appropriate service    package directory:</p> </li> </ol> <p>Resource Files</p> <pre><code>  git mv aws/resource_aws_{service_name}_{resource_name}.go \\\n  internal/service/{service_name}/{resource_name}.go\n</code></pre> <p>Resource Test Files</p> <pre><code>  git mv aws/resource_aws_{service_name}_{resource_name}_test.go \\\n  internal/service/{service_name}/{resource_name}_test.go\n</code></pre> <p>Data Source Files</p> <pre><code>  git mv aws/data_source_aws_{service_name}_{resource_name}.go \\\n  internal/service/{service_name}/{resource_name}_data_source.go\n</code></pre> <p>Data Source Test Files</p> <pre><code>  git mv aws/data_source_aws_{service_name}_{resource_name}_test.go \\\n  internal/service/{service_name}/{resource_name}_data_source_test.go\n</code></pre> <ol> <li>For any new function, rename the function appropriately:</li> </ol> <p>Resource Schema Functions</p> <pre><code>  func resourceAws{ResourceName}() =&gt;\n  func Resource{ResourceName}()\n</code></pre> <p>Resource Generic Functions</p> <pre><code>  func resourceAws{ServiceName}{ResourceName}{FunctionName}() =&gt;\n  func resource{ResourceName}{FunctionName}()\n</code></pre> <p>Resource Acceptance Test Functions</p> <pre><code>  func TestAccAWS{ServiceName}{ResourceName}_{testType}() =&gt;\n  func TestAcc{ResourceName}_{testType}()\n</code></pre> <p>Data Source Schema Functions</p> <pre><code>  func dataSourceAws{ResourceName}() =&gt;\n  func DataSource{ResourceName}()\n</code></pre> <p>Data Source Generic Functions</p> <pre><code>  func dataSourceAws{ServiceName}{ResourceName}{FunctionName}() =&gt;\n  func dataSource{ResourceName}{FunctionName}()\n</code></pre> <p>Data Source Acceptance Test Functions</p> <pre><code>  func TestAccDataSourceAWS{ServiceName}{ResourceName}_{testType}() =&gt;\n  func TestAcc{ResourceName}DataSource_{testType}()\n</code></pre> <p>Finder Functions</p> <pre><code>  func finder.{FunctionName}() =&gt;\n  func Find{FunctionName}()\n</code></pre> <p>Status Functions</p> <pre><code>  func waiter.{FunctionName}Status() =&gt;\n  func status{FunctionName}()\n</code></pre> <p>Waiter Functions</p> <pre><code>  func waiter.{FunctionName}() =&gt;\n  func wait{FunctionName}()\n</code></pre> <ol> <li>If a file has a package declaration of <code>package aws</code>, you will need to change    it to the new package location. For example, if you moved a file to <code>internal/service/ecs</code>,    the declaration will now be <code>package ecs</code>.</li> </ol> <p>Any file that imports <code>\"github.com/hashicorp/terraform-provider-aws/internal/acctest\"</code> must    be in the <code>&lt;package&gt;_test</code> package. For example, <code>internal/service/ecs/account_setting_default_test.go</code>    does import the <code>acctest</code> package and must have a package declaration of <code>package ecs_test</code>.</p> <ol> <li>If you have made any changes to <code>aws/provider.go</code>, you will have to manually    re-enact those changes on the new <code>internal/provider/provider.go</code> file.</li> </ol> <p>Most commonly, these changes involve the addition of an entry to either the    <code>DataSourcesMap</code> or <code>ResourcesMap</code>. If this is the case for your PR, you will have    to adapt your entry to follow our new code conventions.</p> <p>Resources Map Entries</p> <pre><code>  \"{aws_terraform_resource_type}\":   resourceAws{ServiceName}{ResourceName}(), =&gt;\n  \"{aws_terraform_resource_type}\":   {serviceName}.Resource{ResourceName}(),\n</code></pre> <p>Data Source Map Entries</p> <pre><code>  \"{aws_terraform_data_source_type}\":   dataSourceAws{ServiceName}{ResourceName}(), =&gt;\n  \"{aws_terraform_data_source_type}\":   {serviceName}.DataSource{ResourceName}(),\n</code></pre> <ol> <li>Some functions, constants, and variables have been moved, removed, or renamed.    This table shows some of the common changes you may need to make to fix compile errors.</li> </ol> Before Now <code>isAWSErr(\u03b1, \u03b2, \"&lt;message&gt;\")</code> <code>tfawserr.ErrMessageContains(\u03b1, \u03b2, \"&lt;message&gt;\")</code> <code>isAWSErr(\u03b1, \u03b2, \"\")</code> <code>tfawserr.ErrCodeEquals(\u03b1, \u03b2)</code> <code>isResourceNotFoundError(\u03b1)</code> <code>tfresource.NotFound(\u03b1)</code> <code>isResourceTimeoutError(\u03b1)</code> <code>tfresource.TimedOut(\u03b1)</code> <code>testSweepSkipResourceError(\u03b1)</code> <code>tfawserr.ErrCodeContains(\u03b1, \"AccessDenied\")</code> <code>testAccPreCheck(t)</code> <code>acctest.PreCheck(t)</code> <code>testAccProviders</code> <code>acctest.Providers</code> <code>acctest.RandomWithPrefix(\"tf-acc-test\")</code> <code>sdkacctest.RandomWithPrefix(acctest.ResourcePrefix)</code> <code>composeConfig(\u03b1)</code> <code>acctest.ConfigCompose(\u03b1)</code> <ol> <li>Use <code>git status</code> to report the state of the merge. Review any merge    conflicts -- being sure to adopt the new naming conventions described in the    previous step where relevant. Use <code>git add</code> to add any new files to the commit.</li> </ol>"},{"location":"skaff/","title":"Provider Scaffolding (skaff)","text":"<p><code>skaff</code> is a Terraform AWS Provider scaffolding command line tool. It generates resource/data source files and accompanying test files which adhere to the latest best practice. These files are heavily commented with instructions so serve as the best way to get started with provider development.</p>"},{"location":"skaff/#overview-workflow-steps","title":"Overview workflow steps","text":"<ol> <li>Figure out what you're trying to do:<ul> <li>Create a resource or a data source?</li> <li>AWS Go SDK v1 or v2 code?</li> <li>Name of the new resource or data source?</li> </ul> </li> <li>Use <code>skaff</code> to generate provider code</li> <li>Go through the generated code completing code and customizing for the AWS Go SDK API</li> <li>Run, test, refine</li> <li>Remove \"TIP\" comments</li> <li>Submit code in pull request</li> </ol>"},{"location":"skaff/#running-skaff","title":"Running <code>skaff</code>","text":"<ol> <li>Use Git to clone the GitHub https://github.com/hashicorp/terraform-provider-aws repository.</li> <li><code>cd skaff</code></li> <li><code>go install .</code></li> <li>Change directories to the service where your new resource will reside. E.g., <code>cd ../internal/service/mq</code>.</li> <li>To get help, enter <code>skaff</code> without arguments.</li> <li>Generate a resource. E.g., <code>skaff resource --name BrokerReboot</code> (or equivalently <code>skaff resource -n BrokerReboot</code>).</li> </ol>"},{"location":"skaff/#usage","title":"Usage","text":""},{"location":"skaff/#help","title":"Help","text":"<pre><code>$ skaff --help\nUsage:\n  skaff [command]\n\nAvailable Commands:\n  completion  Generate the autocompletion script for the specified shell\n  datasource  Create scaffolding for a data source\n  help        Help about any command\n  resource    Create scaffolding for a resource\n\nFlags:\n  -h, --help   help for skaff\n</code></pre>"},{"location":"skaff/#autocompletion","title":"Autocompletion","text":"<p>Generate the autocompletion script for skaff for the specified shell</p> <pre><code>$ skaff completion --help\nUsage:\n  skaff completion [command]\n\nAvailable Commands:\n  bash        Generate the autocompletion script for bash\n  fish        Generate the autocompletion script for fish\n  powershell  Generate the autocompletion script for powershell\n  zsh         Generate the autocompletion script for zsh\n\nFlags:\n  -h, --help   help for completion\n\nUse \"skaff completion [command] --help\" for more information about a command\n</code></pre>"},{"location":"skaff/#data-source","title":"Data Source","text":"<p>Create scaffolding for a data source</p> <pre><code>$ skaff datasource --help\nUsage:\n  skaff datasource [flags]\n\nFlags:\n  -c, --clear-comments     Do not include instructional comments in source\n  -f, --force              Force creation, overwriting existing files\n  -h, --help               help for datasource\n  -n, --name string        Name of the entity\n  -s, --snakename string   If skaff doesn't get it right, explicitly give name in snake case (e.g., db_vpc_instance)\n  -o, --v1                 Generate code targeting aws-sdk-go v1 (some existing services) \n</code></pre>"},{"location":"skaff/#resource","title":"Resource","text":"<p>Create scaffolding for a resource</p> <pre><code>$ skaff resource --help\nUsage:\n  skaff resource [flags]\n\nFlags:\n  -c, --clear-comments     Do not include instructional comments in source\n  -f, --force              Force creation, overwriting existing files\n  -h, --help               help for resource\n  -n, --name string        Name of the entity\n  -s, --snakename string   If skaff doesn't get it right, explicitly give name in snake case (e.g., db_vpc_instance)\n  -o, --v1                 Generate code targeting aws-sdk-go v1 (some existing services) \n</code></pre>"}]}